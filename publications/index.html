<!DOCTYPE html>
<html lang="en">

  <head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">

  <title>Yonatan Belinkov | publications</title>
  <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
">

  <link rel="stylesheet" href="https://belinkov.com/assets/css/main.css">
  <link rel="canonical" href="https://belinkov.com/publications/">

  <!-- Styles -->
  
  <link rel="shortcut icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>🎓</text></svg>">
  

</head>


  <body>

    <header class="site-header">

  <div class="wrapper">

    
    <span class="site-title">
        
        <strong>Yonatan</strong> Belinkov
    </span>
    


    <nav class="site-nav">

      <div class="trigger">
        <!-- About -->
        <a class="page-link" href="https://belinkov.com/">about</a>

        <!-- Blog -->
	<!-- 
        <a class="page-link" href="https://belinkov.com/blog/">blog</a>
	-->

        <!-- Pages -->
        
          
        
          
            <a class="page-link" href="https://belinkov.com/publications/">publications</a>
          
        
          
            <a class="page-link" href="https://belinkov.com/students/">group</a>
          
        
          
        
          
            <a class="page-link" href="https://belinkov.com/teaching/">teaching</a>
          
        
          
            <a class="page-link" href="https://belinkov.com/talks/">talks</a>
          
        
          
            <a class="page-link" href="https://belinkov.com/advice/">advice</a>
          
        
          
            <a class="page-link" href="https://belinkov.com/assets/pdf/cv-belinkov.pdf">cv</a>
          
        
          
        
          
        

      </div>
    </nav>

  </div>

</header>



    <div class="page-content">
      <div class="wrapper">
        <div class="post">

  <header class="post-header">
    <h1 class="post-title">publications</h1>
     <!-- <h5 class="post-description"></h5> -->
  </header>

  <article class="post-content publications clearfix">
    <p><a class="pure-button" href="https://belinkov.com/publications/">By Year</a>
<a class="pure-button" href="https://belinkov.com/publications/publicationstype/">By Type</a></p>

<h2 class="bibliography">Preprints</h2>
<ol class="bibliography"><li>

<div id="suzgun:dyck">
  
    <span class="title">Memory-Augmented Recurrent Neural Networks Can Learn Generalized Dyck Languages.</span>
    <span class="author">
      
        
          
          
            
              Mirac Suzgun,
            
          
        
      
        
          
          
            
              <a href="https://www.sebastiangehrmann.com" target="_blank">Sebastian Gehrmann</a>,
            
          
        
      
        
          
          
            <em>Yonatan Belinkov</em>,
          
        
      
        
          
          
          
            
              <a href="http://www.eecs.harvard.edu/~shieber/index.html" target="_blank">Stuart Shieber</a>
            
          
        
      
    </span>

    
    <span class="periodical">
    
    
    </span>
    

    
  

  

  <span class="links">
  
  
  
  
  
  
  
    [<a href="https://arxiv.org/abs/1911.03329" target="_blank">Arxiv</a>]
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
</div>
</li>
<li>

<div id="hanna:formal-functional">
  
    <span class="title">Are formal and functional linguistic mechanisms dissociated in language models?.</span>
    <span class="author">
      
        
          
          
            
              Michael Hanna,
            
          
        
      
        
          
          
            <em>Yonatan Belinkov</em>,
          
        
      
        
          
          
          
            
              Sandro Pezzelle
            
          
        
      
    </span>

    
    <span class="periodical">
    
    
    </span>
    

    
  

  

  <span class="links">
  
  
  
  
  
  
  
    [<a href="https://arxiv.org/abs/2503.11302" target="_blank">Arxiv</a>]
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
</div>
</li>
<li>

<div id="gekhman:knowledge">
  
    <span class="title">Inside-Out: Hidden Factual Knowledge in LLMs.</span>
    <span class="author">
      
        
          
          
            
              Zorik Gekhman,
            
          
        
      
        
          
          
            
              Eyal Ben David,
            
          
        
      
        
          
          
            
              <a href="https://orgadhadas.github.io" target="_blank">Hadas Orgad</a>,
            
          
        
      
        
          
          
            
              Eran Ofek,
            
          
        
      
        
          
          
            <em>Yonatan Belinkov</em>,
          
        
      
        
          
          
            
              Idan Szpektor,
            
          
        
      
        
          
          
            
              Jonathan Herzig,
            
          
        
      
        
          
          
          
            
              Roi Reichart
            
          
        
      
    </span>

    
    <span class="periodical">
    
    
    </span>
    

    
  

  

  <span class="links">
  
  
  
  
  
  
  
    [<a href="https://arxiv.org/abs/2503.15299" target="_blank">Arxiv</a>]
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
</div>
</li>
<li>

<div id="tutek:cot">
  
    <span class="title">Measuring Faithfulness of Chains of Thought by Unlearning Reasoning Steps.</span>
    <span class="author">
      
        
          
          
            
              Martin Tutek,
            
          
        
      
        
          
          
            
              Fateme Hashemi Chaleshtori,
            
          
        
      
        
          
          
            
              Ana Marasović,
            
          
        
      
        
          
          
          
            <em>Yonatan Belinkov</em>
          
        
      
    </span>

    
    <span class="periodical">
    
    
    </span>
    

    
  

  

  <span class="links">
  
  
  
  
  
  
  
    [<a href="https://arxiv.org/abs/2502.14829" target="_blank">Arxiv</a>]
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
</div>
</li>
<li>

<div id="yu:multi-hop">
  
    <span class="title">Back Attention: Understanding and Enhancing Multi-Hop Reasoning in Large Language Models.</span>
    <span class="author">
      
        
          
          
            
              Zeping Yu,
            
          
        
      
        
          
          
            
              Sophia Ananiadou,
            
          
        
      
        
          
          
          
            <em>Yonatan Belinkov</em>
          
        
      
    </span>

    
    <span class="periodical">
    
    
    </span>
    

    
  

  

  <span class="links">
  
  
  
  
  
  
  
    [<a href="https://arxiv.org/abs/2502.10835" target="_blank">Arxiv</a>]
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
</div>
</li>
<li>

<div id="simhi:hallucinations-certain">
  
    <span class="title">Trust Me, I’m Wrong: High-Certainty Hallucinations in LLMs.</span>
    <span class="author">
      
        
          
          
            
              Adi Simhi,
            
          
        
      
        
          
          
            
              Itay Itzhak,
            
          
        
      
        
          
          
            
              Fazl Barez,
            
          
        
      
        
          
          
            
              Gabriel Stanovsky,
            
          
        
      
        
          
          
          
            <em>Yonatan Belinkov</em>
          
        
      
    </span>

    
    <span class="periodical">
    
    
    </span>
    

    
  

  

  <span class="links">
  
  
  
  
  
  
  
    [<a href="https://arxiv.org/abs/2502.12964" target="_blank">Arxiv</a>]
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
</div>
</li>
<li>

<div id="haklay:peap">
  
    <span class="title">Position-aware Automatic Circuit Discovery.</span>
    <span class="author">
      
        
          
          
            
              Tal Haklay,
            
          
        
      
        
          
          
            
              <a href="https://orgadhadas.github.io" target="_blank">Hadas Orgad</a>,
            
          
        
      
        
          
          
            
              <a href="https://baulab.info" target="_blank">David Bau</a>,
            
          
        
      
        
          
          
            
              Aaron Mueller,
            
          
        
      
        
          
          
          
            <em>Yonatan Belinkov</em>
          
        
      
    </span>

    
    <span class="periodical">
    
    
    </span>
    

    
  

  

  <span class="links">
  
  
  
  
  
  
  
    [<a href="https://arxiv.org/abs/2502.04577" target="_blank">Arxiv</a>]
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
</div>
</li>
<li>

<div id="ashuach:revs">
  
    <span class="title">REVS: Unlearning Sensitive Information in Language Models via Rank Editing in the Vocabulary Space.</span>
    <span class="author">
      
        
          
          
            
              Tomer Ashuach,
            
          
        
      
        
          
          
            
              Martin Tutek,
            
          
        
      
        
          
          
          
            <em>Yonatan Belinkov</em>
          
        
      
    </span>

    
    <span class="periodical">
    
    
    </span>
    

    
  

  

  <span class="links">
  
  
  
  
  
  
  
    [<a href="https://arxiv.org/abs/2406.09325" target="_blank">Arxiv</a>]
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
</div>
</li>
<li>

<div id="dotan:describe">
  
    <span class="title">Protein2Text: Providing Rich Descriptions for Protein Sequences.</span>
    <span class="author">
      
        
          
          
            
              Edo Dotan,
            
          
        
      
        
          
          
            
              Iris Lyubman,
            
          
        
      
        
          
          
            
              Eran Bacharach,
            
          
        
      
        
          
          
            
              Tal Pupko,
            
          
        
      
        
          
          
          
            <em>Yonatan Belinkov</em>
          
        
      
    </span>

    
    <span class="periodical">
    
    
    </span>
    

    
  

  

  <span class="links">
  
  
  
  
  
  
  
    [<a href="https://www.biorxiv.org/content/10.1101/2024.12.04.626777v1" target="_blank">Arxiv</a>]
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
</div>
</li>
<li>

<div id="tail">
  
    <span class="title">Growing a Tail: Increasing Output Diversity in Large Language Models.</span>
    <span class="author">
      
        
          
          
            
              Michal Shur-Ofry,
            
          
        
      
        
          
          
            
              Bar Horowitz-Amsalem,
            
          
        
      
        
          
          
            
              Adir Rahamim,
            
          
        
      
        
          
          
          
            <em>Yonatan Belinkov</em>
          
        
      
    </span>

    
    <span class="periodical">
    
    
    </span>
    

    
  

  

  <span class="links">
  
  
  
  
  
  
  
    [<a href="https://arxiv.org/abs/2411.02989" target="_blank">Arxiv</a>]
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
</div>
</li>
<li>

<div id="simhi:hallucinations">
  
    <span class="title">Distinguishing Ignorance from Error in LLM Hallucinations.</span>
    <span class="author">
      
        
          
          
            
              Adi Simhi,
            
          
        
      
        
          
          
            
              Jonathan Herzig,
            
          
        
      
        
          
          
            
              Idan Szpektor,
            
          
        
      
        
          
          
          
            <em>Yonatan Belinkov</em>
          
        
      
    </span>

    
    <span class="periodical">
    
    
    </span>
    

    
  

  

  <span class="links">
  
  
  
  
  
  
  
    [<a href="https://arxiv.org/abs/2410.22071" target="_blank">Arxiv</a>]
  
  
   
  
    [Media: <a href="https://www.forbes.com/sites/lanceeliot/2024/12/03/breakthrough-in-preemptive-detection-of-ai-hallucinations-reveals-vital-clues-to-writing-prompts-that-keep-generative-ai-from-freaking-out/" target="_blank">Forbes</a>] 
  
  </span>

  <!-- Hidden abstract block -->
  
</div>
</li>
<li>

<div id="mueller:mediator">
  
    <span class="title">The Quest for the Right Mediator: A History, Survey, and Theoretical Grounding of Causal Interpretability.</span>
    <span class="author">
      
        
          
          
            
              Aaron Mueller,
            
          
        
      
        
          
          
            
              Jannik Brinkmann,
            
          
        
      
        
          
          
            
              Millicent Li,
            
          
        
      
        
          
          
            
              Samuel Marks,
            
          
        
      
        
          
          
            
              Koyena Pal,
            
          
        
      
        
          
          
            
              Nikhil Prakash,
            
          
        
      
        
          
          
            
              Can Rager,
            
          
        
      
        
          
          
            
              Aruna Sankaranarayanan,
            
          
        
      
        
          
          
            
              Arnab Sen Sharma,
            
          
        
      
        
          
          
            
              Jiuding Sun,
            
          
        
      
        
          
          
            
              Eric Todd,
            
          
        
      
        
          
          
            
              <a href="https://baulab.info" target="_blank">David Bau</a>,
            
          
        
      
        
          
          
          
            <em>Yonatan Belinkov</em>
          
        
      
    </span>

    
    <span class="periodical">
    
    
    </span>
    

    
  

  

  <span class="links">
  
  
  
  
  
  
  
    [<a href="https://arxiv.org/abs/2408.01416" target="_blank">Arxiv</a>]
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
</div>
</li>
<li>

<div id="simhi:hallucinationt">
  
    <span class="title">Constructing Benchmarks and Interventions for Combating Hallucinations
  in LLMs.</span>
    <span class="author">
      
        
          
          
            
              Adi Simhi,
            
          
        
      
        
          
          
            
              Jonathan Herzig,
            
          
        
      
        
          
          
            
              Idan Szpektor,
            
          
        
      
        
          
          
          
            <em>Yonatan Belinkov</em>
          
        
      
    </span>

    
    <span class="periodical">
    
    
    </span>
    

    
  

  

  <span class="links">
  
  
  
  
  
  
  
    [<a href="https://arxiv.org/abs/2404.09971" target="_blank">Arxiv</a>]
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
</div>
</li>
<li>

<div id="elazar2022">
  
    <span class="title">Measuring Causal Effects of Data Statistics on Language Model’s ‘Factual’ Predictions.</span>
    <span class="author">
      
        
          
          
            
              Yanai Elazar,
            
          
        
      
        
          
          
            
              Nora Kassner,
            
          
        
      
        
          
          
            
              Shauli Ravfogel,
            
          
        
      
        
          
          
            
              Amir Feder,
            
          
        
      
        
          
          
            
              Abhilasha Ravichander,
            
          
        
      
        
          
          
            
              Marius Mosbach,
            
          
        
      
        
          
          
            <em>Yonatan Belinkov</em>,
          
        
      
        
          
          
            
              Hinrich Schütze,
            
          
        
      
        
          
          
          
            
              <a href="https://www.cs.bgu.ac.il/~yoavg/uni/" target="_blank">Yoav Goldberg</a>
            
          
        
      
    </span>

    
    <span class="periodical">
    
    
    </span>
    

    
  

  

  <span class="links">
  
  
  
  
  
  
  
    [<a href="https://arxiv.org/abs/2207.14251" target="_blank">Arxiv</a>]
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
</div>
</li>
<li>

<div id="vig:mediation">
  
    <span class="title">Causal Mediation Analysis for Interpreting NLP Models: The Case of Gender Bias.</span>
    <span class="author">
      
        
          
          
            
              Jesse Vig,
            
          
        
      
        
          
          
            
              <a href="https://www.sebastiangehrmann.com" target="_blank">Sebastian Gehrmann</a>,
            
          
        
      
        
          
          
            <em>Yonatan Belinkov</em>,
          
        
      
        
          
          
            
              Sharon Qian,
            
          
        
      
        
          
          
            
              Daniel Nevo,
            
          
        
      
        
          
          
            
              Yaron Singer,
            
          
        
      
        
          
          
          
            
              <a href="http://www.eecs.harvard.edu/~shieber/index.html" target="_blank">Stuart Shieber</a>
            
          
        
      
    </span>

    
    <span class="periodical">
    
    
    </span>
    

    
  

  

  <span class="links">
  
  
  
  
  
  
  
    [<a href="https://arxiv.org/abs/2004.12265" target="_blank">Arxiv</a>]
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
</div>
</li>
<li>

<div id="kaplan:toker:flow">
  
    <span class="title">Follow the Flow: On Information Flow Across Textual Tokens in Text-to-Image Models.</span>
    <span class="author">
      
        
          
          
            
              Guy Kaplan,
            
          
        
      
        
          
          
            
              Michael Toker,
            
          
        
      
        
          
          
            
              Yuval Reif,
            
          
        
      
        
          
          
            <em>Yonatan Belinkov</em>,
          
        
      
        
          
          
          
            
              Roy Schwartz
            
          
        
      
    </span>

    
    <span class="periodical">
    
    
    </span>
    

    
  

  

  <span class="links">
  
  
  
  
  
  
  
    [<a href="https://arxiv.org/abs/2504.01137" target="_blank">Arxiv</a>]
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
</div>
</li></ol>

<h2 class="bibliography">2025</h2>
<ol class="bibliography"><li>
  <abbr>[SIGIR]</abbr>


<div id="reusch:2025">
  
    <span class="title">How Generative IR Retrieves Documents Mechanistically.</span>
    <span class="author">
      
        
          
          
            
              Anja Reusch,
            
          
        
      
        
          
          
          
            <em>Yonatan Belinkov</em>
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In The 48th International ACM SIGIR Conference on Research and Development in Information Retrieval</em>
    
    
      2025
    
    </span>
    

    
  

  

  <span class="links">
  
  
  
  
  
  
  
    [<a href="https://arxiv.org/abs/2503.19715" target="_blank">Arxiv</a>]
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
</div>
</li>
<li>
  <abbr>[NAACL]</abbr>


<div id="toker:2025:naacl">
  
    <span class="title">Padding Tone: A Mechanistic Analysis of Padding Tokens in T2I Models.</span>
    <span class="author">
      
        
          
          
            
              Michael Toker,
            
          
        
      
        
          
          
            
              Ido Galil,
            
          
        
      
        
          
          
            
              <a href="https://orgadhadas.github.io" target="_blank">Hadas Orgad</a>,
            
          
        
      
        
          
          
            
              Rinon Gal,
            
          
        
      
        
          
          
            
              Yoad Tewel,
            
          
        
      
        
          
          
            
              Gal Chechik,
            
          
        
      
        
          
          
          
            <em>Yonatan Belinkov</em>
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Proceedings of the 2025 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)</em>
    
    
      2025
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
  
  
  
  
  
    [<a href="https://arxiv.org/abs/2501.06751" target="_blank">Arxiv</a>]
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Text-to-image (T2I) diffusion models rely on encoded prompts to guide the image generation process. Typically, these prompts are extended to a fixed length by adding padding tokens before text encoding. Despite being a default practice, the influence of padding tokens on the image generation process has not been investigated. In this work, we conduct the first in-depth analysis of the role padding tokens play in T2I models. We develop two causal techniques to analyze how information is encoded in the representation of tokens across different components of the T2I pipeline. Using these techniques, we investigate when and how padding tokens impact the image generation process. Our findings reveal three distinct scenarios: padding tokens may affect the model’s output during text encoding, during the diffusion process, or be effectively ignored. Moreover, we identify key relationships between these scenarios and the model’s architecture (cross or self-attention) and its training process (frozen or trained text encoder). These insights contribute to a deeper understanding of the mechanisms of padding tokens, potentially informing future model design and training practices in T2I systems.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[ICLR]</abbr>


<div id="carmeli:2025:iclr">
  
    <span class="title">CtD: Composition through Decomposition in Emergent Communication.</span>
    <span class="author">
      
        
          
          
            
              Boaz Carmeli,
            
          
        
      
        
          
          
            
              Ron Meir,
            
          
        
      
        
          
          
          
            <em>Yonatan Belinkov</em>
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In International Conference on Learning Representations (ICLR)</em>
    
    
      2025
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/iclr2025-ctd" target="_blank">PDF</a>]
  
  
  
  
  
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Compositionality is a cognitive mechanism that allows humans to systematically combine known concepts in novel ways. This study demonstrates how artificial neural agents acquire and utilize compositional generalization to describe previously unseen images. Our method, termed “Composition through Decomposition”, involves two sequential training steps. In the ’Decompose’ step, the agents learn to decompose an image into basic concepts using a codebook acquired during interaction in a multi-target coordination game. Subsequently, in the ‘Compose’ step, the agents employ this codebook to describe novel images by composing basic concepts into complex phrases. Remarkably, we observe cases where generalization in the ‘Compose’ step is achieved zero-shot, without the need for additional training.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[ICLR]</abbr>


<div id="marks:2025:iclr">
  
    <span class="title">Sparse Feature Circuits: Discovering and Editing Interpretable Causal Graphs in Language Models.</span>
    <span class="author">
      
        
          
          
            
              Samuel Marks,
            
          
        
      
        
          
          
            
              Can Rager,
            
          
        
      
        
          
          
            
              Eric J. Michaud,
            
          
        
      
        
          
          
            <em>Yonatan Belinkov</em>,
          
        
      
        
          
          
            
              <a href="https://baulab.info" target="_blank">David Bau</a>,
            
          
        
      
        
          
          
          
            
              Aaron Mueller
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In International Conference on Learning Representations (ICLR)</em>
    
    
      2025
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
  
  
    [<a href="https://belinkov.com/assets/pdf/iclr2025-sfc-poster.pdf" target="_blank">Poster</a>]
  
  
  
  
    [<a href="https://arxiv.org/abs/2403.19647" target="_blank">Arxiv</a>]
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>We introduce methods for discovering and applying sparse feature circuits. These are causally implicated subnetworks of human-interpretable features for explaining language model behaviors. Circuits identified in prior work consist of polysemantic and difficult-to-interpret units like attention heads or neurons, rendering them unsuitable for many downstream applications. In contrast, sparse feature circuits enable detailed understanding of unanticipated mechanisms. Because they are based on fine-grained units, sparse feature circuits are useful for downstream tasks: We introduce SHIFT, where we improve the generalization of a classifier by ablating features that a human judges to be task-irrelevant. Finally, we demonstrate an entirely unsupervised and scalable interpretability pipeline by discovering thousands of sparse feature circuits for automatically discovered model behaviors.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[ICLR]</abbr>


<div id="orgad:2025:iclr">
  
    <span class="title">LLMs Know More Than They Show: On the Intrinsic Representation of LLM Hallucinations.</span>
    <span class="author">
      
        
          
          
            
              <a href="https://orgadhadas.github.io" target="_blank">Hadas Orgad</a>,
            
          
        
      
        
          
          
            
              Michael Toker,
            
          
        
      
        
          
          
            
              Zorik Gekhman,
            
          
        
      
        
          
          
            
              Roi Reichart,
            
          
        
      
        
          
          
            
              Idan Szpektor,
            
          
        
      
        
          
          
            
              Hadas Kotek,
            
          
        
      
        
          
          
          
            <em>Yonatan Belinkov</em>
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In International Conference on Learning Representations (ICLR)</em>
    
    
      2025
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
  
  
  
  
  
    [<a href="https://arxiv.org/abs/2410.02707" target="_blank">Arxiv</a>]
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Large language models (LLMs) often produce errors, including factual inaccuracies, biases, and reasoning failures, collectively referred to as "hallucinations". Recent studies have demonstrated that LLMs’ internal states encode information regarding the truthfulness of their outputs, and that this information can be utilized to detect errors. In this work, we show that the internal representations of LLMs encode much more information about truthfulness than previously recognized. We first discover that the truthfulness information is concentrated in specific tokens, and leveraging this property significantly enhances error detection performance. Yet, we show that such error detectors fail to generalize across datasets, implying that – contrary to prior claims – truthfulness encoding is not universal but rather multifaceted. Next, we show that internal representations can also be used for predicting the types of errors the model is likely to make, facilitating the development of tailored mitigation strategies. Lastly, we reveal a discrepancy between LLMs’ internal encoding and external behavior: they may encode the correct answer, yet consistently generate an incorrect one. Taken together, these insights deepen our understanding of LLM errors from the model’s internal perspective, which can guide future research on enhancing error analysis and mitigation.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[ICLR]</abbr>


<div id="nikankin:2025:iclr">
  
    <span class="title">Arithmetic Without Algorithms: Language Models Solve Math With a Bag of Heuristics.</span>
    <span class="author">
      
        
          
          
            
              Yaniv Nikankin,
            
          
        
      
        
          
          
            
              Anja Reusch,
            
          
        
      
        
          
          
            
              Aaron Mueller,
            
          
        
      
        
          
          
          
            <em>Yonatan Belinkov</em>
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In International Conference on Learning Representations (ICLR)</em>
    
    
      2025
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
  
  
    [<a href="https://belinkov.com/assets/pdf/iclr2025-arithmetic-poster.pdf" target="_blank">Poster</a>]
  
  
  
  
    [<a href="https://arxiv.org/abs/2410.21272" target="_blank">Arxiv</a>]
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Do large language models (LLMs) solve reasoning tasks by learning robust generalizable algorithms, or do they memorize training data? To investigate this question, we use arithmetic reasoning as a representative task. Using causal analysis, we identify a subset of the model (a circuit) that explains most of the model’s behavior for basic arithmetic logic and examine its functionality. By zooming in on the level of individual circuit neurons, we discover a sparse set of important neurons that implement simple heuristics. Each heuristic identifies a numerical input pattern and outputs corresponding answers. We hypothesize that the combination of these heuristic neurons is the mechanism used to produce correct arithmetic answers. To test this, we categorize each neuron into several heuristic types-such as neurons that activate when an operand falls within a certain range-and find that the unordered combination of these heuristic types is the mechanism that explains most of the model’s accuracy on arithmetic prompts. Finally, we demonstrate that this mechanism appears as the main source of arithmetic accuracy early in training. Overall, our experimental results across several LLMs show that LLMs perform arithmetic using neither robust algorithms nor memorization; rather, they rely on a "bag of heuristics".</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[ICLR]</abbr>


<div id="wiegreffe:2025:iclr">
  
    <span class="title">Answer, Assemble, Ace: Understanding How Transformers Answer Multiple
  Choice Question.</span>
    <span class="author">
      
        
          
          
            
              Sarah Wiegreffe,
            
          
        
      
        
          
          
            
              Oyvind Tafjord,
            
          
        
      
        
          
          
            <em>Yonatan Belinkov</em>,
          
        
      
        
          
          
            
              Hannaneh Hajishirzi,
            
          
        
      
        
          
          
          
            
              Ashish Sabharwal
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In International Conference on Learning Representations (ICLR)</em>
    
    
      2025
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
  
  
  
  
  
    [<a href="https://arxiv.org/abs/2407.15018" target="_blank">Arxiv</a>]
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Multiple-choice question answering (MCQA) is a key competence of performant transformer language models that is tested by mainstream benchmarks. However, recent evidence shows that models can have quite a range of performance, particularly when the task format is diversified slightly (such as by shuffling answer choice order). In this work we ask: how do successful models perform formatted MCQA? We employ vocabulary projection and activation patching methods to localize key hidden states that encode relevant information for predicting the correct answer. We find that the prediction of a specific answer symbol is causally attributed to a few middle layers, and specifically their multi-head self-attention mechanisms. We show that subsequent layers increase the probability of the predicted answer symbol in vocabulary space, and that this probability increase is associated with a sparse set of attention heads with unique roles. We additionally uncover differences in how different models adjust to alternative symbols. Finally, we demonstrate that a synthetic task can disentangle sources of model error to pinpoint when a model has learned formatted MCQA, and show that logit differences between answer choice tokens continue to grow over the course of training.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[ICLR]</abbr>


<div id="jamba:2025:iclr">
  
    <span class="title">Jamba: Hybrid Transformer-Mamba Language Models.</span>
    <span class="author">
      
        
          
          
            
              Barak Lenz,
            
          
        
      
        
          
          
            
              Opher Lieber,
            
          
        
      
        
          
          
            
              Alan Arazi,
            
          
        
      
        
          
          
            
              Amir Bergman,
            
          
        
      
        
          
          
            
              Avshalom Manevich,
            
          
        
      
        
          
          
            
              Barak Peleg,
            
          
        
      
        
          
          
            
              Ben Aviram,
            
          
        
      
        
          
          
            
              Chen Almagor,
            
          
        
      
        
          
          
            
              Clara Fridman,
            
          
        
      
        
          
          
            
              Dan Padnos,
            
          
        
      
        
          
          
            
              Daniel Gissin,
            
          
        
      
        
          
          
            
              Daniel Jannai,
            
          
        
      
        
          
          
            
              Dor Muhlgay,
            
          
        
      
        
          
          
            
              Dor Zimberg,
            
          
        
      
        
          
          
            
              Edden M. Gerber,
            
          
        
      
        
          
          
            
              Elad Dolev,
            
          
        
      
        
          
          
            
              Eran Krakovsky,
            
          
        
      
        
          
          
            
              Erez Safahi,
            
          
        
      
        
          
          
            
              Erez Schwartz,
            
          
        
      
        
          
          
            
              Gal Cohen,
            
          
        
      
        
          
          
            
              Gal Shachaf,
            
          
        
      
        
          
          
            
              Haim Rozenblum,
            
          
        
      
        
          
          
            
              Hofit Bata,
            
          
        
      
        
          
          
            
              Ido Blass,
            
          
        
      
        
          
          
            
              Inbal Magar,
            
          
        
      
        
          
          
            
              Itay Dalmedigos,
            
          
        
      
        
          
          
            
              Jhonathan Osin,
            
          
        
      
        
          
          
            
              Julie Fadlon,
            
          
        
      
        
          
          
            
              Maria Rozman,
            
          
        
      
        
          
          
            
              Matan Danos,
            
          
        
      
        
          
          
            
              Michael Gokhman,
            
          
        
      
        
          
          
            
              Mor Zusman,
            
          
        
      
        
          
          
            
              Naama Gidron,
            
          
        
      
        
          
          
            
              Nir Ratner,
            
          
        
      
        
          
          
            
              Noam Gat,
            
          
        
      
        
          
          
            
              Noam Rozen,
            
          
        
      
        
          
          
            
              Oded Fried,
            
          
        
      
        
          
          
            
              Ohad Leshno,
            
          
        
      
        
          
          
            
              Omer Antverg,
            
          
        
      
        
          
          
            
              Omri Abend,
            
          
        
      
        
          
          
            
              Or Dagan,
            
          
        
      
        
          
          
            
              Orit Cohavi,
            
          
        
      
        
          
          
            
              Raz Alon,
            
          
        
      
        
          
          
            
              Ro’i Belson,
            
          
        
      
        
          
          
            
              Roi Cohen,
            
          
        
      
        
          
          
            
              Rom Gilad,
            
          
        
      
        
          
          
            
              Roman Glozman,
            
          
        
      
        
          
          
            
              Shahar Lev,
            
          
        
      
        
          
          
            
              Shai Shalev-Shwartz,
            
          
        
      
        
          
          
            
              Shaked Meirom,
            
          
        
      
        
          
          
            
              Tal Delbari,
            
          
        
      
        
          
          
            
              Tal Ness,
            
          
        
      
        
          
          
            
              Tomer Asida,
            
          
        
      
        
          
          
            
              Tom Ben Gal,
            
          
        
      
        
          
          
            
              Tom Braude,
            
          
        
      
        
          
          
            
              Uriya Pumerantz,
            
          
        
      
        
          
          
            
              Josh Cohen,
            
          
        
      
        
          
          
            <em>Yonatan Belinkov</em>,
          
        
      
        
          
          
            
              <a href="http://www.cs.tau.ac.il/~gamir/" target="_blank">Yuval Globerson</a>,
            
          
        
      
        
          
          
            
              Yuval Peleg Levy,
            
          
        
      
        
          
          
          
            
              Yoav Shoham
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In International Conference on Learning Representations (ICLR)</em>
    
    
      2025
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
  
  
  
  
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>We present Jamba, a novel hybrid Transformer-Mamba mixture-of-experts (MoE) architecture. Jamba interleaves blocks of Transformer and Mamba layers, enjoying the benefits of both model families. MoE is added in some of these layers to increase model capacity while keeping active parameter usage manageable. This flexible architecture allows resource- and objective-specific configurations. We implement two configurations: Jamba-1.5-Large, with 94B active parameters, and Jamba-1.5-mini, with 12B active parameters. Built at large scale, Jamba models provide high throughput and small memory footprint compared to vanilla Transformers, especially at long-context tasks, with an effective context length of 256K tokens, the largest amongst open-weight models. At the same time, they are also competitive on standard language modeling and chatbot benchmarks. We study various architectural decisions, such as how to combine Transformer and Mamba layers, and how to mix experts, and show that some of them are crucial in large scale modeling. To support cost-effective inference, we introduce ExpertsInt8, a novel quantization technique that allows fitting Jamba-1.5-Large on a machine with 8 80GB GPUs when processing 256K-token contexts without loss of quality. We also describe several interesting properties of this architecture that the training and evaluation of Jamba have revealed. The model weights are publicly available.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[AAAI]</abbr>


<div id="levy:2025:AAAI">
  
    <span class="title">Unsupervised Translation of Emergent Communication.</span>
    <span class="author">
      
        
          
          
            
              Ido Levy,
            
          
        
      
        
          
          
            
              Orr Paradise,
            
          
        
      
        
          
          
            
              Boaz Carmeli,
            
          
        
      
        
          
          
            
              Ron Meir,
            
          
        
      
        
          
          
            
              Shafi Goldwasser,
            
          
        
      
        
          
          
          
            <em>Yonatan Belinkov</em>
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Proceedings of the Thirty-Ninth AAAI Conference on Artificial Intelligence (AAAI)</em>
    
    
      2025
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/aaai2025.pdf" target="_blank">PDF</a>]
  
  
  
  
  
  
    [<a href="https://arxiv.org/abs/2502.07552" target="_blank">Arxiv</a>]
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Emergent Communication (EC) provides a unique window into the  language systems that emerge autonomously when agents are trained to jointly achieve shared goals. However, it is difficult to interpret EC and evaluate its relationship with natural languages (NL). This study employs unsupervised neural machine translation (UNMT) techniques to decipher ECs formed during referential games with varying task complexities, influenced by the semantic diversity of the environment. Our findings demonstrate UNMT’s potential to translate EC, illustrating that task complexity characterized by semantic diversity enhances EC translatability, while higher task complexity with constrained semantic variability exhibits pragmatic EC, which, although challenging to interpret, remains suitable for translation. This research marks the first attempt, to our knowledge, to translate EC without the aid of parallel data.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[Bioinfo.]</abbr>


<div id="dotan:2025:bioinformatics">
  
    <span class="title">BetaAlign: a deep learning approach for multiple sequence alignment.</span>
    <span class="author">
      
        
          
          
            
              Edo Dotan,
            
          
        
      
        
          
          
            
              Elya Wigoda,
            
          
        
      
        
          
          
            
              Noa Ecker,
            
          
        
      
        
          
          
            
              Michael Alburquerque,
            
          
        
      
        
          
          
            
              Oren Avram,
            
          
        
      
        
          
          
            <em>Yonatan Belinkov</em>,
          
        
      
        
          
          
          
            
              Tal Pupko
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>Bioinformatics</em>
    
    
      2025
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/bioinformatics2025.pdf" target="_blank">PDF</a>]
  
  
  
  
  
  
    [<a href="https://www.biorxiv.org/content/10.1101/2024.03.24.586462" target="_blank">Arxiv</a>]
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>The multiple sequence alignment (MSA) problem is a fundamental pillar in bioinformatics, comparative genomics, and phylogenetics. Here we characterize and improve BetaAlign, the first deep learning aligner, which substantially deviates from conventional algorithms of alignment computation. BetaAlign draws on natural language processing (NLP) techniques and trains transformers to map a set of unaligned biological sequences to an MSA. We show that our approach is highly accurate, comparable and sometimes better than state-of-the-art alignment tools. We characterize the performance of BetaAlign and the effect of various aspects on accuracy; for example, the size of the training data, the effect of different transformer architectures, and the effect of learning on a subspace of indel-model parameters (subspace learning). We also introduce a new technique that leads to improved performance compared to our previous approach. Our findings further uncover the potential of NLP-based approaches for sequence alignment, highlighting that AI-based methodologies can substantially challenge classic tasks in phylogenomics and bioinformatics.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[RepL4NLP]</abbr>


<div id="bamberger:repl4nlp:2025">
  
    <span class="title">DEPTH: Discourse Education through Pre-Training Hierarchically.</span>
    <span class="author">
      
        
          
          
            
              Zachary Bamberger,
            
          
        
      
        
          
          
            
              Ofek Glick,
            
          
        
      
        
          
          
            
              Chaim Baskin,
            
          
        
      
        
          
          
          
            <em>Yonatan Belinkov</em>
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Proceedings of the 10th Workshop on Representation Learning for NLP (RepL4NLP)</em>
    
    
      2025
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
  
  
  
  
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Language Models (LMs) struggle with linguistic understanding at the discourse level, even though discourse patterns such as coherence, cohesion, and narrative flow are prevalent in their pre-training data. To improve the discourse capabilities of LMs already at the pre-training stage, we introduce DEPTH, an encoder-decoder model that learns latent representations for sentences using a discourse-oriented pre-training objective. DEPTH combines hierarchical sentence representations with two objectives: (1) , and (2) . Our approach trains the model to represent both sub-word-level and sentence-level dependencies over a pre-training corpora. When trained either from scratch or continuing from a pre-trained T5 checkpoint, DEPTH learns semantic and discourse-level representations faster than T5, outperforming it in span-corruption loss despite the additional sentence-un-shuffling objective. Evaluations on the GLUE, DiscoEval, and NI benchmarks demonstrate DEPTH’s ability to quickly learn diverse downstream tasks, which require syntactic, semantic, and discourse capabilities. Our approach extends the discourse capabilities of T5, while minimally impacting other natural language understanding (NLU) capabilities in the resulting LM.</p>
  </span>
  
</div>
</li></ol>
<h2 class="bibliography">2024</h2>
<ol class="bibliography"><li>
  <abbr>[NeurIPS]</abbr>


<div id="stolfo:2024:neurips">
  
    <span class="title">Confidence Regulation Neurons in Language Models.</span>
    <span class="author">
      
        
          
          
            
              Alessandro Stolfo,
            
          
        
      
        
          
          
            
              Ben Peng Wu,
            
          
        
      
        
          
          
            
              Wes Gurnee,
            
          
        
      
        
          
          
            <em>Yonatan Belinkov</em>,
          
        
      
        
          
          
            
              Xingyi Song,
            
          
        
      
        
          
          
            
              Mrinmaya Sachan,
            
          
        
      
        
          
          
          
            
              Neel Nanda
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Advances in Neural Information Processing Systems (NeurIPS)</em>
    
    
      2024
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
  
  
  
  
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Despite their widespread use, the mechanisms by which large language models (LLMs) represent and regulate uncertainty in next-token predictions remain largely unexplored. This study investigates two critical components believed to influence this uncertainty: the recently discovered entropy neurons and a new set of components that we term token frequency neurons. Entropy neurons are characterized by an unusually high weight norm and influence the final layer normalization (LayerNorm) scale to effectively scale down the logits. Our work shows that entropy neurons operate by writing onto an \textitunembedding null space, allowing them to impact the residual stream norm with minimal direct effect on the logits themselves. We observe the presence of entropy neurons across a range of models, up to 7 billion parameters. On the other hand, token frequency neurons, which we discover and describe here for the first time, boost or suppress each token’s logit proportionally to its log frequency, thereby shifting the output distribution towards or away from the unigram distribution. Finally, we present a detailed case study where entropy neurons actively manage confidence: the setting of induction, i.e. detecting and continuing repeated subsequences.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[NeurIPS]</abbr>


<div id="benzion:2024:neurips">
  
    <span class="title">Semantics and Spatiality of Emergent Communication.</span>
    <span class="author">
      
        
          
          
            
              Rotem Ben Zion,
            
          
        
      
        
          
          
            
              Boaz Carmeli,
            
          
        
      
        
          
          
            
              Orr Paradise,
            
          
        
      
        
          
          
          
            <em>Yonatan Belinkov</em>
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Advances in Neural Information Processing Systems (NeurIPS)</em>
    
    
      2024
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/neurips2025-comm.pdf" target="_blank">PDF</a>]
  
  
  
  
  
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>When artificial agents are jointly trained to perform collaborative tasks using a communication channel, they develop opaque goal-oriented communication protocols. Good task performance is often considered sufficient evidence that meaningful communication is taking place, but existing empirical results show that communication strategies induced by common objectives can be counterintuitive whilst solving the task nearly perfectly. In this work, we identify a goal-agnostic prerequisite to meaningful communication, which we term semantic consistency, based on the idea that messages should have similar meanings across instances. We provide a formal definition for this idea, and use it to compare the two most common objectives in the field of emergent communication: discrimination and reconstruction. We prove, under mild assumptions, that semantically inconsistent communication protocols can be optimal solutions to the discrimination task, but not to reconstruction. We further show that the reconstruction objective encourages a stricter property, spatial meaningfulness, which also accounts for the distance between messages. Experiments with emergent communication games validate our theoretical results. These findings demonstrate an inherent advantage of distance-based communication goals, and contextualize previous empirical discoveries.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[EMNLP]</abbr>


<div id="rahamim:2024:emnlp">
  
    <span class="title">Fast Forwarding Low-Rank Training.</span>
    <span class="author">
      
        
          
          
            
              Adir Rahamim,
            
          
        
      
        
          
          
            
              Naomi Saphra,
            
          
        
      
        
          
          
            
              Sara Kangaslahti,
            
          
        
      
        
          
          
          
            <em>Yonatan Belinkov</em>
          
        
      
    </span>

    
    <span class="periodical">
    
    
      2024
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
  
  
  
  
  
    [<a href="https://arxiv.org/abs/2409.04206" target="_blank">Arxiv</a>]
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Parameter efficient finetuning methods like low-rank adaptation (LoRA) aim to reduce the computational costs of finetuning pretrained Language Models (LMs). Enabled by these low-rank settings, we propose an even more efficient optimization strategy: Fast Forward, a simple and effective approach to accelerate large segments of SGD training. In a Fast Forward stage, we repeat the most recent optimizer step until the loss stops improving on a tiny validation set. By alternating between regular optimization steps and Fast Forward stages, Fast Forward provides up to an 87% reduction in FLOPs over standard SGD with Adam. We validate Fast Forward by finetuning various models on different tasks and demonstrate that it speeds up training without compromising model performance. Additionally, we analyze when and how to apply Fast Forward.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[EMNLP]</abbr>


<div id="katz:2024:emnlp">
  
    <span class="title">Backward Lens: Projecting Language Model Gradients into the Vocabulary Space.</span>
    <span class="author">
      
        
          
          
            
              Shachar Katz,
            
          
        
      
        
          
          
            <em>Yonatan Belinkov</em>,
          
        
      
        
          
          
            
              Mor Geva,
            
          
        
      
        
          
          
          
            
              Lior Wolf
            
          
        
      
    </span>

    
    <span class="periodical">
    
    
      2024
    
    </span>
    

    
  

  
    <span style="color:Tomato;">Best paper award</span>
  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
  
  
  
  
  
    [<a href="https://arxiv.org/abs/2402.12865" target="_blank">Arxiv</a>]
  
  
   
  
    [Media: <a href="https://news.qq.com/rain/a/20241115A06P1H00" target="_blank">QQ News</a>] 
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Understanding how Transformer-based Language Models (LMs) learn and recall information is a key goal of the deep learning community. Recent interpretability methods project weights and hidden states obtained from the forward pass to the models’ vocabularies, helping to uncover how information flows within LMs. In this work, we extend this methodology to LMs’ backward pass and gradients. We first prove that a gradient matrix can be cast as a low-rank linear combination of its forward and backward passes’ inputs. We then develop methods to project these gradients into vocabulary items and explore the mechanics of how new information is stored in the LMs’ neurons.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[COLM]</abbr>


<div id="hanna:2024:colm">
  
    <span class="title">Have Faith in Faithfulness: Going Beyond Circuit Overlap When Finding Model Mechanism.</span>
    <span class="author">
      
        
          
          
            
              Michael Hanna,
            
          
        
      
        
          
          
            
              Sandro Pezzelle,
            
          
        
      
        
          
          
          
            <em>Yonatan Belinkov</em>
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Proceedings of the 2024 Conference on Language Models (COLM)</em>
    
    
      2024
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
  
  
  
  
  
    [<a href="https://arxiv.org/abs/2403.17806" target="_blank">Arxiv</a>]
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Many recent language model (LM) interpretability studies have adopted the circuits framework, which aims to find the minimal computational subgraph, or circuit, that explains LM behavior on a given task. Most studies determine which edges belong in a LM’s circuit by performing causal interventions on each edge independently, but this scales poorly with model size. Edge attribution patching (EAP), gradient-based approximation to interventions, has emerged as a scalable but imperfect solution to this problem. In this paper, we introduce a new method - EAP with integrated gradients (EAP-IG) - that aims to better maintain a core property of circuits: faithfulness. A circuit is faithful if all model edges outside the circuit can be ablated without changing the model’s performance on the task; faithfulness is what justifies studying circuits, rather than the full model. Our experiments demonstrate that circuits found using EAP are less faithful than those found using EAP-IG, even though both have high node overlap with circuits found previously using causal interventions. We conclude more generally that when using circuits to compare the mechanisms models use to solve tasks, faithfulness, not overlap, is what should be measured.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[AAAI]</abbr>


<div id="mor:2024:AAAI">
  
    <span class="title">Accelerating the Global Aggregation of Local Explanations.</span>
    <span class="author">
      
        
          
          
            
              Alon Mor,
            
          
        
      
        
          
          
            <em>Yonatan Belinkov</em>,
          
        
      
        
          
          
          
            
              Benny Kimelfeld
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Proceedings of the Thirty-Eighth AAAI Conference on Artificial Intelligence (AAAI)</em>
    
    
      2024
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/aaai2024.pdf" target="_blank">PDF</a>]
  
  
  
  
  
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Local explanation methods highlight the input tokens that have a considerable impact on the outcome of classifying the document at hand. For example, the Anchor algorithm applies a statistical analysis of the sensitivity of the classifier to changes in the token. Aggregating local explanations over a dataset provides a global explanation of the model. Such aggregation aims to detect words with the most impact, giving valuable insights about the model, like what it has learned in training and which adversarial examples expose its weaknesses. However, standard aggregation methods bear a high computational cost: a naive implementation applies a costly algorithm to each token of each document, and hence, it is infeasible for a simple user running in the scope of a short analysis session. We devise techniques for accelerating the global aggregation of the Anchor algorithm. Specifically, our goal is to compute a set of top-k words with the highest global impact according to different aggregation functions. Some of our techniques are lossless and some are lossy. We show that for a very mild loss of quality, we are able to accelerate the computation by up to 30x, reducing the computation from hours to minutes. We also devise and study a probabilistic model that accounts for noise in the Anchor algorithm and diminishes the bias toward words that are frequent yet low in impact.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[WACV]</abbr>


<div id="gandikota:2024:uce">
  
    <span class="title">Unified Concept Editing in Diffusion Models.</span>
    <span class="author">
      
        
          
          
            
              Rohit Gandikota,
            
          
        
      
        
          
          
            
              <a href="https://orgadhadas.github.io" target="_blank">Hadas Orgad</a>,
            
          
        
      
        
          
          
            <em>Yonatan Belinkov</em>,
          
        
      
        
          
          
            
              Joanna Materzyńska,
            
          
        
      
        
          
          
          
            
              <a href="https://baulab.info" target="_blank">David Bau</a>
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Proceedings of the Winter Conference on Applications of Computer Vision (WACV)</em>
    
    
      2024
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
  
  
  
  
  
    [<a href="https://arxiv.org/abs/2308.14761" target="_blank">Arxiv</a>]
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Text-to-image models suffer from various safety issues that may limit their suitability for deployment. Previous methods have separately addressed individual issues of bias, copyright, and offensive content in text-to-image models. However, in the real world, all of these issues appear simultaneously in the same model. We present a method that tackles all issues with a single approach. Our method, Unified Concept Editing (UCE), edits the model without training using a closed-form solution, and scales seamlessly to concurrent edits on text-conditional diffusion models. We demonstrate scalable simultaneous debiasing, style erasure, and content moderation by editing text-to-image projections, and we present extensive experiments demonstrating improved efficacy and scalability over prior work. Our code is available at unified.baulab.info.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[NAACL]</abbr>


<div id="iskander:2024:naacl">
  
    <span class="title">Leveraging Prototypical Representations for Mitigating Social Bias without Demographic Information.</span>
    <span class="author">
      
        
          
          
            
              Shadi Iskander,
            
          
        
      
        
          
          
            
              Kira Radinsky,
            
          
        
      
        
          
          
          
            <em>Yonatan Belinkov</em>
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)</em>
    
    
      2024
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/naacl2024-dafair.pdf" target="_blank">PDF</a>]
  
  
  
  
  
    [<a href="https://github.com/technion-cs-nlp/DAFair" target="_blank">Code</a>]
  
  
    [<a href="https://arxiv.org/abs/2403.09516" target="_blank">Arxiv</a>]
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Mitigating social biases typically requires identifying the social groups associated with each data sample. In this paper, we present DAFair, a novel approach to address social bias in language models. Unlike traditional methods that rely on explicit demographic labels, our approach does not require any such information. Instead, we leverage predefined prototypical demographic texts and incorporate a regularization term during the fine-tuning process to mitigate bias in the model’s representations. Our empirical results across two tasks and two models demonstrate the effectiveness of our method compared to previous approaches that do not rely on labeled data. Moreover, with limited demographic-annotated data, our approach outperforms common debiasing approaches.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[RepL4NLP]</abbr>


<div id="igbaria:repl4nlp:2024">
  
    <span class="title">Learning from Others: Similarity-based Regularization for Mitigating Dataset Bias.</span>
    <span class="author">
      
        
          
          
            
              Reda Igbaria,
            
          
        
      
        
          
          
          
            <em>Yonatan Belinkov</em>
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Proceedings of the 9th Workshop on Representation Learning for NLP (RepL4NLP)</em>
    
    
      2024
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
  
  
  
  
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Common methods for mitigating spurious correlations in natural language understanding (NLU) usually operate in the output space, encouraging a main model to behave differently from a bias model by down-weighing examples where the bias model is confident. While improving out of distribution (OOD) performance, it was recently observed that the internal representations of the presumably debiased models are actually more, rather than less biased. We propose SimgReg, a new method for debiasing internal model components via similarity-based regularization, in representation space: We encourage the model to learn representations that are either similar to an unbiased model or different from a biased model. We experiment with three NLU tasks and different kinds of biases. We find that SimReg improves OOD performance, with little in-distribution degradation. Moreover, the representations learned by SimReg are less biased than in other methods.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[ACL]</abbr>


<div id="toker:acl:2024">
  
    <span class="title">Diffusion Lens: Interpreting Text Encoders in Text-to-Image Pipelines.</span>
    <span class="author">
      
        
          
          
            
              Michael Toker,
            
          
        
      
        
          
          
            
              <a href="https://orgadhadas.github.io" target="_blank">Hadas Orgad</a>,
            
          
        
      
        
          
          
            
              Mor Ventura,
            
          
        
      
        
          
          
            
              Dana Arad,
            
          
        
      
        
          
          
          
            <em>Yonatan Belinkov</em>
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (ACL)</em>
    
    
      2024
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
  
  
  
  
  
    [<a href="https://arxiv.org/abs/2403.05846" target="_blank">Arxiv</a>]
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Text-to-image diffusion models (T2I) use a latent representation of a text prompt to guide the image generation process. However, the process by which the encoder produces the text representation is unknown. We propose the Diffusion Lens, a method for analyzing the text encoder of T2I models by generating images from its intermediate representations. Using the Diffusion Lens, we perform an extensive analysis of two recent T2I models. Exploring compound prompts, we find that complex scenes describing multiple objects are composed progressively and more slowly compared to simple scenes; Exploring knowledge retrieval, we find that representation of uncommon concepts require further computation compared to common concepts, and that knowledge retrieval is gradual across layers. Overall, our findings provide valuable insights into the text encoder component in T2I pipelines.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[ACL]</abbr>


<div id="carmeli:acl:2024">
  
    <span class="title">Concept-Best-Matching: Evaluating Compositionality in Emergent
  Communication.</span>
    <span class="author">
      
        
          
          
            
              Boaz Carmeli,
            
          
        
      
        
          
          
            <em>Yonatan Belinkov</em>,
          
        
      
        
          
          
          
            
              Ron Meir
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Findings of the Association for Computational Linguistics (ACL)</em>
    
    
      2024
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
  
  
  
  
  
    [<a href="https://arxiv.org/abs/2403.14705" target="_blank">Arxiv</a>]
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Artificial agents that learn to communicate in order to accomplish a given task acquire communication protocols that are typically opaque to a human. A large body of work has attempted to evaluate the emergent communication via various evaluation measures, with <i>compositionality</i> featuring as a prominent desired trait. However, current evaluation procedures do not directly expose the compositionality of the emergent communication. We propose a procedure to assess the compositionality of emergent communication by finding the best-match between emerged words and natural language concepts. The best-match algorithm provides both a global score and a translation-map from emergent words to natural language concepts. To the best of our knowledge, it is the first time that such direct and interpretable mapping between emergent words and human concepts is provided.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[Bioinfo.]</abbr>


<div id="dotan:2024:bioinformatics">
  
    <span class="title">Effect of Tokenization on Transformers for Biological Sequences.</span>
    <span class="author">
      
        
          
          
            
              Edo Dotan,
            
          
        
      
        
          
          
            
              Gal Jaschek,
            
          
        
      
        
          
          
            
              Tal Pupko,
            
          
        
      
        
          
          
          
            <em>Yonatan Belinkov</em>
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>Bioinformatics</em>
    
    
      2024
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/bioinformatics2024.pdf" target="_blank">PDF</a>]
  
  
  
  
  
    [<a href="https://github.com/technion-cs-nlp/BiologicalTokenizers" target="_blank">Code</a>]
  
  
    [<a href="https://www.biorxiv.org/content/10.1101/2023.08.15.553415" target="_blank">Arxiv</a>]
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Deep-learning models are transforming biological research, including many bioinformatics and comparative genomics algorithms, such as sequence alignments, phylogenetic tree inference, and automatic classification of protein functions. Among these deep-learning algorithms, models for processing natural languages, developed in the natural language processing (NLP) community, were recently applied to biological sequences. However, biological sequences are different from natural languages, such as English, and French, in which segmentation of the text to separate words is relatively straightforward. Moreover, biological sequences are characterized by extremely long sentences, which hamper their processing by current machine-learning models, notably the transformer architecture. In NLP, one of the first processing steps is to transform the raw text to a list of tokens. Deep-learning applications to biological sequence data mostly segment proteins and DNA to single characters. In this work, we study the effect of alternative tokenization algorithms on eight different tasks in biology, from predicting the function of proteins and their stability, through nucleotide sequence alignment, to classifying proteins to specific families. We demonstrate that applying alternative tokenization algorithms can increase accuracy and at the same time, substantially reduce the input length compared to the trivial tokenizer in which each character is a token. Furthermore, applying these tokenization algorithms allows interpreting trained models, taking into account dependencies among positions. Finally, we trained these tokenizers on a large dataset of protein sequences containing more than 400 billion amino acids, which resulted in over a three-fold decrease in the number of tokens. We then tested these tokenizers trained on large-scale data on the above specific tasks and showed that for some tasks it is highly beneficial to train database-specific tokenizers. Our study suggests that tokenizers are likely to be a critical component in future deep-network analysis of biological sequence data.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[NAACL]</abbr>


<div id="rahamim:2024:naacl">
  
    <span class="title">ContraSim – A Similarity Measure Based on Contrastive Learning.</span>
    <span class="author">
      
        
          
          
            
              Adir Rahamim,
            
          
        
      
        
          
          
          
            <em>Yonatan Belinkov</em>
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)</em>
    
    
      2024
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/naacl2024-contrasim.pdf" target="_blank">PDF</a>]
  
  
  
  
  
    [<a href="https://github.com/technion-cs-nlp/ContraSim" target="_blank">Code</a>]
  
  
    [<a href="https://arxiv.org/abs/2303.16992" target="_blank">Arxiv</a>]
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Recent work has compared neural network representations via similarity-based analyses to improve model interpretation. The quality of a similarity measure is typically evaluated by its success in assigning a high score to representations that are expected to be matched. However, existing similarity measures perform mediocrely on standard benchmarks.  In this work, we develop a new similarity measure, dubbed ContraSim, based on contrastive learning. In contrast to common closed-form similarity measures, ContraSim learns a parameterized measure by using both similar and dissimilar examples.  We perform an extensive experimental evaluation of our method, with both language and vision models, on the standard layer prediction benchmark and two new benchmarks that we introduce: the multilingual benchmark and the image–caption benchmark. In all cases, ContraSim achieves much higher accuracy than previous similarity measures, even when presented with challenging examples. Finally, ContraSim is more suitable for the analysis of neural networks, revealing new insights not captured by previous measures.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[NAACL]</abbr>


<div id="arad:2024:naacl">
  
    <span class="title">ReFACT: Updating Text-to-Image Models by Editing the Text Encoder.</span>
    <span class="author">
      
        
          
          
            
              Dana Arad*,
            
          
        
      
        
          
          
            
              <a href="https://orgadhadas.github.io" target="_blank">Hadas Orgad*</a>,
            
          
        
      
        
          
          
          
            <em>Yonatan Belinkov</em>
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)</em>
    
    
      2024
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/naacl2024-refact.pdf" target="_blank">PDF</a>]
  
  
  
  
  
    [<a href="https://technion-cs-nlp.github.io/ReFACT/" target="_blank">Code</a>]
  
  
    [<a href="https://arxiv.org/abs/2306.00738" target="_blank">Arxiv</a>]
  
  
   
  
    [Media: <a href="https://techxplore.com/news/2024-06-biases-image-generator.html" target="_blank">Tech Xplore</a>, <a href="https://www.geektime.co.il/isareli-researchers-may-be-able-to-help-ai-image-generators/" target="_blank">Geektime</a>] 
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Our world is marked by unprecedented technological, global, and socio-political transformations, posing a significant challenge to textto-image generative models. These models encode factual associations within their parameters that can quickly become outdated, diminishing their utility for end-users. To that end, we introduce ReFACT, a novel approach for editing factual associations in text-to-image models without relaying on explicit input from end-users or costly re-training. ReFACT updates the weights of a specific layer in the text encoder, modifying only a tiny portion of the model’s parameters and leaving the rest of the model unaffected. We empirically evaluate ReFACT on an existing benchmark, alongside a newly curated dataset. Compared to other methods, ReFACT achieves superior performance in both generalization to related concepts and preservation of unrelated concepts. Furthermore, ReFACT maintains image generation quality, making it a practical tool for updating and correcting factual information in text-to-image models.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[TACL]</abbr>


<div id="itzhak:2024:tacl">
  
    <span class="title">Instructed to Bias: Instruction-Tuned Language Models Exhibit Emergent Cognitive Bias.</span>
    <span class="author">
      
        
          
          
            
              Itay Itzhak,
            
          
        
      
        
          
          
            
              Gabriel Stanovsky,
            
          
        
      
        
          
          
            
              Nir Rosenfeld,
            
          
        
      
        
          
          
          
            <em>Yonatan Belinkov</em>
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>Transactions of the Association for Computational Linguistics (TACL)</em>
    
    
      2024
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/tacl2024.pdf" target="_blank">PDF</a>]
  
  
  
  
  
  
    [<a href="https://arxiv.org/abs/2308.00225" target="_blank">Arxiv</a>]
  
  
    [<a href="https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00673/121541/Instructed-to-Bias-Instruction-Tuned-Language" target="_blank">URL</a>]
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Recent studies show that instruction tuning (IT) and reinforcement learning from human feedback (RLHF) improve the abilities of large language models (LMs) dramatically. While these tuning methods can help align models with human objectives and generate high-quality text, not much is known about their potential adverse effects. In this work, we investigate the effect of IT and RLHF on decision making and reasoning in LMs, focusing on three cognitive biases—the decoy effect, the certainty effect, and the belief bias—all of which are known to influence human decision-making and reasoning. Our findings highlight the presence of these biases in various models from the GPT-3, Mistral, and T5 families. Notably, we find a stronger presence of biases in models that have undergone instruction tuning, such as Flan-T5, Mistral-Instruct, GPT3.5, and GPT4. Our work constitutes a step toward comprehending cognitive biases in instruction-tuned LMs, which is crucial for the development of more reliable and unbiased language models.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[EACL]</abbr>


<div id="toker:2024:eacl">
  
    <span class="title">A Dataset for Metaphor Detection in Early Medieval Hebrew Poetry.</span>
    <span class="author">
      
        
          
          
            
              Michael Toker,
            
          
        
      
        
          
          
            <em>Yonatan Belinkov</em>,
          
        
      
        
          
          
            
              Oren Mishali,
            
          
        
      
        
          
          
            
              Ophir Muenz-Manor,
            
          
        
      
        
          
          
          
            
              Benny Kimelfeld
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics</em>
    
    
      2024
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/eacl2024.pdf" target="_blank">PDF</a>]
  
  
  
  
  
  
    [<a href="https://arxiv.org/abs/2402.17371" target="_blank">Arxiv</a>]
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>The corpora of late antique and medieval Hebrew texts are vast. They represent a crucial linguistic and cultural bridge between Biblical and modern Hebrew. 
Poetry is prominent in these corpora and one of its main characteristics is the frequent use of metaphor. Distinguishing figurative and literal language use is a major task for scholars of the Humanities, especially in the fields of literature, linguistics, and hermeneutics. This paper presents a new, challenging dataset of late antique and medieval Hebrew poetry with expert annotations of metaphor, as well as some baseline results, which we hope will facilitate further research in this area.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[EACL]</abbr>


<div id="muhlgay:2024:eacl">
  
    <span class="title">Generating Benchmarks for Factuality Evaluation of Language Models.</span>
    <span class="author">
      
        
          
          
            
              Dor Muhlgay,
            
          
        
      
        
          
          
            
              Ori Ram,
            
          
        
      
        
          
          
            
              Inbal Magar,
            
          
        
      
        
          
          
            
              Yoav Levine,
            
          
        
      
        
          
          
            
              Nir Ratner,
            
          
        
      
        
          
          
            <em>Yonatan Belinkov</em>,
          
        
      
        
          
          
            
              Omri Abend,
            
          
        
      
        
          
          
            
              Kevin Leyton-Brown,
            
          
        
      
        
          
          
            
              Amnon Shashua,
            
          
        
      
        
          
          
          
            
              Yoav Shoham
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics</em>
    
    
      2024
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
  
  
  
  
  
    [<a href="https://arxiv.org/abs/2307.06908" target="_blank">Arxiv</a>]
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Before deploying a language model (LM) within a given domain, it is important to measure its tendency to generate factually incorrect information in that domain. Existing methods for factuality evaluation of LLM generation focus on facts sampled from the LM itself, and thus do not control the set of evaluated facts and might under-represent domain specific or rare facts.  We propose FACTOR: Factual Assessment via Corpus TransfORmation, a scalable approach for evaluating LM factuality.  FACTOR automatically transforms a factual corpus of interest into a benchmark evaluating an LM’s propensity to generate true facts from the corpus vs. similar but incorrect statements. We use our framework to create three benchmarks: Wiki-FACTOR, News-FACTOR and Expert-FACTOR. We show that: (i) our benchmark scores increase with model size and improve when the LM is augmented with retrieval; (ii) benchmark score and perplexity do not always agree on model ranking; (iii) when perplexity and benchmark score disagree, the latter better reflects factuality in open-ended generation, as measured by human annotators.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[ICLR]</abbr>


<div id="prakash:2024:iclr">
  
    <span class="title">Fine-Tuning Enhances Existing Mechanisms: A Case Study on Entity Tracking.</span>
    <span class="author">
      
        
          
          
            
              Nikhil Prakash,
            
          
        
      
        
          
          
            
              Tamar Rott Shaham,
            
          
        
      
        
          
          
            
              Tal Haklay,
            
          
        
      
        
          
          
            <em>Yonatan Belinkov</em>,
          
        
      
        
          
          
          
            
              <a href="https://baulab.info" target="_blank">David Bau</a>
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In International Conference on Learning Representations (ICLR)</em>
    
    
      2024
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
  
  
  
  
  
    [<a href="http://arxiv.org/abs/2402.14811" target="_blank">Arxiv</a>]
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Fine-tuning on generalized tasks such as instruction following, code generation, and mathematics has been shown to enhance language models’ performance on a range of tasks. Nevertheless, explanations of how such fine-tuning influences the internal computations in these models remain elusive. We study how fine-tuning affects the internal mechanisms implemented in language models. As a case study, we explore the property of entity tracking, a crucial facet of language comprehension, where models fine-tuned on mathematics have substantial performance gains. We identify a mechanism that enables entity tracking and show that (i) both the original model and its fine-tuned version implement entity tracking with the same circuit. In fact, the entity tracking circuit of the fine-tuned version performs better than the full original model. (ii) The circuits of all the models implement roughly the same functionality, that is entity tracking is performed by tracking the position of the correct entity in both the original model and its fine-tuned version. (iii) Performance boost in the fine-tuned model is primarily attributed to its improved ability to handle positional information. To uncover these findings, we employ two methods: DCM, which automatically detects model components responsible for specific semantics, and CMAP, a new approach for patching activations across models to reveal improved mechanisms. Our findings suggest that fine-tuning enhances, rather than fundamentally alters, the mechanistic operation of the model.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[ICLR]</abbr>


<div id="hernandez:2024:iclr">
  
    <span class="title">Linearity of Relation Decoding in Transformer Language Models.</span>
    <span class="author">
      
        
          
          
            
              Evan Hernandez*,
            
          
        
      
        
          
          
            
              Arnab Sen Sharma*,
            
          
        
      
        
          
          
            
              Tal Haklay,
            
          
        
      
        
          
          
            
              Kevin Meng,
            
          
        
      
        
          
          
            
              Martin Wattenberg,
            
          
        
      
        
          
          
            
              Jacob Andreas,
            
          
        
      
        
          
          
            <em>Yonatan Belinkov</em>,
          
        
      
        
          
          
          
            
              <a href="https://baulab.info" target="_blank">David Bau</a>
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In International Conference on Learning Representations (ICLR, Spotlight)</em>
    
    
      2024
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
  
  
  
  
  
    [<a href="https://arxiv.org/abs/2308.09124" target="_blank">Arxiv</a>]
  
  
   
  
    [Media: <a href="https://news.mit.edu/2024/large-language-models-use-surprisingly-simple-mechanism-retrieve-stored-knowledge-0325" target="_blank">MIT News</a>, <a href="https://www.sflorg.com/2024/03/ai03252401.html" target="_blank">Scientific Frontline</a>] 
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Much of the knowledge encoded in transformer language models (LMs) may be expressed in terms of relations: relations between words and their synonyms, entities and their attributes, etc. We show that, for a subset of relations, this computation is well-approximated by a single linear transformation on the subject representation. Linear relation representations may be obtained by constructing a first-order approximation to the LM from a single prompt, and they exist for a variety of factual, commonsense, and linguistic relations. However, we also identify many cases in which LM predictions capture relational knowledge accurately, but this knowledge is not linearly encoded in their representations. Our results thus reveal a simple, interpretable, but heterogeneously deployed knowledge representation strategy in transformer LMs.</p>
  </span>
  
</div>
</li></ol>
<h2 class="bibliography">2023</h2>
<ol class="bibliography"><li>
  <abbr>[EMNLP]</abbr>


<div id="katz:2023:emnlp:viz">
  
    <span class="title">VISIT: Visualizing and Interpreting the Semantic Information Flow of Transformers.</span>
    <span class="author">
      
        
          
          
            
              Shachar Katz,
            
          
        
      
        
          
          
          
            <em>Yonatan Belinkov</em>
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Findings of the Association for Computational Linguistics: EMNLP 2023 (EMNLP)</em>
    
    
      2023
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/emnlp2023-findings-visit.pdf" target="_blank">PDF</a>]
  
  
  
  
  
  
    [<a href="https://arxiv.org/abs/2305.13417" target="_blank">Arxiv</a>]
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Recent advances in interpretability suggest we can project weights and hidden states of transformer-based language models (LMs) to their vocabulary, a transformation that makes them more human interpretable. In this paper, we investigate LM attention heads and memory values, the vectors the models dynamically create and recall while processing a given input. By analyzing the tokens they represent through this projection, we identify patterns in the information flow inside the attention mechanism.  Based on our discoveries, we create a tool to visualize a forward pass of Generative Pre-trained Transformers (GPTs) as an interactive flow graph, with nodes representing neurons or hidden states and edges representing the interactions between them. Our visualization  simplifies huge amounts of data into easy-to-read plots that can reflect the models’ internal processing, uncovering the contribution of each component to the models’ final prediction. Our visualization also unveils new insights about the role of layer norms as semantic filters that influence the models’ output, and about neurons that are always activated during forward passes and act as regularization vectors.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[EMNLP]</abbr>


<div id="stolfo:2023:emnlp:math">
  
    <span class="title">A Mechanistic Interpretation of Arithmetic Reasoning in Language Models using Causal Mediation Analysis.</span>
    <span class="author">
      
        
          
          
            
              Alessandro Stolfo,
            
          
        
      
        
          
          
            <em>Yonatan Belinkov</em>,
          
        
      
        
          
          
          
            
              Mrinmaya Sachan
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>
    
    
      2023
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/emnlp2023-math.pdf" target="_blank">PDF</a>]
  
  
  
  
  
  
    [<a href="https://arxiv.org/abs/2305.15054" target="_blank">Arxiv</a>]
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Mathematical reasoning in large language models (LMs) has garnered significant attention in recent work, but there is a limited understanding of how these models process and store information related to arithmetic tasks within their architecture. In order to improve our understanding of this aspect of language models, we present a mechanistic interpretation of Transformer-based LMs on arithmetic questions using a causal mediation analysis framework. By intervening on the activations of specific model components and measuring the resulting changes in predicted probabilities, we identify the subset of parameters responsible for specific predictions. This provides insights into how information related to arithmetic is processed by LMs. Our experimental results indicate that LMs process the input by transmitting the information relevant to the query from mid-sequence early layers to the final token using the attention mechanism. Then, this information is processed by a set of MLP modules, which generate result-related information that is incorporated into the residual stream. To assess the specificity of the observed activation dynamics, we compare the effects of different model components on arithmetic queries with other tasks, including number retrieval from prompts and factual knowledge questions.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[EMNLP]</abbr>


<div id="hanna:2023:emnlp">
  
    <span class="title">When Language Models Fall in Love: Animacy Processing in Transformer Language Models.</span>
    <span class="author">
      
        
          
          
            
              Michael Hanna,
            
          
        
      
        
          
          
            <em>Yonatan Belinkov</em>,
          
        
      
        
          
          
          
            
              Sandro Pezzelle
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>
    
    
      2023
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/emnlp2023-animacy.pdf" target="_blank">PDF</a>]
  
  
  
  
  
    [<a href="https://github.com/hannamw/lms-in-love" target="_blank">Code</a>]
  
  
    [<a href="https://arxiv.org/abs/2310.15004" target="_blank">Arxiv</a>]
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Animacy—whether an entity is alive and sentient—is fundamental to cognitive processing, impacting areas such as memory, vision, and language. However, animacy is not always expressed directly in language: in English it often manifests indirectly, in the form of selectional constraints on verbs and adjectives.  This poses a potential issue for transformer language models (LMs): they often train only on text, and thus lack access to extralinguistic information from which humans learn about animacy. We ask: how does this impact LMs’ animacy processing—do they still behave as humans do? Like previous studies, we find that LMs behave much like humans when presented with entities whose animacy is typical. However, we also show that even when presented with stories about atypically animate entities, such as \textita peanut in love, LMs adapt: they treat these entities as animate, though they do not adapt as well as humans. Even when the context indicating atypical animacy is very short, LMs pick up on subtle clues and change their behavior. We conclude that despite the limited signal through which LMs can learn about animacy, they are indeed sensitive to the relevant lexical semantic nuances available in English.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[ICCV]</abbr>


<div id="orgad:2023:iccv:time">
  
    <span class="title">Editing Implicit Assumptions in Text-to-Image Diffusion Models.</span>
    <span class="author">
      
        
          
          
            
              <a href="https://orgadhadas.github.io" target="_blank">Hadas Orgad</a>,
            
          
        
      
        
          
          
            
              Bahjat Kawar,
            
          
        
      
        
          
          
          
            <em>Yonatan Belinkov</em>
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</em>
    
    
      2023
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/iccv2023.pdf" target="_blank">PDF</a>]
  
  
  
  
  
    [<a href="https://github.com/bahjat-kawar/time-diffusion" target="_blank">Code</a>]
  
  
    [<a href="https://arxiv.org/abs/2303.08084" target="_blank">Arxiv</a>]
  
  
    [<a href="https://time-diffusion.github.io" target="_blank">URL</a>]
  
   
  
    [Media: <a href="https://techxplore.com/news/2024-06-biases-image-generator.html" target="_blank">Tech Xplore</a>, <a href="https://www.geektime.co.il/isareli-researchers-may-be-able-to-help-ai-image-generators/" target="_blank">Geektime</a>] 
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Text-to-image diffusion models often make implicit assumptions about the world when generating images. While some assumptions are useful (e.g., the sky is blue), they can also be outdated, incorrect, or reflective of social biases present in the training data. Thus, there is a need to control these assumptions without requiring explicit user input or costly re-training. In this work, we aim to edit a given implicit assumption in a pre-trained diffusion model. Our Text-to-Image Model Editing method, TIME for short, receives a pair of inputs: a "source" under-specified prompt for which the model makes an implicit assumption (e.g., "a pack of roses"), and a "destination" prompt that describes the same setting, but with a specified desired attribute (e.g., "a pack of blue roses"). TIME then updates the model’s cross-attention layers, as these layers assign visual meaning to textual tokens. We edit the projection matrices in these layers such that the source prompt is projected close to the destination prompt. Our method is highly efficient, as it modifies a mere 2.2% of the model’s parameters in under one second. To evaluate model editing approaches, we introduce TIMED (TIME Dataset), containing 147 source and destination prompt pairs from various domains. Our experiments (using Stable Diffusion) show that TIME is successful in model editing, generalizes well for related prompts unseen during editing, and imposes minimal effect on unrelated generations.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[ACL]</abbr>


<div id="ram:2023:acl">
  
    <span class="title">What Are You Token About? Dense Retrieval as Distributions Over the Vocabulary.</span>
    <span class="author">
      
        
          
          
            
              Ori Ram,
            
          
        
      
        
          
          
            
              Liat Bezalel,
            
          
        
      
        
          
          
            
              Adi Zicher,
            
          
        
      
        
          
          
            <em>Yonatan Belinkov</em>,
          
        
      
        
          
          
            
              Jonathan Berant,
            
          
        
      
        
          
          
          
            
              <a href="http://www.cs.tau.ac.il/~gamir/" target="_blank">Amir Globerson</a>
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (ACL)</em>
    
    
      2023
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/acl2023-token.pdf" target="_blank">PDF</a>]
  
  
  
  
  
    [<a href="https://github.com/oriram/dense-retrieval-projections" target="_blank">Code</a>]
  
  
    [<a href="https://arxiv.org/abs/2212.10380" target="_blank">Arxiv</a>]
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Dual encoders are now the dominant architecture for dense retrieval. Yet, we have little understanding of how they represent text, and why this leads to good performance. In this work, we shed light on this question via distributions over the vocabulary. We propose to interpret the vector representations produced by dual encoders by projecting them into the model’s vocabulary space. We show that the resulting projections contain rich semantic information, and draw connection between them and sparse retrieval. We find that this view can offer an explanation for some of the failure cases of dense retrievers. For example, we observe that the inability of models to handle tail entities is correlated with a tendency of the token distributions to forget some of the tokens of those entities. We leverage this insight and propose a simple way to enrich query and passage representations with lexical information at inference time, and show that this significantly improves performance compared to the original model in zero-shot settings, and specifically on the BEIR benchmark.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[ACL]</abbr>


<div id="orgad:2023:acl:dfl">
  
    <span class="title">BLIND: Bias Removal With No Demographics.</span>
    <span class="author">
      
        
          
          
            
              <a href="https://orgadhadas.github.io" target="_blank">Hadas Orgad</a>,
            
          
        
      
        
          
          
          
            <em>Yonatan Belinkov</em>
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (ACL)</em>
    
    
      2023
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/acl2023-blind.pdf" target="_blank">PDF</a>]
  
  
  
  
  
    [<a href="https://github.com/technion-cs-nlp/BLIND" target="_blank">Code</a>]
  
  
    [<a href="https://arxiv.org/abs/2212.10563" target="_blank">Arxiv</a>]
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Models trained on real-world data tend to imitate and amplify social biases. Common methods to mitigate biases require prior information on the types of biases that should be mitigated (e.g., gender or racial bias) and the social groups associated with each data sample. In this work, we introduce BLIND, a method for bias removal with no prior knowledge of the demographics in the dataset. While training a model on a downstream task, BLIND detects biased samples using an auxiliary model that predicts the main model’s success, and down-weights those samples during the training process. Experiments with racial and gender biases in sentiment classification and occupation classification tasks demonstrate that BLIND mitigates social biases without relying on a costly demographic annotation process. Our method is competitive with other methods that require demographic information and sometimes even surpasses them.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[ACL]</abbr>


<div id="iskander:2023:acl:igbp">
  
    <span class="title">Shielded Representations: Protecting Sensitive Attributes Through
Iterative Gradient-Based Projection.</span>
    <span class="author">
      
        
          
          
            
              Shadi Iskander,
            
          
        
      
        
          
          
            
              Kira Radinsky,
            
          
        
      
        
          
          
          
            <em>Yonatan Belinkov</em>
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Findings of the Association for Computational Linguistics (ACL)</em>
    
    
      2023
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/acl2023-findings.pdf" target="_blank">PDF</a>]
  
  
  
  
  
    [<a href="https://github.com/technion-cs-nlp/igbp_nonlinear-removal" target="_blank">Code</a>]
  
  
    [<a href="https://arxiv.org/abs/2305.10204" target="_blank">Arxiv</a>]
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Natural language processing models tend to learn and encode social biases present in the data. One popular approach for addressing such biases is to eliminate encoded information from the model’s representations. However, current methods are restricted to removing only linearly encoded information. In this work, we propose Iterative Gradient-Based Projection (IGBP), a novel method for removing non-linear encoded concepts from neural representations. Our method consists of iteratively training neural classifiers to predict a particular attribute we seek to eliminate, followed by a projection of the representation on a hypersurface, such that the classifiers become oblivious to the target attribute. We evaluate the effectiveness of our method on the task of removing gender and race information as sensitive attributes. Our results demonstrate that IGBP is effective in mitigating bias through intrinsic and extrinsic evaluations, with minimal impact on downstream task accuracy.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[ACL]</abbr>


<div id="ratner:2023:pcw">
  
    <span class="title">Parallel Context Windows for Large Language Models.</span>
    <span class="author">
      
        
          
          
            
              Nir Ratner,
            
          
        
      
        
          
          
            
              Yoav Levine,
            
          
        
      
        
          
          
            <em>Yonatan Belinkov</em>,
          
        
      
        
          
          
            
              Ori Ram,
            
          
        
      
        
          
          
            
              Omri Abend,
            
          
        
      
        
          
          
            
              Udi Karpas,
            
          
        
      
        
          
          
            
              Amnon Shashua,
            
          
        
      
        
          
          
            
              Kevin Leyton-Brown,
            
          
        
      
        
          
          
          
            
              Yoav Shoham
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (ACL)</em>
    
    
      2023
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/acl2023-pcw.pdf" target="_blank">PDF</a>]
  
  
  
  
  
    [<a href="https://github.com/ai21labs/parallel-context-windows" target="_blank">Code</a>]
  
  
    [<a href="https://arxiv.org/abs/2212.10947" target="_blank">Arxiv</a>]
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>When applied for processing long text, Large Language Models (LLMs) are limited by their context window. Existing efforts to address this limitation involve training specialized architectures, and cannot be easily applied to off-the-shelf LLMs. We present Parallel Context Windows (PCW), a method that alleviates the context window restriction for any off-the-shelf LLM without further training. The key to the approach is to carve a long context into chunks (“windows”), restrict the attention mechanism to apply only within each window, and re-use the positional embeddings across the windows. Our main results test the PCW approach on in-context learning with models that range in size between 750 million and 178 billion parameters, and show substantial improvements for tasks with diverse input and output spaces. We show additional benefits in other settings where long context windows may be beneficial: multi-hop questions and retrieval-augmented question answering with multiple retrieved documents. Our results highlight Parallel Context Windows as a promising method for applying off-the-shelf LLMs in a range of settings that require long text sequences. We make our code publicly available at https://github.com/ai21labs/parallel-context-windows.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[ICLR]</abbr>


<div id="meng:2023:memit">
  
    <span class="title">Mass-Editing Memory in a Transformer.</span>
    <span class="author">
      
        
          
          
            
              Kevin Meng,
            
          
        
      
        
          
          
            
              Arnab Sen Sharma,
            
          
        
      
        
          
          
            
              Alex Andonian,
            
          
        
      
        
          
          
            <em>Yonatan Belinkov</em>,
          
        
      
        
          
          
          
            
              <a href="https://baulab.info" target="_blank">David Bau</a>
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In International Conference on Learning Representations (ICLR, notable top-25%)</em>
    
    
      2023
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/iclr2023-memit.pdf" target="_blank">PDF</a>]
  
  
  
  
  
  
    [<a href="https://arxiv.org/abs/2210.07229" target="_blank">Arxiv</a>]
  
  
    [<a href="http://memit.baulab.info" target="_blank">URL</a>]
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Recent work has shown exciting promise in updating large language models with new memories, so as to replace obsolete information or add specialized knowledge. However, this line of work is predominantly limited to updating single associations. We develop MEMIT, a method for directly updating a language model with many memories, demonstrating experimentally that it can scale up to thousands of associations for GPT-J (6B) and GPT-NeoX (20B), exceeding prior work by an order of magnitude. Our code and data will be open-sourced upon publication.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[ICLR]</abbr>


<div id="dotan:2023:iclr">
  
    <span class="title">Multiple sequence alignment as a sequence-to-sequence learning problem.</span>
    <span class="author">
      
        
          
          
            
              Edo Dotan,
            
          
        
      
        
          
          
            <em>Yonatan Belinkov</em>,
          
        
      
        
          
          
            
              Oren Avram,
            
          
        
      
        
          
          
            
              Elya Wygoda,
            
          
        
      
        
          
          
            
              Noa Ecker,
            
          
        
      
        
          
          
            
              Michael Alburquerque,
            
          
        
      
        
          
          
            
              Omri Keren,
            
          
        
      
        
          
          
            
              Gil Loewenthal,
            
          
        
      
        
          
          
          
            
              Tal Pupko
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In International Conference on Learning Representations (ICLR)</em>
    
    
      2023
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/iclr2023-msa.pdf" target="_blank">PDF</a>]
  
  
  
  
  
  
    [<a href="https://www.biorxiv.org/content/10.1101/2022.07.22.501063" target="_blank">Arxiv</a>]
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>The sequence alignment problem is one of the most fundamental problems in bioinformatics and a plethora of methods were devised to tackle it. Here we introduce BetaAlign, a methodology for aligning sequences using an NLP approach. BetaAlign accounts for the possible variability of the evolutionary process among different datasets by using an ensemble of transformers, each trained on millions of samples generated from a different evolutionary model. Our approach leads to alignment accuracy that is similar and often better than commonly used methods, such as MAFFT, DIALIGN, ClustalW, T-Coffee, PRANK, and MUSCLE.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[AAAI]</abbr>


<div id="carmeli:2023:AAAI">
  
    <span class="title">Emergent Quantized Communication.</span>
    <span class="author">
      
        
          
          
            
              Boaz Carmeli,
            
          
        
      
        
          
          
            
              Ron Meir,
            
          
        
      
        
          
          
          
            <em>Yonatan Belinkov</em>
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Proceedings of the Thirty-Seventh AAAI Conference on Artificial Intelligence (AAAI)</em>
    
    
      2023
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/aaai2023.pdf" target="_blank">PDF</a>]
  
  
  
    [<a href="https://belinkov.com/assets/pdf/aaai2023-poster.pdf" target="_blank">Poster</a>]
  
  
  
  
    [<a href="http://arxiv.org/abs/2211.02412" target="_blank">Arxiv</a>]
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>The field of emergent communication aims to understand the characteristics of communication as it emerges from artificial agents solving tasks that require information exchange. Communication with discrete messages is considered a desired characteristic, for both scientific and applied reasons. However, training a multi-agent system with discrete communication is not straightforward, requiring either reinforcement learning algorithms or relaxing the discreteness requirement via a continuous approximation such as the Gumbel-softmax. Both these solutions result in poor performance compared to fully continuous communication. In this work, we propose an alternative approach to achieve discrete communication – quantization of communicated messages. Using message quantization allows us to train the model end-to-end, achieving superior performance in multiple setups. Moreover, quantization is a natural framework that runs the gamut from continuous to discrete communication. Thus, it sets the ground for a broader view of multi-agent communication in the deep learning era.</p>
  </span>
  
</div>
</li></ol>
<h2 class="bibliography">2022</h2>
<ol class="bibliography"><li>
  <abbr>[NEJLT]</abbr>


<div id="tirosh-becker:2022">
  
    <span class="title">Part-of-Speech and Morphological Tagging of Algerian Judeo-Arabic.</span>
    <span class="author">
      
        
          
          
            
              Ofra Tirosh-Becker*,
            
          
        
      
        
          
          
            
              Michal Kessler*,
            
          
        
      
        
          
          
            
              Oren Becker,
            
          
        
      
        
          
          
          
            <em>Yonatan Belinkov</em>
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>The Northern European Journal of Language Technology (NEJLT)</em>
    
    
      2022
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/nejlt2022.pdf" target="_blank">PDF</a>]
  
  
  
  
  
    [<a href="https://github.com/technion-cs-nlp/nlp4aja" target="_blank">Code</a>]
  
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Most linguistic studies of Judeo-Arabic, the ensemble of dialects spoken and written by Jews in Arab lands, are qualitative in nature and rely on laborious manual annotation work, and are therefore limited in scale. In this work, we develop automatic methods for morpho-syntactic tagging of Algerian Judeo-Arabic texts published by Algerian Jews in the 19th–20th centuries, based on a linguistically tagged corpus. First, we describe our semi-automatic approach for preprocessing these texts. Then, we experiment with both an off-the-shelf morphological tagger and several specially designed neural network taggers. Finally, we perform a real-world evaluation of new texts that were never tagged before in comparison with human expert annotators. Our experimental results demonstrate that these methods can dramatically speed up and improve the linguistic research pipeline, enabling linguists to study these dialects on a much greater scale.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[EMNLP]</abbr>


<div id="zaman2022nli">
  
    <span class="title">A Multilingual Perspective Towards the Evaluation of Attribution Methods in Natural Language Inference.</span>
    <span class="author">
      
        
          
          
            
              Kerem Zaman,
            
          
        
      
        
          
          
          
            <em>Yonatan Belinkov</em>
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>
    
    
      2022
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
  
  
  
  
    [<a href="https://github.com/KeremZaman/explaiNLI" target="_blank">Code</a>]
  
  
    [<a href="https://arxiv.org/abs/2204.05428" target="_blank">Arxiv</a>]
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Most evaluations of attribution methods focus on the English language. In this work, we present a multilingual approach for evaluating attribution methods for the Natural Language Inference (NLI) task in terms of plausibility and faithfulness properties. First, we introduce a novel cross-lingual strategy to measure faithfulness based on word alignments, which eliminates the potential downsides of erasure-based evaluations. We then perform a comprehensive evaluation of attribution methods, considering different output mechanisms and aggregation methods. Finally, we augment the XNLI dataset with highlight-based explanations, providing a multilingual NLI dataset with highlights, which may support future exNLP studies. Our results show that attribution methods performing best for plausibility and faithfulness are different.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[NeurIPS]</abbr>


<div id="bansal:2022:NeurIPS">
  
    <span class="title">Measures of Information Reflect Memorization Patterns.</span>
    <span class="author">
      
        
          
          
            
              Rachit Bansal,
            
          
        
      
        
          
          
            
              Danish Pruthi,
            
          
        
      
        
          
          
          
            <em>Yonatan Belinkov</em>
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Advances in Neural Information Processing Systems (NeurIPS)</em>
    
    
      2022
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
  
  
  
  
    [<a href="https://github.com/technion-cs-nlp/Information-Reflects-Memorization" target="_blank">Code</a>]
  
  
    [<a href="https://arxiv.org/abs/2210.09404" target="_blank">Arxiv</a>]
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Neural networks are known to exploit spurious artifacts (or shortcuts) that co-occur with a target label, exhibiting heuristic memorization. On the other hand, networks have been shown to memorize training examples, resulting in example-level memorization. These kinds of memorization impede generalization of networks beyond their training distributions. Detecting such memorization could be challenging, often requiring researchers to curate tailored test sets. In this work, we hypothesize—and subsequently show—that the diversity in the activation patterns of different neurons is reflective of model generalization and memorization. We quantify the diversity in the neural activations through information-theoretic measures and find support for our hypothesis on experiments spanning several natural language and vision tasks. Importantly, we discover that information organization points to the two forms of memorization, even for neural activations computed on unlabeled in-distribution examples. Lastly, we demonstrate the utility of our findings for the problem of model selection.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[NeurIPS]</abbr>


<div id="meng:2022:NeurIPS">
  
    <span class="title">Locating and Editing Factual Associations in GPT.</span>
    <span class="author">
      
        
          
          
            
              Kevin Meng*,
            
          
        
      
        
          
          
            
              <a href="https://baulab.info" target="_blank">David Bau*</a>,
            
          
        
      
        
          
          
            
              Alex Andonian,
            
          
        
      
        
          
          
          
            <em>Yonatan Belinkov</em>
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Advances in Neural Information Processing Systems (NeurIPS)</em>
    
    
      2022
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/neurips2022-rome.pdf" target="_blank">PDF</a>]
  
  
  
  
  
    [<a href="https://github.com/kmeng01/rome" target="_blank">Code</a>]
  
  
    [<a href="https://arxiv.org/abs/2202.05262" target="_blank">Arxiv</a>]
  
  
    [<a href="http://rome.baulab.info" target="_blank">URL</a>]
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>We analyze the storage and recall of factual associations in autoregressive transformer language models, finding evidence that these associations correspond to localized, directly-editable computations. We first develop a causal intervention for identifying neuron activations that are decisive in a model’s factual predictions. This reveals a distinct set of steps in middle-layer feed-forward modules that mediate factual predictions while processing subject tokens. To test our hypothesis that these computations correspond to factual association recall, we modify feed-forward weights to update specific factual associations using Rank-One Model Editing (ROME).  We find that ROME is effective on a standard zero-shot relation extraction (zsRE) model-editing task, comparable to existing methods. To perform a more sensitive evaluation, we also evaluate ROME on a new dataset of counterfactual assertions, on which it simultaneously maintains both specificity and generalization, whereas other methods sacrifice one or another. Our results confirm an important role for mid-layer feed-forward modules in storing factual associations and suggest that direct manipulation of computational mechanisms may be a feasible approach for model editing. The code, dataset, visualizations, and an interactive demo notebook are available in the supplemental materials.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[NAACL]</abbr>


<div id="orgad:2022:NAACL">
  
    <span class="title">How Gender Debiasing Affects Internal Model Representations, and Why It Matters.</span>
    <span class="author">
      
        
          
          
            
              <a href="https://orgadhadas.github.io" target="_blank">Hadas Orgad</a>,
            
          
        
      
        
          
          
            
              Seraphina Goldfarb-Tarrant,
            
          
        
      
        
          
          
          
            <em>Yonatan Belinkov</em>
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)</em>
    
    
      2022
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/naacl2022.pdf" target="_blank">PDF</a>]
  
  
  
  
  
    [<a href="https://github.com/technion-cs-nlp/gender_internal" target="_blank">Code</a>]
  
  
    [<a href="https://arxiv.org/abs/2204.06827" target="_blank">Arxiv</a>]
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Common studies of gender bias in NLP focus either on extrinsic bias measured by model performance on a downstream task or on intrinsic bias found in models’ internal representations. However, the relationship between extrinsic and intrinsic bias is relatively unknown. In this work, we illuminate this relationship by measuring both quantities together: we debias a model during downstream fine-tuning, which reduces extrinsic bias, and measure the effect on intrinsic bias, which is operationalized as bias extractability with information-theoretic probing. Through experiments on two tasks and multiple bias metrics, we show that our intrinsic bias metric is a better indicator of debiasing than (a contextual adaptation of) the standard WEAT metric, and can also expose cases of superficial debiasing. Our framework provides a comprehensive perspective on bias in NLP models, which can be applied to deploy NLP systems in a more informed manner. Our code will be made publicly available.</p>
  </span>
  
</div>
</li>
<li>

<div id="antverg:2022:DeepLoNLP">
  
    <span class="title">IDANI: Inference-time Domain Adaptation via Neuron-level Interventions.</span>
    <span class="author">
      
        
          
          
            
              Omer Antverg,
            
          
        
      
        
          
          
            
              Eyal Ben-David,
            
          
        
      
        
          
          
          
            <em>Yonatan Belinkov</em>
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Proceedings of the Third Workshop on Deep Learning for Low-Resource NLP (DeepLoNLP)</em>
    
    
      2022
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/deeplo2022.pdf" target="_blank">PDF</a>]
  
  
  
    [<a href="https://belinkov.com/assets/pdf/deeplo2022-poster.pdf" target="_blank">Poster</a>]
  
  
  
    [<a href="https://github.com/technion-cs-nlp/idani" target="_blank">Code</a>]
  
  
    [<a href="https://arxiv.org/abs/2206.00259" target="_blank">Arxiv</a>]
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Large pre-trained models are usually fine-tuned on downstream task data, and tested on unseen data. When the train and test data come from different domains, the model is likely to struggle, as it is not adapted to the test domain. We propose a new approach for domain adaptation (DA), using neuron-level interventions: We modify the representation of each test example in specific neurons, resulting in a counterfactual example from the source domain, which the model is more familiar with. The modified example is then fed back into the model. While most other DA methods are applied during training time, ours is applied during inference only, making it more efficient and applicable. Our experiments show that our method improves performance on unseen domains.</p>
  </span>
  
</div>
</li>
<li>

<div id="orgad:2022:GeBNLP">
  
    <span class="title">Choose Your Lenses: Flaws in Gender Bias Evaluation.</span>
    <span class="author">
      
        
          
          
            
              <a href="https://orgadhadas.github.io" target="_blank">Hadas Orgad</a>,
            
          
        
      
        
          
          
          
            <em>Yonatan Belinkov</em>
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Proceedings of the Fourth Workshop on Gender Bias in NLP (GeBNLP)</em>
    
    
      2022
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/genbnlp2022.pdf" target="_blank">PDF</a>]
  
  
  
  
  
  
    [<a href="https://arxiv.org/abs/2210.11471" target="_blank">Arxiv</a>]
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Considerable efforts to measure and mitigate gender bias in recent years have led to the introduction of an abundance of tasks, datasets, and metrics used in this vein. 
In this position paper, we assess the current paradigm of gender bias evaluation and identify several flaws in it. First, we highlight the importance of extrinsic bias metrics that measure how a model’s performance on some task is affected by gender, as opposed to intrinsic evaluations of model representations, which are less strongly connected to specific harms to people interacting with systems. Second, we find that datasets and metrics are often coupled, and discuss how their coupling hinders the ability to obtain reliable conclusions, and how one may decouple them. We then investigate the effect of the chosen dataset or metric on bias measurement, finding significant variations across each of them. Finally, we propose several guidelines for more reliable gender bias evaluation.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[*SEM]</abbr>


<div id="asael:2022:starsem">
  
    <span class="title">A Generative Approach for Mitigating Structural Biases in Natural Language Inference.</span>
    <span class="author">
      
        
          
          
            
              Dimion Asael,
            
          
        
      
        
          
          
            
              Zachary Ziegler,
            
          
        
      
        
          
          
          
            <em>Yonatan Belinkov</em>
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Proceedings of the Eleventh Joint Conference on Lexical and Computational Semantics (*SEM)</em>
    
    
      2022
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/starsem2022.pdf" target="_blank">PDF</a>]
  
  
  
  
  
    [<a href="https://github.com/technion-cs-nlp/Generative-NLI" target="_blank">Code</a>]
  
  
    [<a href="https://arxiv.org/abs/2108.14006" target="_blank">Arxiv</a>]
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Many natural language inference (NLI) datasets contain biases that allow models to perform well by only using a biased subset of the input, without considering the remainder features. For instance, models are able to classify samples by only using the hypothesis, without learning the true relationship between it and the premise. These structural biases lead discriminative models to learn unintended superficial features and generalize poorly out of the training distribution. In this work, we reformulate NLI as a generative task, where a model is conditioned on the biased subset of the input and the label and generates the remaining subset of the input. We show that by imposing a uniform prior, we obtain a provably unbiased model. Through synthetic experiments, we find this approach to be highly robust to large amounts of bias. We then demonstrate empirically on two types of natural bias that this approach leads to fully unbiased models in practice. However, we find that generative models are difficult to train and generally perform worse than discriminative baselines. We highlight the difficulty of the generative modeling task in the context of NLI as a cause for this worse performance. Finally, by fine-tuning the generative model with a discriminative objective, we reduce the performance gap between the generative model and the discriminative baseline, while allowing for a small amount of bias.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[ICLR]</abbr>


<div id="antverg:2022:iclr">
  
    <span class="title">On the Pitfalls of Analyzing Individual Neurons in Language Models.</span>
    <span class="author">
      
        
          
          
            
              Omer Antverg,
            
          
        
      
        
          
          
          
            <em>Yonatan Belinkov</em>
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In International Conference on Learning Representations (ICLR)</em>
    
    
      2022
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/iclr2022.pdf" target="_blank">PDF</a>]
  
  
  
    [<a href="https://belinkov.com/assets/pdf/iclr2022-poster.pdf" target="_blank">Poster</a>]
  
  
  
    [<a href="https://github.com/technion-cs-nlp/Individual-Neurons-Pitfalls" target="_blank">Code</a>]
  
  
    [<a href="https://arxiv.org/abs/2110.07483" target="_blank">Arxiv</a>]
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>While many studies have shown that linguistic information is encoded in hidden word representations, few have studied individual neurons, to show how and in which neurons it is encoded. Among these, the common approach is to use an external probe to rank neurons according to their relevance to some linguistic attribute, and to evaluate the obtained ranking using the same probe that produced it. We show two pitfalls in this methodology: 1. It confounds distinct factors: probe quality and ranking quality. We separate them and draw conclusions on each. 2. It focuses on encoded information, rather than information that is used by the model. We show that these are not the same. We compare two recent ranking methods and a simple one we introduce, and evaluate them with regard to both of these aspects.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[CL]</abbr>


<div id="belinkov:2022:cl">
  
    <span class="title">Probing Classifiers: Promises, Shortcomings, and Advances.</span>
    <span class="author">
      
        
          
          
          
            <em>Yonatan Belinkov</em>
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>Computational Linguistics</em>
    
    
      2022
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/cl2021.pdf" target="_blank">PDF</a>]
  
  
  
  
  
  
    [<a href="https://arxiv.org/abs/2102.12452" target="_blank">Arxiv</a>]
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Probing classifiers have emerged as one of the prominent methodologies for interpreting and analyzing deep neural network models of natural language processing. The basic idea is simple — a classifier is trained to predict some linguistic property from a model’s representations — and has been used to examine a wide variety of models and properties. However, recent studies have demonstrated various methodological limitations of this approach. This article critically reviews the probing classifiers framework, highlighting their promises, shortcomings, and advances.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[AAAI]</abbr>


<div id="stacey:2022:AAAI">
  
    <span class="title">Supervising Model Attention with Human Explanations for Robust Natural Language Inference.</span>
    <span class="author">
      
        
          
          
            
              Joe Stacey,
            
          
        
      
        
          
          
            <em>Yonatan Belinkov</em>,
          
        
      
        
          
          
          
            
              Marek Rei
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Proceedings of the Thirty-Sixth AAAI Conference on Artificial Intelligence (AAAI)</em>
    
    
      2022
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/aaai2022.pdf" target="_blank">PDF</a>]
  
  
  
  
  
    [<a href="https://github.com/joestacey/NLI_with_a_human_touch" target="_blank">Code</a>]
  
  
    [<a href="https://arxiv.org/abs/2104.08142" target="_blank">Arxiv</a>]
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Natural Language Inference (NLI) models are known to learn from biases and artefacts within their training data, impacting how well they generalise to other unseen datasets. Existing de-biasing approaches focus on preventing the models from learning these biases, which can result in restrictive models and lower performance. We instead investigate teaching the model how a human would approach the NLI task, in order to learn features that will generalise better to previously unseen examples. Using natural language explanations, we supervise the model’s attention weights to encourage more attention to be paid to the words present in the explanations, significantly improving model performance. Our experiments show that the in-distribution improvements of this method are also accompanied by out-of-distribution improvements, with the supervised models learning from features that generalise better to other NLI datasets. Analysis of the model indicates that human explanations encourage increased attention on the important words, with more attention paid to words in the premise and less attention paid to punctuation and stop-words.</p>
  </span>
  
</div>
</li>
<li>

<div id="belinkov:2022:aima">
  
    <span class="title">Large-Scale Electronic Corpora and the Study of Middle and Mixed Arabic.</span>
    <span class="author">
      
        
          
          
          
            <em>Yonatan Belinkov</em>
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Middle and Mixed Arabic over Time and across Written and Oral Genres: From Legal Documents to Television and Internet through Literature. Proceedings of the IVth AIMA International Conference (Emory University, Atlanta, GA, USA, 12–15 October 2013)</em>
    
    
      2022
    
    </span>
    

    
  

  

  <span class="links">
  
  
    [<a href="https://belinkov.com/assets/pdf/aima2021.pdf" target="_blank">PDF</a>]
  
  
  
  
  
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
</div>
</li></ol>
<h2 class="bibliography">2021</h2>
<ol class="bibliography"><li>
  <abbr>[NeurIPS]</abbr>


<div id="dranker:2021:neurips">
  
    <span class="title">IRM—when it works and when it doesn’t: A test case of natural language inference.</span>
    <span class="author">
      
        
          
          
            
              Yana Dranker,
            
          
        
      
        
          
          
            
              He He,
            
          
        
      
        
          
          
          
            <em>Yonatan Belinkov</em>
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Advances in Neural Information Processing Systems (NeurIPS)</em>
    
    
      2021
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/neurips2021.pdf" target="_blank">PDF</a>]
  
  
  
    [<a href="https://belinkov.com/assets/pdf/neurips2021-poster.pdf" target="_blank">Poster</a>]
  
  
  
    [<a href="https://github.com/technion-cs-nlp/irm-for-nli" target="_blank">Code</a>]
  
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Invariant Risk Minimization (IRM) is a recently proposed framework for out- of-distribution (o.o.d) generalization. Most of the studies on IRM so far have focused on theoretical results, toy problems, and simple models. In this work, we investigate the applicability of IRM to bias mitigation—a special case of o.o.d generalization—in increasingly naturalistic settings and deep models. Using natural language inference (NLI) as a test case, we start with a setting where both the dataset and the bias are synthetic, continue with a natural dataset and synthetic bias, and end with a fully realistic setting with natural datasets and bias. Our results show that in naturalistic settings, learning complex features in place of the bias proves to be difficult, leading to a rather small improvement over empirical risk minimization. Moreover, we find that in addition to being sensitive to random seeds, the performance of IRM also depends on several critical factors, notably dataset size, bias prevalence, and bias strength, thus limiting IRM’s advantage in practical scenarios. Our results highlight key challenges in applying IRM to real-world scenarios, calling for a more naturalistic characterization of the problem setup for o.o.d generalization.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[EMNLP]</abbr>


<div id="Mendelson:2021:emnlp">
  
    <span class="title">Debiasing Methods in Natural Language Understanding Make Bias More Accessible.</span>
    <span class="author">
      
        
          
          
            
              Michael Mendelson,
            
          
        
      
        
          
          
          
            <em>Yonatan Belinkov</em>
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>
    
    
      2021
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/emnlp2021.pdf" target="_blank">PDF</a>]
  
  
  
  
  
    [<a href="https://github.com/technion-cs-nlp/bias-probing" target="_blank">Code</a>]
  
  
    [<a href="https://arxiv.org/abs/2109.04095" target="_blank">Arxiv</a>]
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Model robustness to bias is often determined by the generalization on carefully designed out-of-distribution datasets. Recent debiasing methods in natural language understanding (NLU) improve performance on such datasets by pressuring models into making unbiased predictions. An underlying assumption behind such methods is that this also leads to the discovery of more robust features in the model’s inner representations. We propose a general probing-based framework that allows for post-hoc interpretation of biases in language models, and use an information-theoretic approach to measure the extractability of certain biases from the model’s representations. We experiment with several NLU datasets and known biases, and show that, counter-intuitively, the more a language model is pushed towards a debiased regime, the more bias is actually encoded in its inner representations.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[ACL]</abbr>


<div id="finlayson-meuller:2021:acl">
  
    <span class="title">Causal Analysis of Syntactic Agreement Mechanisms in Neural Language Models.</span>
    <span class="author">
      
        
          
          
            
              Matthew Finlayson*,
            
          
        
      
        
          
          
            
              Aaron Mueller*,
            
          
        
      
        
          
          
            
              <a href="https://www.sebastiangehrmann.com" target="_blank">Sebastian Gehrmann</a>,
            
          
        
      
        
          
          
            
              <a href="http://www.eecs.harvard.edu/~shieber/index.html" target="_blank">Stuart Shieber</a>,
            
          
        
      
        
          
          
            
              Tal Linzen,
            
          
        
      
        
          
          
          
            <em>Yonatan Belinkov</em>
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Proceedings of the Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (ACL-IJCNLP)</em>
    
    
      2021
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/acl2021.pdf" target="_blank">PDF</a>]
  
  
  
  
  
    [<a href="https://github.com/mattf1n/lm- intervention" target="_blank">Code</a>]
  
  
    [<a href="http://arxiv.org/abs/2106.06087" target="_blank">Arxiv</a>]
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Targeted syntactic evaluations have demonstrated the ability of language models to perform subject-verb agreement given difficult contexts. To elucidate the mechanisms by which the models accomplish this behavior, this study applies causal mediation analysis to pre-trained neural language models. We investigate the magnitude of models’ preferences for grammatical inflections, as well as whether neurons process subject-verb agreement similarly across sentences with different syntactic structures. We uncover similarities and differences across architectures and model sizes—notably, that larger models do not necessarily learn stronger preferences. We also observe two distinct mechanisms for producing subject-verb agreement depending on the syntactic structure of the input sentence. Finally, we find that language models rely on similar sets of neurons when given sentences with similar syntactic structure.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[ICASSP]</abbr>


<div id="chung:2021:icassp">
  
    <span class="title">Similarity Analysis of Self-Supervised Speech Representations.</span>
    <span class="author">
      
        
          
          
            
              Yu-An Chung,
            
          
        
      
        
          
          
            <em>Yonatan Belinkov</em>,
          
        
      
        
          
          
          
            
              <a href="http://people.csail.mit.edu/jrg/" target="_blank">James Glass</a>
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>
    
    
      2021
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/icassp2021.pdf" target="_blank">PDF</a>]
  
  
  
  
  
  
    [<a href="http://arxiv.org/abs/2010.11481" target="_blank">Arxiv</a>]
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Self-supervised speech representation learning has recently been a prosperous research topic. Many algorithms have been proposed for learning useful representations from large-scale unlabeled data, and their applications to a wide range of speech tasks have also been investigated. However, there has been little research focusing on understanding the properties of existing approaches. In this work, we aim to provide a comparative study of some of the most representative self-supervised algorithms. Specifically, we quantify the similarities between different self-supervised representations using existing similarity measures. We also design probing tasks to study the correlation between the models’ pre-training loss and the amount of specific speech information contained in their learned representations. In addition to showing how various self-supervised models behave differently given the same input, our study also finds that the training objective has a higher impact on representation similarity than architectural choices such as building blocks (RNN/Transformer/CNN) and directionality (uni/bidirectional). Our results also suggest that there exists a strong correlation between pre-training loss and downstream performance for some self-supervised algorithms.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[ICLR]</abbr>


<div id="sanh:2021:iclr">
  
    <span class="title">Learning from others’ mistakes: Avoiding dataset biases without modeling them.</span>
    <span class="author">
      
        
          
          
            
              Victor Sanh,
            
          
        
      
        
          
          
            
              Thomas Wolf,
            
          
        
      
        
          
          
            <em>Yonatan Belinkov</em>,
          
        
      
        
          
          
          
            
              Alexander M. Rush
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In International Conference on Learning Representations (ICLR)</em>
    
    
      2021
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/iclr2021-implicit.pdf" target="_blank">PDF</a>]
  
  
  
  
  
  
    [<a href="https://arxiv.org/abs/2012.01300" target="_blank">Arxiv</a>]
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>State-of-the-art natural language processing (NLP) models often learn to model dataset biases and surface form correlations instead of features that target the intended underlying task. Previous work has demonstrated effective methods to circumvent these issues when knowledge of the bias is available. We consider cases where the bias issues may not be explicitly identified, and show a method for training models that learn to ignore these problematic correlations. Our approach relies on the observation that models with limited capacity primarily learn to exploit biases in the dataset. We can leverage the errors of such limited capacity models to train a more robust model in a product of experts, thus bypassing the need to hand-craft a biased model. We show the effectiveness of this method to retain improvements in out-of-distribution settings even if no particular bias is targeted by the biased model.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[ICLR]</abbr>


<div id="mahabadi:2021:iclr">
  
    <span class="title">Variational Information Bottleneck for Effective Low-Resource Fine-Tuning.</span>
    <span class="author">
      
        
          
          
            
              Rabeeh Karimi Mahabadi,
            
          
        
      
        
          
          
            <em>Yonatan Belinkov</em>,
          
        
      
        
          
          
          
            
              James Henderson
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In International Conference on Learning Representations (ICLR)</em>
    
    
      2021
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/iclr2021-vib.pdf" target="_blank">PDF</a>]
  
  
  
  
  
    [<a href="https://github.com/rabeehk/vibert" target="_blank">Code</a>]
  
  
    [<a href="https://arxiv.org/abs/2106.05469" target="_blank">Arxiv</a>]
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>While large-scale pretrained language models have obtained impressive results when fine-tuned on a wide variety of tasks, they still often suffer from overfitting in low-resource scenarios. Since such models are general-purpose feature extractors, many of these features are inevitably irrelevant for a given target task.  We propose to use Variational Information Bottleneck (VIB) to suppress irrelevant features when fine-tuning on low-resource target tasks, and show that our method successfully reduces overfitting.  Moreover, we show that our VIB model finds sentence representations that are more robust to biases in natural language inference datasets, and thereby obtains better generalization to out-of-domain datasets. Evaluation on seven low-resource datasets in different tasks shows that our method significantly improves transfer learning in low-resource scenarios, surpassing prior work. Moreover, it improves generalization on 13 out of 15 out-of-domain natural language inference benchmarks.  Our code is publicly available in https://github.com/rabeehk/vibert.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[EACL]</abbr>


<div id="ravichander:2021:eacl">
  
    <span class="title">Probing the Probing Paradigm: Does Probing Accuracy Entail Task Relevance?.</span>
    <span class="author">
      
        
          
          
            
              Abhilasha Ravichander,
            
          
        
      
        
          
          
            <em>Yonatan Belinkov</em>,
          
        
      
        
          
          
          
            
              Eduard Hovy
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics (EACL)</em>
    
    
      2021
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/eacl2021.pdf" target="_blank">PDF</a>]
  
  
  
  
  
  
    [<a href="https://arxiv.org/abs/2005.00719" target="_blank">Arxiv</a>]
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Although neural models have achieved impressive results on several NLP benchmarks, little is understood about the mechanisms they use to perform language tasks. Thus, much recent attention has been devoted to analyzing the sentence representations learned by neural encoders, through the lens of ‘probing’ tasks. However, to what extent was the information encoded in sentence representations, as discovered through a probe, actually used by the model to perform its task? In this work, we examine this probing paradigm through a case study in Natural Language Inference, showing that models can learn to encode linguistic properties even if they are not needed for the task on which the model was trained. We further identify that pretrained word embeddings play a considerable role in encoding these properties rather than the training task itself, highlighting the importance of careful controls when designing probing experiments. Finally, through a set of controlled synthetic tasks, we demonstrate models can encode these properties considerably above chance-level even when distributed in the data as random noise, calling into question the interpretation of absolute claims on probing tasks.</p>
  </span>
  
</div>
</li></ol>
<h2 class="bibliography">2020</h2>
<ol class="bibliography"><li>
  <abbr>[NeurIPS]</abbr>


<div id="vig:2020:neurips">
  
    <span class="title">Investigating Gender Bias in Language Models Using Causal Mediation Analysis.</span>
    <span class="author">
      
        
          
          
            
              Jesse Vig*,
            
          
        
      
        
          
          
            
              <a href="https://www.sebastiangehrmann.com" target="_blank">Sebastian Gehrmann*</a>,
            
          
        
      
        
          
          
            <em>Yonatan Belinkov*</em>,
          
        
      
        
          
          
            
              Sharon Qian,
            
          
        
      
        
          
          
            
              Daniel Nevo,
            
          
        
      
        
          
          
            
              Yaron Singer,
            
          
        
      
        
          
          
          
            
              <a href="http://www.eecs.harvard.edu/~shieber/index.html" target="_blank">Stuart Shieber</a>
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Advances in Neural Information Processing Systems (NeurIPS, Spotlight presentation)</em>
    
    
      2020
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/neurips2020.pdf" target="_blank">PDF</a>]
  
  
  
  
  
    [<a href="https://github.com/sebastianGehrmann/CausalMediationAnalysis" target="_blank">Code</a>]
  
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Many interpretation methods for neural models in natural language processing investigate how information is encoded inside hidden representations. However, these methods can only measure whether the information exists, not whether it is actually used by the model.  We propose a methodology grounded in the theory of causal mediation analysis for interpreting which parts of a model are causally implicated in its behavior. The approach enables us to analyze the mechanisms which facilitate the flow of information from input to output through various model components, known as mediators. As a case study, we apply this methodology to analyzing gender bias in pre-trained Transformer language models. We study the role of individual neurons and attention heads in mediating gender bias across three datasets designed to gauge a model’s sensitivity to gender bias. Our mediation analysis reveals that gender bias effects are concentrated in specific components of the model that may exhibit highly specialized behavior.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[WMT]</abbr>


<div id="li-etal-2019-findingt">
  
    <span class="title">Findings of the WMT 2020 Shared Task on Machine Translation Robustness.</span>
    <span class="author">
      
        
          
          
            
              Lucia Specia,
            
          
        
      
        
          
          
            
              Zhenhao Li,
            
          
        
      
        
          
          
            
              Juan Pino,
            
          
        
      
        
          
          
            
              Vishrav Chaudhary,
            
          
        
      
        
          
          
            
              Guzmán Guzman,
            
          
        
      
        
          
          
            
              Graham Neubig,
            
          
        
      
        
          
          
            
              <a href="http://alt.qcri.org/~ndurrani/" target="_blank">Nadir Durrani</a>,
            
          
        
      
        
          
          
            <em>Yonatan Belinkov</em>,
          
        
      
        
          
          
            
              Philipp Koehn,
            
          
        
      
        
          
          
            
              <a href="https://hsajjad.github.io" target="_blank">Hassan Sajjad</a>,
            
          
        
      
        
          
          
            
              Paul Michel,
            
          
        
      
        
          
          
          
            
              Xian Li
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Proceedings of the Fifth Conference on Machine Translation</em>
    
    
      2020
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/wmt2020-robustness.pdf" target="_blank">PDF</a>]
  
  
  
  
  
  
  
    [<a href="http://www.statmt.org/wmt20/robustness.html" target="_blank">URL</a>]
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>We report the findings of the second edition of the shared task on improving robustness in Machine Translation (MT). The task aims to test current machine translation systems in their ability to handle challenges facing MT models to be deployed in the real world, including domain diversity and non-standard texts common in user generated content, especially in social media. We cover two language pairs – English-German and English-Japanese and provide test sets in zero-shot and few-shot variants. Participating systems are evaluated both automatically and manually, with an additional human evaluation for “catastrophic errors”. We received 59 submissions by 11 participating teams from a variety of types of institutions.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[EMNLP]</abbr>


<div id="durrani:2020:emnlp">
  
    <span class="title">Analyzing Individual Neurons in Pre-trained Language Models.</span>
    <span class="author">
      
        
          
          
            
              <a href="http://alt.qcri.org/~ndurrani/" target="_blank">Nadir Durrani</a>,
            
          
        
      
        
          
          
            
              <a href="https://hsajjad.github.io" target="_blank">Hassan Sajjad</a>,
            
          
        
      
        
          
          
            
              <a href="https://fdalvi.github.io" target="_blank">Fahim Dalvi</a>,
            
          
        
      
        
          
          
          
            <em>Yonatan Belinkov</em>
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>
    
    
      2020
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/emnlp2020-neurons.pdf" target="_blank">PDF</a>]
  
  
  
  
  
  
    [<a href="https://arxiv.org/abs/2010.02695" target="_blank">Arxiv</a>]
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>While a lot of analysis has been carried to demonstrate linguistic knowledge captured by the representations learned within deep NLP models, very little attention has been paid towards individual neurons. We carry out a neuron-level analysis using core linguistic tasks of predicting morphology, syntax and semantics, on pre-trained language models, with questions like: i) do individual neurons in pre-trained models capture linguistic information? ii) which parts of the network learn more about certain linguistic phenomena? iii) how distributed or focused is the information? and iv) how do various architectures differ in learning these properties? We found small subsets of neurons to predict linguistic tasks, with lower level tasks (such as morphology) localized in fewer neurons, compared to higher level task of predicting syntax. Our study also reveals interesting cross architectural comparisons. For example, we found neurons in XLNet to be more localized and disjoint when predicting properties compared to BERT and others, where they are more distributed and coupled.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[EMNLP]</abbr>


<div id="dalvi:2020:emnlp">
  
    <span class="title">Analyzing Redundancy in Pretrained Transformer Models.</span>
    <span class="author">
      
        
          
          
            
              <a href="https://fdalvi.github.io" target="_blank">Fahim Dalvi</a>,
            
          
        
      
        
          
          
            
              <a href="https://hsajjad.github.io" target="_blank">Hassan Sajjad</a>,
            
          
        
      
        
          
          
            
              <a href="http://alt.qcri.org/~ndurrani/" target="_blank">Nadir Durrani</a>,
            
          
        
      
        
          
          
          
            <em>Yonatan Belinkov</em>
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>
    
    
      2020
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/emnlp2020-redundancy.pdf" target="_blank">PDF</a>]
  
  
  
  
  
  
    [<a href="https://arxiv.org/abs/2004.04010" target="_blank">Arxiv</a>]
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Transformer-based deep NLP models are trained using hundreds of millions of parameters, limiting their applicability in computationally constrained environments. In this paper, we study the cause of these limitations by defining a notion of Redundancy, which we categorize into two classes: General Redundancy and Task-specific Redundancy. We dissect two popular pretrained models, BERT and XLNet, studying how much redundancy they exhibit at a representation-level and at a more fine-grained neuron-level. Our analysis reveals interesting insights, such as: i) 85% of the neurons across the network are redundant and ii) at least 92% of them can be removed when optimizing towards a downstream task. Based on our analysis, we present an efficient feature-based transfer learning procedure, which maintains 97% performance while using at-most 10% of the original neurons.</p>
  </span>
  
</div>
</li>
<li>

<div id="saleh:2020">
  
    <span class="title">Probing Neural Dialog Models for Conversational Understanding.</span>
    <span class="author">
      
        
          
          
            
              Abdelrhman Saleh,
            
          
        
      
        
          
          
            
              Tovly Deutsch,
            
          
        
      
        
          
          
            
              Stephen Casper,
            
          
        
      
        
          
          
            <em>Yonatan Belinkov</em>,
          
        
      
        
          
          
          
            
              <a href="http://www.eecs.harvard.edu/~shieber/index.html" target="_blank">Stuart Shieber</a>
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Proceedings of the Second Workshop on NLP for Conversational AI (NLP4ConvAI)</em>
    
    
      2020
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/nlp4convai2020.pdf" target="_blank">PDF</a>]
  
  
  
  
  
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>The predominant approach to open-domain dialog generation relies on end-to-end training of neural models on chat datasets. However, this approach provides little insight as to what these models learn (or do not learn) about engaging in dialog. In this study, we analyze the internal representations learned by neural open-domain dialog systems and evaluate the quality of these representations for learning basic conversational skills. Our results suggest that standard open-domain dialog systems struggle with answering questions, inferring contradiction, and determining the topic of conversation, among other tasks. We also find that the dyadic, turn-taking nature of dialog is not fully leveraged by these models. By exploring these limitations, we highlight the need for additional research into architectures and training methods that can better capture high-level information about dialog.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[ACL]</abbr>


<div id="wu:2020:acl">
  
    <span class="title">Similarity Analysis of Contextual Word Representation Models.</span>
    <span class="author">
      
        
          
          
            
              John M. Wu*,
            
          
        
      
        
          
          
            <em>Yonatan Belinkov*</em>,
          
        
      
        
          
          
            
              <a href="https://hsajjad.github.io" target="_blank">Hassan Sajjad</a>,
            
          
        
      
        
          
          
            
              <a href="http://alt.qcri.org/~ndurrani/" target="_blank">Nadir Durrani</a>,
            
          
        
      
        
          
          
            
              <a href="https://fdalvi.github.io" target="_blank">Fahim Dalvi</a>,
            
          
        
      
        
          
          
          
            
              <a href="http://people.csail.mit.edu/jrg/" target="_blank">James Glass</a>
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL)</em>
    
    
      2020
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/acl2020.pdf" target="_blank">PDF</a>]
  
  
  
  
  
    [<a href="https://github.com/johnmwu/contextual-corr-analysis" target="_blank">Code</a>]
  
  
    [<a href="https://arxiv.org/abs/2005.01172" target="_blank">Arxiv</a>]
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>This paper investigates contextual word representation models from the lens of similarity analysis. Given a collection of trained models, we measure the similarity of their internal representations and attention. Critically, these models come from  vastly different architectures. We use existing and novel similarity measures that aim to gauge the level of localization of information in the deep models, and facilitate the investigation of which design factors affect model similarity, without requiring any external linguistic annotation.  The analysis reveals that  models within the same family are more similar to one another, as may be expected. Surprisingly, different architectures  have rather similar representations, but different individual neurons. We also observed differences in information localization in lower and higher layers and found that higher layers are more affected by fine-tuning on downstream tasks.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[ACL]</abbr>


<div id="Mahabadi:2020:acl">
  
    <span class="title">End-to-End Bias Mitigation by Modelling Biases in Corpora.</span>
    <span class="author">
      
        
          
          
            
              Rabeeh Karimi Mahabadi,
            
          
        
      
        
          
          
            <em>Yonatan Belinkov</em>,
          
        
      
        
          
          
          
            
              James Henderson
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL)</em>
    
    
      2020
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/acl2020-biases.pdf" target="_blank">PDF</a>]
  
  
  
  
  
    [<a href="https://github.com/rabeehk/robust-nli" target="_blank">Code</a>]
  
  
    [<a href="http://arxiv.org/abs/1909.06321" target="_blank">Arxiv</a>]
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Several recent studies have shown that strong natural language understanding
(NLU) models are prone to relying on unwanted dataset biases without learning
the underlying task, resulting in models that fail to generalize to
out-of-domain datasets and are likely to perform poorly in real-world
scenarios. We propose two learning strategies to train neural models, which are
more robust to such biases and transfer better to out-of-domain datasets. The
biases are specified in terms of one or more bias-only models, which learn to
leverage the dataset biases. During training, the bias-only models’ predictions
are used to adjust the loss of the base model to reduce its reliance on biases
by down-weighting the biased examples and focusing the training on the hard
examples. We experiment on large-scale natural language inference and fact
verification benchmarks, evaluating on out-of-domain datasets that are
specifically designed to assess the robustness of models against known biases
in the training data. Results show that our debiasing methods greatly improve
robustness in all settings and better transfer to other textual entailment
datasets. Our code and data are publicly available in
\urlhttps://github.com/rabeehk/robust-nli.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[ACL]</abbr>


<div id="abdou:2020:acl">
  
    <span class="title">The Sensitivity of Language Models and Humans to Winograd Schema Perturbations.</span>
    <span class="author">
      
        
          
          
            
              Mostafa Abdou,
            
          
        
      
        
          
          
            
              Vinit Ravishankar,
            
          
        
      
        
          
          
            
              Maria Barrett,
            
          
        
      
        
          
          
            <em>Yonatan Belinkov</em>,
          
        
      
        
          
          
            
              Desmond Elliott,
            
          
        
      
        
          
          
          
            
              Anders Søgaard
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL)</em>
    
    
      2020
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/acl2020-winograd.pdf" target="_blank">PDF</a>]
  
  
  
  
  
  
    [<a href="https://arxiv.org/abs/2005.01348" target="_blank">Arxiv</a>]
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Large-scale pretrained language models are the major driving force behind recent improvements in performance on the Winograd Schema Challenge, a widely employed test of commonsense reasoning ability. We show, however, with a new diagnostic dataset, that these models are sensitive to linguistic perturbations of the Winograd examples that minimally affect human understanding. Our results highlight interesting differences between humans and language models: language models are more sensitive to a number or gender alternations and synonym replacements than humans, and humans are more stable and consistent in their predictions, maintain a much higher absolute performance, and perform better on non-associative instances than associative ones. 
Overall, humans are correct more often than out-of-the-box models, and the models are sometimes right for the wrong reasons. Finally, we show that fine-tuning on a large, task-specific dataset can offer a solution to these issues.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[ICLR]</abbr>


<div id="Rosenfeld2019ACP">
  
    <span class="title">A Constructive Prediction of the Generalization Error Across Scales.</span>
    <span class="author">
      
        
          
          
            
              Jonathan S. Rosenfeld,
            
          
        
      
        
          
          
            
              Amir Rosenfeld,
            
          
        
      
        
          
          
            <em>Yonatan Belinkov</em>,
          
        
      
        
          
          
          
            
              Nir Shavit
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In International Conference on Learning Representations (ICLR)</em>
    
    
      2020
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/iclr2020.pdf" target="_blank">PDF</a>]
  
  
  
  
  
  
    [<a href="https://arxiv.org/abs/1909.12673" target="_blank">Arxiv</a>]
  
  
   
  
    [Media: <a href="https://www.csail.mit.edu/news/predicting-how-well-neural-networks-will-scale" target="_blank">MIT CSAIL News</a>, <a href="https://info.deeplearning.ai/the-batch-tracking-chinas-covid-19-revival-a-robot-star-is-born-discovering-new-antibiotics-rightsizing-neural-networks" target="_blank">The Batch</a>] 
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>The dependency of the generalization error of neural networks on model and dataset size is of critical importance both in practice and for understanding the theory of neural networks. Nevertheless, the functional form of this dependency remains elusive. In this work, we present a functional form which approximates well the generalization error in practice. Capitalizing on the successful concept of model scaling (e.g., width, depth), we are able to simultaneously construct such a form and specify the exact models which can attain it across model/data scales. Our construction follows insights obtained from observations conducted over a range of model/data scales, in various model types and datasets, in vision and language tasks. We show that the form both fits the observations well across scales, and provides accurate predictions from small- to large-scale models and data.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[CL]</abbr>


<div id="belinkov:durrani:nmt">
  
    <span class="title">On the Linguistic Representational Power of Neural Machine Translation Models.</span>
    <span class="author">
      
        
          
          
            <em>Yonatan Belinkov*</em>,
          
        
      
        
          
          
            
              <a href="http://alt.qcri.org/~ndurrani/" target="_blank">Nadir Durrani*</a>,
            
          
        
      
        
          
          
            
              <a href="https://fdalvi.github.io" target="_blank">Fahim Dalvi</a>,
            
          
        
      
        
          
          
            
              <a href="https://hsajjad.github.io" target="_blank">Hassan Sajjad</a>,
            
          
        
      
        
          
          
          
            
              <a href="http://people.csail.mit.edu/jrg/" target="_blank">James Glass</a>
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>Computational Linguistics</em>
    
    
      2020
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/cl2020.pdf" target="_blank">PDF</a>]
  
  
  
  
  
  
    [<a href="https://arxiv.org/abs/1911.00317" target="_blank">Arxiv</a>]
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Despite the recent success of deep neural networks in natural language processing (NLP), their interpretability remains a challenge. We analyze the representations learned by neural machine translation models at various levels of granularity and evaluate their quality through relevant extrinsic properties. In particular, we seek answers to the following questions: (i) How accurately is word-structure captured within the learned representations, an important aspect in translating morphologically-rich languages? (ii) Do the representations capture long-range dependencies, and effectively handle syntactically divergent languages? (iii) Do the representations capture lexical semantics? We conduct a thorough investigation along several parameters: (i) Which layers in the architecture capture each of these linguistic phenomena; (ii) How does the choice of translation unit (word, character, or subword unit) impact the linguistic properties captured by the underlying representations? (iii) Do the encoder and decoder learn differently and independently? (iv) Do the representations learned by multilingual NMT models capture the same amount of linguistic information as their bilingual counterparts? Our data-driven, quantitative evaluation illuminates important aspects in NMT models and their ability to capture various linguistic phenomena. We show that deep NMT models learn a non-trivial amount of linguistic information. Notable findings include: i) Word morphology and part-of-speech information are captured at the lower layers of the model; (ii) In contrast, lexical semantics or non-local syntactic and semantic dependencies are better represented at the higher layers; (iii) Representations learned using characters are more informed about wordmorphology compared to those learned using subword units; and (iv) Representations learned by multilingual models are richer compared to bilingual models.</p>
  </span>
  
</div>
</li></ol>
<h2 class="bibliography">2019</h2>
<ol class="bibliography"><li>
  <abbr>[Interspeech]</abbr>


<div id="belinkov:2019:interpseech">
  
    <span class="title">Analyzing Phonetic and Graphemic Representations in End-to-End Automatic Speech Recognition.</span>
    <span class="author">
      
        
          
          
            <em>Yonatan Belinkov</em>,
          
        
      
        
          
          
            
              Ahmed Ali,
            
          
        
      
        
          
          
          
            
              <a href="http://people.csail.mit.edu/jrg/" target="_blank">James Glass</a>
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Proceedings of Interspeech</em>
    
    
      2019
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/interspeech2019.pdf" target="_blank">PDF</a>]
  
  
  
  
  
  
    [<a href="https://arxiv.org/abs/1907.04224" target="_blank">Arxiv</a>]
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>End-to-end neural network systems for automatic speech recognition (ASR) are trained from acoustic features to text transcriptions. In contrast to modular ASR systems, which contain separately-trained components for acoustic modeling, pronunciation lexicon, and language modeling, the end-to-end paradigm is both conceptually simpler and has the potential benefit of training the entire system on the end task. However, such neural network models are more opaque: it is not clear how to interpret the role of different parts of the network and what information it learns during training. In this paper, we analyze the learned internal representations in an end-to-end ASR model. We evaluate the representation quality in terms of several classification tasks, comparing phonemes and graphemes, as well as different articulatory features. We study two languages (English and Arabic) and three datasets, finding remarkable consistency in how different properties are represented in different layers of the deep neural network.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[Blackbox]</abbr>


<div id="vig:2019:blackbox">
  
    <span class="title">Analyzing the Structure of Attention in a Transformer Language Model.</span>
    <span class="author">
      
        
          
          
            
              Jesse Vig,
            
          
        
      
        
          
          
          
            <em>Yonatan Belinkov</em>
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Proceedings of the Second BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP at ACL</em>
    
    
      2019
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/blackboxnlp2019.pdf" target="_blank">PDF</a>]
  
  
  
  
  
  
    [<a href="https://arxiv.org/abs/1906.04284" target="_blank">Arxiv</a>]
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>The Transformer is a fully attention-based alternative to recurrent networks that has achieved state-of-the-art results across a range of NLP tasks. In this paper, we analyze the structure of attention in a Transformer language model, the GPT-2 small pretrained model. We visualize attention for individual instances and analyze the interaction between attention and syntax over a large corpus. We find that attention targets different parts of speech at different layer depths within the model, and that attention aligns with dependency relations most strongly in the middle layers. We also find that the deepest layers of the model capture the most distant relationships. Finally, we extract exemplar sentences that reveal highly specific patterns targeted by particular attention heads.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[WMT]</abbr>


<div id="li-etal-2019-findings">
  
    <span class="title">Findings of the First Shared Task on Machine Translation Robustness.</span>
    <span class="author">
      
        
          
          
            
              Xian Li,
            
          
        
      
        
          
          
            
              Paul Michel,
            
          
        
      
        
          
          
            
              Antonios Anastasopoulos,
            
          
        
      
        
          
          
            <em>Yonatan Belinkov</em>,
          
        
      
        
          
          
            
              <a href="http://alt.qcri.org/~ndurrani/" target="_blank">Nadir Durrani</a>,
            
          
        
      
        
          
          
            
              Orhan Firat,
            
          
        
      
        
          
          
            
              Philipp Koehn,
            
          
        
      
        
          
          
            
              Graham Neubig,
            
          
        
      
        
          
          
            
              Juan Pino,
            
          
        
      
        
          
          
          
            
              <a href="https://hsajjad.github.io" target="_blank">Hassan Sajjad</a>
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Proceedings of the Fourth Conference on Machine Translation</em>
    
    
      2019
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/wmt2019-robustness.pdf" target="_blank">PDF</a>]
  
  
  
  
  
  
  
    [<a href="http://www.statmt.org/wmt19/robustness.html" target="_blank">URL</a>]
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>We share the findings of the first shared task on improving robustness of Machine Translation (MT). The task provides a testbed representing challenges facing MT models deployed in the real world, and facilitates new approaches to improve models’ robustness to noisy input and domain mismatch. We focus on two language pairs (English-French and English-Japanese), and the submitted systems are evaluated on a blind test set consisting of noisy comments on Reddit and professionally sourced translations. As a new task, we received 23 submissions by 11 participating teams from universities, companies, national labs, etc. All submitted systems achieved large improvements over baselines, with the best improvement having +22.33 BLEU. We evaluated submissions by both human judgment and automatic evaluation (BLEU), which shows high correlations (Pearson’s r = 0.94 and 0.95). Furthermore, we conducted a qualitative analysis of the submitted systems using compare-mt, which revealed their salient differences in handling challenges in this task. Such analysis provides additional insights when there is occasional disagreement between human judgment and BLEU, e.g. systems better at producing colloquial expressions received higher score from human judgment.</p>
  </span>
  
</div>
</li>
<li>

<div id="suzgun:2019:delfol">
  
    <span class="title">LSTM Networks Can Perform Dynamic Counting.</span>
    <span class="author">
      
        
          
          
            
              Mirac Suzgun,
            
          
        
      
        
          
          
            
              <a href="https://www.sebastiangehrmann.com" target="_blank">Sebastian Gehrmann</a>,
            
          
        
      
        
          
          
            <em>Yonatan Belinkov</em>,
          
        
      
        
          
          
          
            
              <a href="http://www.eecs.harvard.edu/~shieber/index.html" target="_blank">Stuart Shieber</a>
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Proceedings of the First Workshop on Deep Learning and Formal Languages: Building Bridges</em>
    
    
      2019
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/delfol2019.pdf" target="_blank">PDF</a>]
  
  
  
  
  
  
    [<a href="https://arxiv.org/abs/1906.03648" target="_blank">Arxiv</a>]
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>In this paper, we systematically assess the ability of standard recurrent networks to perform dynamic counting and to encode hierarchical representations. All the neural models in our experiments are designed to be small-sized networks both to prevent them from memorizing the training sets and to visualize and interpret their behaviour at test time. Our results demonstrate that the Long Short-Term Memory (LSTM) networks can learn to recognize the well-balanced parenthesis language (Dyck-1) and the shuffles of multiple Dyck-1 languages, each defined over different parenthesis-pairs, by emulating simple real-time k-counter machines. To the best of our knowledge, this work is the first study to introduce the shuffle languages to analyze the computational power of neural networks. We also show that a single-layer LSTM with only one hidden unit is practically sufficient for recognizing the Dyck-1 language. However, none of our recurrent networks was able to yield a good performance on the Dyck-2 language learning task, which requires a model to have a stack-like mechanism for</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[ACL]</abbr>


<div id="luo:2019:acl">
  
    <span class="title">Improving Neural Language Models by Segmenting, Attending, and Predicting the Future.</span>
    <span class="author">
      
        
          
          
            
              Hongyin Luo,
            
          
        
      
        
          
          
            
              Lan Jiang,
            
          
        
      
        
          
          
            <em>Yonatan Belinkov</em>,
          
        
      
        
          
          
          
            
              <a href="http://people.csail.mit.edu/jrg/" target="_blank">James Glass</a>
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (ACL)</em>
    
    
      2019
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/acl2019-lm.pdf" target="_blank">PDF</a>]
  
  
  
  
  
  
    [<a href="https://arxiv.org/abs/1906.01702" target="_blank">Arxiv</a>]
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Common language models typically predict the next word given the context. In this work, we propose a method that improves language modeling by learning to align the given context and the following phrase. The model does not require any linguistic annotation of phrase segmentation. Instead, we define syntactic heights and phrase segmentation rules, enabling the model to automatically induce phrases, recognize their task-specific heads, and generate phrase embeddings in an unsupervised learning manner. Our method can easily be applied to language models with different network architectures since an independent module is used for phrase induction and context-phrase alignment, and no change is required in the underlying language modeling network. Experiments have shown that our model outperformed several strong baseline models on different data sets. We achieved a new state-of-the-art performance of 17.4 perplexity on the Wikitext-103 dataset. Additionally, visualizing the outputs of the phrase induction module showed that our model is able to learn approximate phrase-level structural knowledge without any annotation.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[CogSci]</abbr>


<div id="hahn:2019:surprisal">
  
    <span class="title">Character-based Surprisal as a Model of Human Reading in the Presence of Errors.</span>
    <span class="author">
      
        
          
          
            
              Michael Hahn,
            
          
        
      
        
          
          
            
              Frank Keller,
            
          
        
      
        
          
          
            
              <a href="http://yonatanbisk.com" target="_blank">Yonatan Bisk</a>,
            
          
        
      
        
          
          
          
            <em>Yonatan Belinkov</em>
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Proceedings of the 41st Annual Meeting of the Cognitive Science Society (CogSci, Oral presentation)</em>
    
    
      2019
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/cogsci2019.pdf" target="_blank">PDF</a>]
  
  
  
  
  
  
    [<a href="https://arxiv.org/abs/1902.00595" target="_blank">Arxiv</a>]
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Intuitively, human readers cope easily with errors in text; typos, misspelling, word substitutions, etc. do not unduly disrupt natural reading. Previous work indicates that letter transpositions result in increased reading times, but it is unclear if this effect generalizes to more natural errors. In this paper, we report an eye-tracking study that compares two error types (letter transpositions and naturally occurring misspelling) and two error rates (10% or 50% of all words contain errors). We find that human readers show unimpaired comprehension in spite of these errors, but error words cause more reading difficulty than correct words. Also, transpositions are more difficult than misspellings, and a high error rate increases difficulty for all words, including correct ones. We then present a computational model that uses character-based (rather than traditional word-based) surprisal to account for these results. The model explains that transpositions are harder than misspellings because they contain unexpected letter combinations. It also explains the error rate effect: upcoming words are more difficult to predict when the context is degraded, leading to increased surprisal.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[ACL]</abbr>


<div id="belinkov:2019:acl">
  
    <span class="title">Don’t Take the Premise for Granted: Mitigating Artifacts in Natural Language Inference.</span>
    <span class="author">
      
        
          
          
            <em>Yonatan Belinkov*</em>,
          
        
      
        
          
          
            
              <a href="http://www.cs.jhu.edu/~apoliak1/" target="_blank">Adam Poliak*</a>,
            
          
        
      
        
          
          
            
              <a href="http://www.eecs.harvard.edu/~shieber/index.html" target="_blank">Stuart M. Shieber</a>,
            
          
        
      
        
          
          
            
              <a href="http://www.cs.jhu.edu/~vandurme/index.html" target="_blank">Benjamin Van Durme</a>,
            
          
        
      
        
          
          
          
            
              Alexander M. Rush
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (ACL)</em>
    
    
      2019
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/acl2019.pdf" target="_blank">PDF</a>]
  
  
  
  
    [<a href="https://belinkov.com/assets/pdf/acl2019-slides.pdf" target="_blank">Slides</a>]
  
  
    [<a href="https://github.com/azpoliak/robust-nli" target="_blank">Code</a>]
  
  
    [<a href="https://arxiv.org/abs/1907.04380" target="_blank">Arxiv</a>]
  
  
  
    [<a href="https://vimeo.com/384034160" target="_blank">Talk</a>]
   
  
    [Media: <a href="https://www.seas.harvard.edu/news/2019/07/better-way-to-train-machine-learning-models" target="_blank">Havard News</a>, <a href="https://techxplore.com/news/2019-07-ai-human-bias.html" target="_blank">TechXplore</a>] 
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Natural Language Inference (NLI) datasets often contain hypothesis-only biases—artifacts that allow models to achieve non-trivial performance without learning whether a premise entails a hypothesis. We propose two probabilistic methods to build models that are more robust to such biases and better transfer across datasets. In contrast to standard approaches to NLI, our methods predict the probability of a premise given a hypothesis and NLI label, discouraging models from ignoring the premise. We evaluate our methods on synthetic and existing NLI datasets by training on datasets containing biases and testing on datasets containing no (or different) hypothesis-only biases. Our results indicate that these methods can make NLI models more robust to dataset-specific artifacts, transferring better than a baseline architecture in 9 out of 12 NLI datasets. Additionally, we provide an extensive analysis of the interplay of our methods with known biases in NLI datasets, as well as the effects of encouraging models to ignore biases and fine-tuning on target datasets.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[NAACL]</abbr>


<div id="liu:2019:NAACL">
  
    <span class="title">Linguistic Knowledge and Transferability of Contextual Representations.</span>
    <span class="author">
      
        
          
          
            
              Nelson F. Liu,
            
          
        
      
        
          
          
            
              Matt Gardner,
            
          
        
      
        
          
          
            <em>Yonatan Belinkov</em>,
          
        
      
        
          
          
            
              Matthew Peters,
            
          
        
      
        
          
          
          
            
              Noah A. Smith
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Proceedings of the 17th Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)</em>
    
    
      2019
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/naacl2019-cwr.pdf" target="_blank">PDF</a>]
  
  
  
  
  
  
    [<a href="https://arxiv.org/abs/1903.08855" target="_blank">Arxiv</a>]
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Contextual word representations derived from large-scale neural language models are successful across a diverse set of NLP tasks, suggesting that they encode useful and transferable features of language. To shed light on the linguistic knowledge they capture, we study the representations produced by several recent pretrained contextualizers (variants of ELMo, the OpenAI transformer LM, and BERT) with a suite of sixteen diverse probing tasks. We find that linear models trained on top of frozen contextual representations are competitive with state-of-the-art task-specific models in many cases, but fail on tasks requiring fine-grained linguistic knowledge (e.g., conjunct identification). To investigate the transferability of contextual word representations, we quantify differences in the transferability of individual layers within contextualizers, especially between RNNs and transformers. For instance, higher layers of RNNs are more task-specific, while transformer layers do not exhibit the same monotonic trend. In addition, to better understand what makes contextual word representations transferable, we compare language model pretraining with eleven supervised pretraining tasks. For any given task, pretraining on a closely related task yields better performance than language model pretraining (which is better on average) when the pretraining dataset is fixed. However, language model pretraining on more data gives the best results.</p>
  </span>
  
</div>
</li>
<li>

<div id="grand:2019:SIVL">
  
    <span class="title">Adversarial Regularization for Visual Question Answering: Strengths, Shortcomings, and Side Effects.</span>
    <span class="author">
      
        
          
          
            
              Gabriel Grand,
            
          
        
      
        
          
          
          
            <em>Yonatan Belinkov</em>
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Proceedings of the 2nd Workshop on Shortcomings in Vision and Language (SiVL) at NAACL-HLT</em>
    
    
      2019
    
    </span>
    

    
  

  
    <span style="color:Tomato;">Best paper award</span>
  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/sivl2019.pdf" target="_blank">PDF</a>]
  
  
  
  
  
  
    [<a href="http://arxiv.org/abs/1906.08430" target="_blank">Arxiv</a>]
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Visual question answering (VQA) models have been shown to over-rely on linguistic biases in VQA datasets, answering questions “blindly” without considering visual context. Adversarial regularization (AdvReg) aims to address this issue via an adversary subnetwork that encourages the main model to learn a bias-free representation of the question. In this work, we investigate the strengths and shortcomings of AdvReg with the goal of better understanding how it affects inference in VQA models. Despite achieving a new stateof-the-art on VQA-CP, we find that AdvReg yields several undesirable side-effects, including unstable gradients and sharply reduced performance on in-domain examples. We demonstrate that gradual introduction of regularization during training helps to alleviate, but not completely solve, these issues. Through error analyses, we observe that AdvReg improves generalization to binary questions, but impairs performance on questions with heterogeneous answer distributions. Qualitatively, we also find that regularized models tend to over-rely on visual features, while ignoring important linguistic cues in the question. Our results suggest that AdvReg requires further refinement before it can be considered a viable bias mitigation technique for VQA.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[NAACL]</abbr>


<div id="durrani:2019:NAACL">
  
    <span class="title">One Size Does Not Fit All: Comparing NMT Representations of Different Granularities.</span>
    <span class="author">
      
        
          
          
            
              <a href="http://alt.qcri.org/~ndurrani/" target="_blank">Nadir Durrani</a>,
            
          
        
      
        
          
          
            
              <a href="https://fdalvi.github.io" target="_blank">Fahim Dalvi</a>,
            
          
        
      
        
          
          
            
              <a href="https://hsajjad.github.io" target="_blank">Hassan Sajjad</a>,
            
          
        
      
        
          
          
            <em>Yonatan Belinkov</em>,
          
        
      
        
          
          
          
            
              Preslav Nakov
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Proceedings of the 17th Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)</em>
    
    
      2019
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/naacl2019-nmt.pdf" target="_blank">PDF</a>]
  
  
  
  
  
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Recent work has shown that contextualized word representations derived from neural machine translation are a viable alternative to such from simple word predictions tasks. This is because the internal understanding that needs to be built in order to be able to translate from one language to another is much more comprehensive. Unfortunately, computational and memory limitations as of present prevent NMT models from using large word vocabularies, and thus alternatives such as subword units (BPE and morphological segmentations) and characters have been used. Here we study the impact of using different kinds of units on the quality of the resulting representations when used to model morphology, syntax, and semantics. We found that while representations derived from subwords are slightly better for modeling syntax, character-based representations are superior for modeling morphology and are also more robust to noisy input.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[*SEM]</abbr>


<div id="belinkov:2019:starsem">
  
    <span class="title">On Adversarial Removal of Hypothesis-only Bias in Natural Language Inference.</span>
    <span class="author">
      
        
          
          
            <em>Yonatan Belinkov*</em>,
          
        
      
        
          
          
            
              <a href="http://www.cs.jhu.edu/~apoliak1/" target="_blank">Adam Poliak*</a>,
            
          
        
      
        
          
          
            
              <a href="http://www.eecs.harvard.edu/~shieber/index.html" target="_blank">Stuart M. Shieber</a>,
            
          
        
      
        
          
          
            
              <a href="http://www.cs.jhu.edu/~vandurme/index.html" target="_blank">Benjamin Van Durme</a>,
            
          
        
      
        
          
          
          
            
              Alexander M. Rush
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Proceedings of the Eighth Joint Conference on Lexical and Computational Semantics (*SEM, Oral presentation)</em>
    
    
      2019
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/starsem2019.pdf" target="_blank">PDF</a>]
  
  
  
  
    [<a href="https://belinkov.com/assets/pdf/starsem2019-slides.pdf" target="_blank">Slides</a>]
  
  
    [<a href="https://github.com/azpoliak/robust-nli" target="_blank">Code</a>]
  
  
    [<a href="https://arxiv.org/abs/1907.04389" target="_blank">Arxiv</a>]
  
  
   
  
    [Media: <a href="https://www.seas.harvard.edu/news/2019/07/better-way-to-train-machine-learning-models" target="_blank">Havard News</a>, <a href="https://techxplore.com/news/2019-07-ai-human-bias.html" target="_blank">TechXplore</a>] 
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Popular Natural Language Inference (NLI) datasets have been shown to be tainted by hypothesis-only biases. Adversarial learning may help models ignore sensitive biases and spurious correlations in data. We evaluate whether adversarial learning can be used in NLI to encourage models to learn representations free of hypothesis-only biases. Our analyses indicate that the representations learned via adversarial learning may be less biased, with only small drops in NLI accuracy.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[ICLR]</abbr>


<div id="bau:2019:ICLR">
  
    <span class="title">Identifying and Controlling Important Neurons in Neural Machine Translation.</span>
    <span class="author">
      
        
          
          
            
              <a href="https://baulab.info" target="_blank">D. Anthony Bau*</a>,
            
          
        
      
        
          
          
            <em>Yonatan Belinkov*</em>,
          
        
      
        
          
          
            
              <a href="https://hsajjad.github.io" target="_blank">Hassan Sajjad</a>,
            
          
        
      
        
          
          
            
              <a href="http://alt.qcri.org/~ndurrani/" target="_blank">Nadir Durrani</a>,
            
          
        
      
        
          
          
            
              <a href="https://fdalvi.github.io" target="_blank">Fahim Dalvi</a>,
            
          
        
      
        
          
          
          
            
              <a href="http://people.csail.mit.edu/jrg/" target="_blank">James Glass</a>
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In International Conference on Learning Representations (ICLR)</em>
    
    
      2019
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/iclr2019.pdf" target="_blank">PDF</a>]
  
  
  
    [<a href="https://belinkov.com/assets/pdf/iclr2019-poster.pdf" target="_blank">Poster</a>]
  
  
  
  
    [<a href="https://arxiv.org/abs/1811.01157" target="_blank">Arxiv</a>]
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Neural machine translation (NMT) models learn representations containing substantial linguistic information. However, it is not clear if such information is fully distributed or if some of it can be attributed to individual neurons. We develop unsupervised methods for discovering important neurons in NMT models. Our methods rely on the intuition that different models learn similar properties, and do not require any costly external supervision. We show experimentally that translation quality depends on the discovered neurons, and find that many of them capture common linguistic phenomena. Finally, we show how to control NMT translations in predictable ways, by modifying activations of individual neurons.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[LRE]</abbr>


<div id="belinkov:magidow:arabic">
  
    <span class="title">Studying the History of the Arabic Language: Language Technology and
               a Large-Scale Historical Corpus.</span>
    <span class="author">
      
        
          
          
            <em>Yonatan Belinkov</em>,
          
        
      
        
          
          
            
              <a href="https://uri.academia.edu/AlexanderMagidow" target="_blank">Alexander Magidow</a>,
            
          
        
      
        
          
          
            
              <a href="http://www.lsi.upc.edu/~albarron/" target="_blank">Alberto Barrón-Cedeño</a>,
            
          
        
      
        
          
          
            
              <a href="https://biu.academia.edu/AviShmidman/" target="_blank">Avi Shmidman</a>,
            
          
        
      
        
          
          
          
            
              <a href="https://alraqmiyyat.github.io" target="_blank">Maxim Romanov</a>
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>Language Resources and Evaluation</em>
    
    
      2019
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/lre2019.pdf" target="_blank">PDF</a>]
  
  
  
  
  
    [<a href="https://github.com/boknilev/periodization" target="_blank">Code</a>]
  
  
    [<a href="http://arxiv.org/abs/1809.03891" target="_blank">Arxiv</a>]
  
  
    [<a href="https://link.springer.com/article/10.1007/s10579-019-09460-w" target="_blank">URL</a>]
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Arabic is a widely-spoken language with a long and rich history, but existing corpora and language technology focus mostly on modern Arabic and its varieties. Therefore, studying the history of the language has so far been mostly limited to manual analyses on a small scale. In this work, we present a large-scale historical corpus of the written Arabic language, spanning 1400 years. We describe our efforts to clean and process this corpus using Arabic NLP tools, including the identification of reused text. We study the history of the Arabic language using a novel automatic periodization algorithm, as well as other techniques. Our findings confirm the established division of written Arabic into Modern Standard and Classical Arabic, and confirm other established periodizations, while suggesting that written Arabic may be divisible into still further periods of development.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[SCiL]</abbr>


<div id="suzgun:2019:SCiL">
  
    <span class="title">On Evaluating the Generalization of LSTM Models in Formal Languages.</span>
    <span class="author">
      
        
          
          
            
              Mirac Suzgun,
            
          
        
      
        
          
          
            <em>Yonatan Belinkov</em>,
          
        
      
        
          
          
          
            
              <a href="http://www.eecs.harvard.edu/~shieber/index.html" target="_blank">Stuart M. Shieber</a>
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Proceedings of the Society for Computation in Linguistics (SCiL)</em>
    
    
      2019
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/scil2019.pdf" target="_blank">PDF</a>]
  
  
  
  
  
    [<a href="https://github.com/suzgunmirac/lstm-eval" target="_blank">Code</a>]
  
  
    [<a href="https://arxiv.org/abs/1811.01001" target="_blank">Arxiv</a>]
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Recurrent Neural Networks (RNNs) are theoretically Turing-complete and established themselves as a dominant model for language processing. Yet, there still remains an uncertainty regarding their language learning capabilities. In this paper, we empirically evaluate the inductive learning capabilities of Long Short-Term Memory networks, a popular extension of simple RNNs, to learn simple formal languages, in particular a^nb^n, a^nb^nc^n, and a^nb^nc^nd^n. We investigate the influence of various aspects of learning, such as training data regimes and model capacity, on the generalization to unobserved samples. We find striking differences in model performances under different training settings and highlight the need for careful analysis and assessment when making claims about the learning capabilities of neural network models.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[AAAI]</abbr>


<div id="dalvi:2019:AAAI">
  
    <span class="title">What Is One Grain of Sand in the Desert? Analyzing Individual Neurons in Deep NLP Models.</span>
    <span class="author">
      
        
          
          
            
              <a href="https://fdalvi.github.io" target="_blank">Fahim Dalvi</a>,
            
          
        
      
        
          
          
            
              <a href="http://alt.qcri.org/~ndurrani/" target="_blank">Nadir Durrani</a>,
            
          
        
      
        
          
          
            
              <a href="https://hsajjad.github.io" target="_blank">Hassan Sajjad</a>,
            
          
        
      
        
          
          
            <em>Yonatan Belinkov</em>,
          
        
      
        
          
          
            
              <a href="https://baulab.info" target="_blank">D. Anthony Bau</a>,
            
          
        
      
        
          
          
          
            
              <a href="http://people.csail.mit.edu/jrg/" target="_blank">James Glass</a>
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Proceedings of the Thirty-Third AAAI Conference on Artificial Intelligence (AAAI, Oral presentation)</em>
    
    
      2019
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/aaai2019.pdf" target="_blank">PDF</a>]
  
  
  
    [<a href="https://belinkov.com/assets/pdf/aaai2019-poster.pdf" target="_blank">Poster</a>]
  
  
  
  
    [<a href="https://arxiv.org/abs/1812.09355" target="_blank">Arxiv</a>]
  
  
   
  
    [Media: <a href="http://news.mit.edu/2019/neural-networks-nlp-microscope-0201" target="_blank">MIT News</a>, <a href="https://cacm.acm.org/news/234509-putting-neural-networks-under-the-microscope/fulltext" target="_blank">ACM Tech News</a>] 
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Despite the remarkable evolution of deep neural networks in natural language processing (NLP), their interpretability remains a challenge. Previous work largely focused on what these models learn at the representation level. We break this analysis down further and study individual dimensions (neurons) in the vector representation learned by end-to-end neural models in NLP tasks. We propose two methods: Linguistic Correlation Analysis, based on a supervised method to extract the most relevant neurons with respect to an extrinsic task, and Cross-model Correlation Analysis, an unsupervised method to extract salient neurons w.r.t. the model itself. We evaluate the effectiveness of our techniques by ablating the identified neurons and reevaluating the network’s performance for two tasks: neural machine translation (NMT) and neural language modeling (NLM). We further present a comprehensive analysis of neurons with the aim to address the following questions: i) how localized or distributed are different linguistic properties in the models? ii) are certain neurons exclusive to some properties and not others? iii) is the information more or less distributed in NMT vs. NLM? and iv) how important are the neurons identified through the linguistic correlation method to the overall task? Our code is publicly available1 as part of the NeuroX toolkit (Dalvi et al. 2019).</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[AAAI]</abbr>


<div id="dalvi:2019:AAAI:demo">
  
    <span class="title">NeuroX: A Toolkit for Analyzing Individual Neurons in Neural Networks.</span>
    <span class="author">
      
        
          
          
            
              <a href="https://fdalvi.github.io" target="_blank">Fahim Dalvi</a>,
            
          
        
      
        
          
          
            
              Avery Nortonsmith,
            
          
        
      
        
          
          
            
              <a href="https://baulab.info" target="_blank">D. Anthony Bau</a>,
            
          
        
      
        
          
          
            <em>Yonatan Belinkov</em>,
          
        
      
        
          
          
            
              <a href="https://hsajjad.github.io" target="_blank">Hassan Sajjad</a>,
            
          
        
      
        
          
          
            
              <a href="http://alt.qcri.org/~ndurrani/" target="_blank">Nadir Durrani</a>,
            
          
        
      
        
          
          
          
            
              <a href="http://people.csail.mit.edu/jrg/" target="_blank">James Glass</a>
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Proceedings of the Thirty-Third AAAI Conference on Artificial Intelligence (AAAI): Demonstrations Track</em>
    
    
      2019
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/aaai2019-demo.pdf" target="_blank">PDF</a>]
  
  
  
  
  
    [<a href="https://github.com/fdalvi/NeuroX" target="_blank">Code</a>]
  
  
    [<a href="https://arxiv.org/abs/1812.09359" target="_blank">Arxiv</a>]
  
  
   
  
    [Media: <a href="http://news.mit.edu/2019/neural-networks-nlp-microscope-0201" target="_blank">MIT News</a>, <a href="https://cacm.acm.org/news/234509-putting-neural-networks-under-the-microscope/fulltext" target="_blank">ACM Tech News</a>] 
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>We present a toolkit to facilitate the interpretation and understanding of neural network models. The toolkit provides several methods to identify salient neurons with respect to the model itself or an external task. A user can visualize selected neurons, ablate them to measure their effect on the model accuracy, and manipulate them to control the behavior of the model at the test time. Such an analysis has a potential to serve as a springboard in various research directions, such as understanding the model, better architectural choices, model distillation and controlling data biases. The toolkit is available for download.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[TACL]</abbr>


<div id="belinkov2019analysis">
  
    <span class="title">Analysis Methods in Neural Language Processing: A Survey.</span>
    <span class="author">
      
        
          
          
            <em>Yonatan Belinkov</em>,
          
        
      
        
          
          
          
            
              <a href="http://people.csail.mit.edu/jrg/" target="_blank">James Glass</a>
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>Transactions of the Association for Computational Linguistics (TACL)</em>
    
    
      2019
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/tacl2019.pdf" target="_blank">PDF</a>]
  
  
  
    [<a href="https://belinkov.com/assets/pdf/tacl2019-poster.pdf" target="_blank">Poster</a>]
  
  
  
  
    [<a href="https://arxiv.org/abs/1812.08951" target="_blank">Arxiv</a>]
  
  
    [<a href="https://boknilev.github.io/nlp-analysis-methods" target="_blank">URL</a>]
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>The field of natural language processing has seen impressive progress in recent years, with neural network models replacing many of the traditional systems. A plethora of new models have been proposed, many of which are thought to be opaque compared to their feature-rich counterparts. This has led researchers to analyze, interpret, and evaluate neural networks in novel and more fine-grained ways. In this survey paper, we review analysis methods in neural language processing, categorize them according to prominent research trends, highlight existing limitations, and point to potential directions for future work.</p>
  </span>
  
</div>
</li></ol>
<h2 class="bibliography">2018</h2>
<ol class="bibliography"><li>
  <abbr>[NAACL]</abbr>


<div id="poliak:2018:NAACL">
  
    <span class="title">On the Evaluation of Semantic Phenomena in Neural Machine Translation Using Natural Language Inference.</span>
    <span class="author">
      
        
          
          
            
              <a href="http://www.cs.jhu.edu/~apoliak1/" target="_blank">Adam Poliak</a>,
            
          
        
      
        
          
          
            <em>Yonatan Belinkov</em>,
          
        
      
        
          
          
            
              <a href="http://people.csail.mit.edu/jrg/" target="_blank">James Glass</a>,
            
          
        
      
        
          
          
          
            
              <a href="http://www.cs.jhu.edu/~vandurme/index.html" target="_blank">Benjamin Van Durme</a>
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Proceedings of the 16th Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)</em>
    
    
      2018
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/naacl2018.pdf" target="_blank">PDF</a>]
  
  
  
  
  
    [<a href="https://github.com/boknilev/nmt-repr-analysis" target="_blank">Code</a>]
  
  
    [<a href="https://arxiv.org/abs/1804.09779" target="_blank">Arxiv</a>]
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>We propose a process for investigating the extent to which sentence representations arising from neural machine translation (NMT) systems encode distinct semantic phenomena. We use these representations as features to train a natural language inference (NLI) classifier based on datasets recast from existing semantic annotations. In applying this process to a representative NMT system, we find its encoder appears most suited to supporting inferences at the syntax-semantics interface, as compared to anaphora resolution requiring world-knowledge. We conclude with a discussion on the merits and potential deficiencies of the existing process, and how it may be improved and extended as a broader framework for evaluating semantic coverage.</p>
  </span>
  
</div>
</li>
<li>

<div id="belinkov:2018:phdthesis">
  
    <span class="title">On Internal Language Representations in Deep Learning: An Analysis of Machine Translation and Speech Recognition.</span>
    <span class="author">
      
        
          
          
          
            <em>Yonatan Belinkov</em>
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>PhD Thesis, Massachusetts Institute of Technology</em>
    
    
      2018
    
    </span>
    

    
  

  

  <span class="links">
  
  
    [<a href="https://belinkov.com/assets/pdf/thesis2018.pdf" target="_blank">PDF</a>]
  
  
  
  
  
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
</div>
</li>
<li>
  <abbr>[ICLR]</abbr>


<div id="belinkov:2018:ICLR">
  
    <span class="title">Synthetic and Natural Noise Both Break Neural Machine Translation.</span>
    <span class="author">
      
        
          
          
            <em>Yonatan Belinkov*</em>,
          
        
      
        
          
          
          
            
              <a href="http://yonatanbisk.com" target="_blank">Yonatan Bisk*</a>
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In International Conference on Learning Representations (ICLR, Oral presentation)</em>
    
    
      2018
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/iclr2018.pdf" target="_blank">PDF</a>]
  
  
  
  
  
    [<a href="https://github.com/ybisk/charNMT-noise" target="_blank">Code</a>]
  
  
    [<a href="https://arxiv.org/abs/1711.02173" target="_blank">Arxiv</a>]
  
  
   
  
    [Media: <a href="https://www.inside.com.tw/2017/11/20/machine-translation" target="_blank">Taiwanese Tech news</a>, <a href="https://thegradient.pub/frontiers-of-generalization-in-natural-language-processing/" target="_blank">The Gradient</a>] 
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Character-based neural machine translation (NMT) models alleviate out-of-vocabulary issues, learn morphology, and move us closer to completely end-to-end translation systems.  Unfortunately, they are also very brittle and easily falter when presented with noisy data. In this paper, we confront NMT models with synthetic and natural sources of noise. We find that state-of-the-art models fail to translate even moderately noisy texts that humans have no trouble comprehending. We explore two approaches to increase model robustness: structure-invariant word representations and robust training on noisy texts. We find that a model based on a character convolutional neural network is able to simultaneously learn representations robust to multiple kinds of noise.</p>
  </span>
  
</div>
</li></ol>
<h2 class="bibliography">2017</h2>
<ol class="bibliography"><li>
  <abbr>[NeurIPS]</abbr>


<div id="belinkov:2017:nips">
  
    <span class="title">Analyzing Hidden Representations in End-to-End Automatic Speech Recognition Systems.</span>
    <span class="author">
      
        
          
          
            <em>Yonatan Belinkov</em>,
          
        
      
        
          
          
          
            
              <a href="http://people.csail.mit.edu/jrg/" target="_blank">James Glass</a>
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Advances in Neural Information Processing Systems (NeurIPS)</em>
    
    
      2017
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/nips2017.pdf" target="_blank">PDF</a>]
  
  
  
    [<a href="https://belinkov.com/assets/pdf/nips2017-poster.pdf" target="_blank">Poster</a>]
  
  
  
    [<a href="https://github.com/boknilev/asr-repr-analysis" target="_blank">Code</a>]
  
  
    [<a href="https://arxiv.org/abs/1709.04482" target="_blank">Arxiv</a>]
  
  
   
  
    [Media: <a href="http://news.mit.edu/2017/reading-neural-network-mind-1211" target="_blank">MIT News</a>, <a href="https://cacm.acm.org/news/223498-reading-a-neural-networks-mind/fulltext" target="_blank">ACM Tech News</a>] 
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Neural models have become ubiquitous in automatic speech recognition systems. While neural networks are typically used as acoustic models in more complex systems, recent studies have explored end-to-end speech recognition systems based on neural networks, which can be trained to directly predict text from input acoustic features. Although such systems are conceptually elegant and simpler than traditional systems, it is less obvious how to interpret the trained models. 
In this work, we analyze the speech representations learned by a deep end-to-end model that is based on convolutional and recurrent layers, and trained with a connectionist temporal classification (CTC) loss. We use a pre-trained model to generate frame-level features which are given to a classifier that is trained on frame classification into phones. We evaluate representations from different layers of the deep model and compare their quality for predicting phone labels. Our experiments shed light on important aspects of the end-to-end model such as layer depth, model complexity, and other design choices. </p>
  </span>
  
</div>
</li>
<li>
  <abbr>[IWSLT]</abbr>


<div id="sajjad:2017:iwslt">
  
    <span class="title">Neural Machine Translation Training in a Multi-Domain Scenario.</span>
    <span class="author">
      
        
          
          
            
              <a href="https://hsajjad.github.io" target="_blank">Hassan Sajjad</a>,
            
          
        
      
        
          
          
            
              <a href="http://alt.qcri.org/~ndurrani/" target="_blank">Nadir Durrani</a>,
            
          
        
      
        
          
          
            
              <a href="https://fdalvi.github.io" target="_blank">Fahim Dalvi</a>,
            
          
        
      
        
          
          
            <em>Yonatan Belinkov</em>,
          
        
      
        
          
          
          
            
              Stephan Vogel
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Proceedings of the International Workshop on Spoken Language Translation (IWSLT)</em>
    
    
      2017
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/iwslt2017.pdf" target="_blank">PDF</a>]
  
  
  
  
  
  
    [<a href="https://arxiv.org/abs/1708.08712" target="_blank">Arxiv</a>]
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>In this paper, we explore alternative ways to train a neural machine translation system in a multi-domain scenario. We investigate data concatenation (with fine tuning), model stacking (multi-level fine tuning), data selection and weighted ensemble. We evaluate these methods based on three criteria: i) translation quality, ii) training time, and iii) robustness towards out-of-domain tests. Our findings on Arabic-English and German-English language pairs show that the best translation quality can be achieved by building an initial system on a concatenation of available out-of-domain data and then fine-tuning it on in-domain data. Model stacking works best when training begins with the furthest out-of-domain data and the model is incrementally fine-tuned with the next furthest domain and so on. Data selection did not give the best results, but can be considered as a decent compromise between training time and translation quality. A weighted ensemble of different individual models performed better than data selection. It is beneficial in a scenario when there is no time for fine-tuning.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[IJCNLP]</abbr>


<div id="dalvi:2017:ijcnlp">
  
    <span class="title">Understanding and Improving Morphological Learning in the Neural Machine Translation Decoder.</span>
    <span class="author">
      
        
          
          
            
              <a href="https://fdalvi.github.io" target="_blank">Fahim Dalvi</a>,
            
          
        
      
        
          
          
            
              <a href="http://alt.qcri.org/~ndurrani/" target="_blank">Nadir Durrani</a>,
            
          
        
      
        
          
          
            
              <a href="https://hsajjad.github.io" target="_blank">Hassan Sajjad</a>,
            
          
        
      
        
          
          
            <em>Yonatan Belinkov</em>,
          
        
      
        
          
          
          
            
              Stephan Vogel
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Proceedings of the 8th International Joint Conference on Natural Language Processing (IJCNLP)</em>
    
    
      2017
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/ijcnlp2017-decoder.pdf" target="_blank">PDF</a>]
  
  
  
  
  
    [<a href="https://github.com/fdalvi/seq2seq-attn-multitask" target="_blank">Code</a>]
  
  
  
   
  
    [Media: <a href="http://news.mit.edu/2017/reading-neural-network-mind-1211" target="_blank">MIT News</a>] 
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>End-to-end training makes the neural machine translation (NMT) architecture simpler, yet elegant compared to traditional statistical machine translation (SMT). However, little is known about linguistic patterns of morphology, syntax and semantics learned during the training of NMT systems, and more importantly, which parts of the architecture are responsible for learning each of these phenomena. In this paper we i) analyze how much morphology an NMT decoder learns, and ii) investigate whether injecting target morphology into the decoder helps it produce better translations. To this end we present three methods: i) joint generation, ii) joint-data learning, and iii) multi-task learning. Our results show that explicit morphological information helps the decoder learn target language morphology and improves the translation quality by 0.2–0.6 BLEU points.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[IJCNLP]</abbr>


<div id="belinkov:2017:ijcnlp">
  
    <span class="title">Evaluating Layers of Representation in Neural Machine Translation on Part-of-Speech and Semantic Tagging Tasks.</span>
    <span class="author">
      
        
          
          
            <em>Yonatan Belinkov</em>,
          
        
      
        
          
          
            
              Lluís Màrquez,
            
          
        
      
        
          
          
            
              <a href="https://hsajjad.github.io" target="_blank">Hassan Sajjad</a>,
            
          
        
      
        
          
          
            
              <a href="http://alt.qcri.org/~ndurrani/" target="_blank">Nadir Durrani</a>,
            
          
        
      
        
          
          
            
              <a href="https://fdalvi.github.io" target="_blank">Fahim Dalvi</a>,
            
          
        
      
        
          
          
          
            
              <a href="http://people.csail.mit.edu/jrg/" target="_blank">James Glass</a>
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Proceedings of the 8th International Joint Conference on Natural Language Processing (IJCNLP)</em>
    
    
      2017
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/ijcnlp2017-semantics.pdf" target="_blank">PDF</a>]
  
  
  
  
  
    [<a href="https://github.com/boknilev/nmt-repr-analysis" target="_blank">Code</a>]
  
  
    [<a href="https://arxiv.org/abs/1801.07772" target="_blank">Arxiv</a>]
  
  
   
  
    [Media: <a href="http://news.mit.edu/2017/reading-neural-network-mind-1211" target="_blank">MIT News</a>, <a href="https://cacm.acm.org/news/223498-reading-a-neural-networks-mind/fulltext" target="_blank">ACM Tech News</a>] 
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>While neural machine translation (NMT) models provide improved translation quality in an elegant framework, it is less clear what they learn about language. Recent work has started evaluating the quality of vector representations learned by NMT models on morphological and syntactic tasks. In this paper, we investigate the representations learned at different layers of NMT encoders. We train NMT systems on parallel data and use the models to extract features for training a classifier on two tasks: part-of-speech and semantic tagging. We then measure the performance of the classifier as a proxy to the quality of the original NMT model for the given task. Our quantitative analysis yields interesting insights regarding representation learning in NMT models. For instance, we find that higher layers are better at learning semantics while lower layers tend to be better for part-of-speech tagging. We also observe little effect of the target language on source-side representations, especially in higher quality models.</p>
  </span>
  
</div>
</li>
<li>

<div id="adi:2017:ibmjournal">
  
    <span class="title">Analysis of sentence embedding models using prediction tasks in natural language processing.</span>
    <span class="author">
      
        
          
          
            
              <a href="http://adiyoss.github.io" target="_blank">Yossi Adi</a>,
            
          
        
      
        
          
          
            
              Einat Kermany,
            
          
        
      
        
          
          
            <em>Yonatan Belinkov</em>,
          
        
      
        
          
          
            
              <a href="http://researcher.watson.ibm.com/researcher/view.php?person=il-OFERL" target="_blank">Ofer Lavi</a>,
            
          
        
      
        
          
          
          
            
              <a href="https://www.cs.bgu.ac.il/~yoavg/uni/" target="_blank">Yoav Goldberg</a>
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>IBM Journal of Research and Development</em>
    
    
      2017
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
  
  
  
  
  
  
    [<a href="https://ieeexplore.ieee.org/document/8030297/" target="_blank">URL</a>]
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>The tremendous success of word embeddings in improving the ability of computers to perform natural language tasks has shifted the research on language representation from word representation to focus on sentence representation. This shift introduced a plethora of methods for learning vector representations of sentences, many of them based on compositional methods over word embeddings. These vectors are used as features for subsequent machine learning tasks or for pretraining in the context of deep learning. However, not much is known about the properties that are encoded in these sentence representations and about the language information they encapsulate. Recent studies analyze the encoded representations and the kind of information they capture. In this paper, we analyze results from a previous study on the ability of models to encode basic properties such as content, order, and length. Our analysis led to new insights, such as the effect of word frequency or word distance on the ability to encode content and order.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[IPM]</abbr>


<div id="Romeo2017">
  
    <span class="title">Language processing and learning models for community question answering in Arabic.</span>
    <span class="author">
      
        
          
          
            
              Salvatore Romeo,
            
          
        
      
        
          
          
            
              Giovanni Da San Martino,
            
          
        
      
        
          
          
            <em>Yonatan Belinkov</em>,
          
        
      
        
          
          
            
              <a href="http://www.lsi.upc.edu/~albarron/" target="_blank">Alberto Barrón-Cedeño</a>,
            
          
        
      
        
          
          
            
              Mohamed Eldesouki,
            
          
        
      
        
          
          
            
              Kareem Darwish,
            
          
        
      
        
          
          
            
              Hamdy Mubarak,
            
          
        
      
        
          
          
            
              <a href="http://people.csail.mit.edu/jrg/" target="_blank">James Glass</a>,
            
          
        
      
        
          
          
          
            
              <a href="http://disi.unitn.it/moschitti/" target="_blank">Alessandro Moschitti</a>
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>Information Processing &amp; Management</em>
    
    
      2017
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/ipm2017.pdf" target="_blank">PDF</a>]
  
  
  
  
  
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>In this paper we focus on the problem of question ranking in community question answering (cQA) forums in Arabic. We address the task with machine learning algorithms using advanced Arabic text representations. The latter are obtained by applying tree kernels to constituency parse trees combined with textual similarities, including word embeddings. Our two main contributions are: (i) an Arabic language processing pipeline based on UIMA—from segmentation to constituency parsing—built on top of Farasa, a state-of-the-art Arabic language processing toolkit; and (ii) the application of long short-term memory neural networks to identify the best text fragments in questions to be used in our tree-kernel-based ranker. Our thorough experimentation on a recently released cQA dataset shows that the Arabic linguistic processing provided by Farasa produces strong results and that neural networks combined with tree kernels further boost the performance in terms of both efficiency and accuracy. Our approach also enables an implicit comparison between different processing pipelines as our tests on Farasa and Stanford parsers demonstrate.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[Interspeech]</abbr>


<div id="khurana:2017:interpseech">
  
    <span class="title">QMDIS: QCRI-MIT Advanced Dialect Identification System.</span>
    <span class="author">
      
        
          
          
            
              Sameer Khurana,
            
          
        
      
        
          
          
            
              Maryam Najafian,
            
          
        
      
        
          
          
            
              Ahmed Ali,
            
          
        
      
        
          
          
            
              Tuka Al Hanai,
            
          
        
      
        
          
          
            <em>Yonatan Belinkov</em>,
          
        
      
        
          
          
          
            
              <a href="http://people.csail.mit.edu/jrg/" target="_blank">James Glass</a>
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Proceedings of Interspeech</em>
    
    
      2017
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/interspeech2017.pdf" target="_blank">PDF</a>]
  
  
  
  
  
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>As a continuation of our efforts towards tackling the problem of spoken Dialect Identification (DID) for Arabic languages, we present the QCRI-MIT Advanced Dialect Identification System (QMDIS). QMDIS is an automatic spoken DID system for Dialectal Arabic (DA). In this paper, we report a comprehensive study of the three main components used in the spoken DID task: phonotactic, lexical and acoustic. We use Support Vector Machines (SVMs), Logistic Regression (LR) and Convolutional Neural Networks (CNNs) as backend classifiers throughout the study. We perform all our experiments on a publicly available dataset and present new state-of-the-art results. QMDIS discriminates between the five most widely used dialects of Arabic: namely Egyptian, Gulf, Levantine, North African, and Modern Standard Arabic (MSA). We report around 73% accuracy for system combination. All the data and the code used in our experiments are publicly available for research.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[ACL]</abbr>


<div id="belinkov:2017:acl">
  
    <span class="title">What do Neural Machine Translation Models Learn about Morphology?.</span>
    <span class="author">
      
        
          
          
            <em>Yonatan Belinkov</em>,
          
        
      
        
          
          
            
              <a href="http://alt.qcri.org/~ndurrani/" target="_blank">Nadir Durrani</a>,
            
          
        
      
        
          
          
            
              <a href="https://fdalvi.github.io" target="_blank">Fahim Dalvi</a>,
            
          
        
      
        
          
          
            
              <a href="https://hsajjad.github.io" target="_blank">Hassan Sajjad</a>,
            
          
        
      
        
          
          
          
            
              <a href="http://people.csail.mit.edu/jrg/" target="_blank">James Glass</a>
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL)</em>
    
    
      2017
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/acl2017.pdf" target="_blank">PDF</a>]
  
  
  
    [<a href="https://belinkov.com/assets/pdf/acl2017-poster.pdf" target="_blank">Poster</a>]
  
  
  
    [<a href="https://github.com/boknilev/nmt-repr-analysis" target="_blank">Code</a>]
  
  
    [<a href="https://arxiv.org/abs/1704.03471" target="_blank">Arxiv</a>]
  
  
  
    [<a href="https://vimeo.com/234955191" target="_blank">Talk</a>]
   
  
    [Media: <a href="https://soundcloud.com/nlp-highlights/27-what-do-neural-machine-translation-models-learn-about-morphology-with-yonatan-belinkov" target="_blank">NLP Highlights</a>] 
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Neural machine translation (MT) models obtain state-of-the-art performance while maintaining a simple, end-to-end architecture. However, little is known about what these models learn about source and target languages during the training process. In this work, we analyze the representations learned by neural MT models at various levels of granularity and empirically evaluate the quality of the representations for learning morphology through extrinsic part-of-speech and morphological tagging tasks. We conduct a thorough investigation along several parameters: word-based vs. character-based representations, depth of the encoding layer, the identity of the target language, and encoder vs. decoder representations. Our data-driven, quantitative evaluation sheds light on important aspects in the neural MT system and its ability to capture word structure.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[ACL]</abbr>


<div id="sajjad:2017:acl">
  
    <span class="title">Challenging Language-Dependent Segmentation for Arabic: An Application to Machine Translation and Part-of-Speech Tagging.</span>
    <span class="author">
      
        
          
          
            
              <a href="https://hsajjad.github.io" target="_blank">Hassan Sajjad</a>,
            
          
        
      
        
          
          
            
              <a href="https://fdalvi.github.io" target="_blank">Fahim Dalvi</a>,
            
          
        
      
        
          
          
            
              <a href="http://alt.qcri.org/~ndurrani/" target="_blank">Nadir Durrani</a>,
            
          
        
      
        
          
          
            
              Ahmed Abdelali,
            
          
        
      
        
          
          
            <em>Yonatan Belinkov</em>,
          
        
      
        
          
          
          
            
              Stephan Vogel
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL)</em>
    
    
      2017
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/acl2017-short.pdf" target="_blank">PDF</a>]
  
  
  
  
  
  
    [<a href="https://arxiv.org/abs/1709.00616" target="_blank">Arxiv</a>]
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Word segmentation plays a pivotal role in improving any Arabic NLP application. Therefore, a lot of research has been spent in improving its accuracy. Off-the-shelf tools, however, are: i) complicated to use and ii) domain/dialect dependent. We explore three language-independent alternatives to morphological segmentation using: i) data-driven sub-word units, ii) characters as a unit of learning, and iii) word embeddings learned using a character CNN (Convolution Neural Network). On the tasks of Machine Translation and POS tagging, we found these methods to achieve close to, and occasionally surpass state-of-the-art performance. In our analysis, we show that a neural machine translation system is sensitive to the ratio of source and target tokens, and a ratio close to 1 or greater, gives optimal performance.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[ICLR]</abbr>


<div id="adi:2017:ICLR">
  
    <span class="title">Fine-grained Analysis of Sentence Embeddings Using Auxiliary Prediction Tasks.</span>
    <span class="author">
      
        
          
          
            
              <a href="http://adiyoss.github.io" target="_blank">Yossi Adi</a>,
            
          
        
      
        
          
          
            
              Einat Kermany,
            
          
        
      
        
          
          
            <em>Yonatan Belinkov</em>,
          
        
      
        
          
          
            
              <a href="http://researcher.watson.ibm.com/researcher/view.php?person=il-OFERL" target="_blank">Ofer Lavi</a>,
            
          
        
      
        
          
          
          
            
              <a href="https://www.cs.bgu.ac.il/~yoavg/uni/" target="_blank">Yoav Goldberg</a>
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In International Conference on Learning Representations (ICLR)</em>
    
    
      2017
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/iclr2017.pdf" target="_blank">PDF</a>]
  
  
  
  
  
  
    [<a href="https://arxiv.org/abs/1608.04207" target="_blank">Arxiv</a>]
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>There is a lot of research interest in encoding variable length sentences into fixed length vectors, in a way that preserves the sentence meanings. Two common methods include representations based on averaging word vectors, and representations based on the hidden states of recurrent neural networks such as LSTMs. The sentence vectors are used as features for subsequent machine learning tasks or for pre-training in the context of deep learning. However, not much is known about the properties that are encoded in these sentence representations and about the language information they capture.
We propose a framework that facilitates better understanding of the encoded representations. We define prediction tasks around isolated aspects of sentence structure (namely sentence length, word content, and word order), and score representations by the ability to train a classifier to solve each prediction task when using the representation as input. We demonstrate the potential contribution of the approach by analyzing different sentence representation mechanisms. The analysis sheds light on the relative strengths of different sentence embedding methods with respect to these low level prediction tasks, and on the effect of the encoded vector’s dimensionality on the resulting representations.</p>
  </span>
  
</div>
</li></ol>
<h2 class="bibliography">2016</h2>
<ol class="bibliography"><li>

<div id="belinkov-glass:2016:VarDial">
  
    <span class="title">A Character-level Convolutional Neural Network for Distinguishing Similar Languages and Dialects.</span>
    <span class="author">
      
        
          
          
            <em>Yonatan Belinkov</em>,
          
        
      
        
          
          
          
            
              <a href="http://people.csail.mit.edu/jrg/" target="_blank">James Glass</a>
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Proceedings of the Third Workshop on NLP for Similar Languages, Varieties and Dialects (VarDial at Coling)</em>
    
    
      2016
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/vardial2016.pdf" target="_blank">PDF</a>]
  
  
  
    [<a href="https://belinkov.com/assets/pdf/vardial2016-poster.pdf" target="_blank">Poster</a>]
  
  
  
    [<a href="https://github.com/boknilev/dsl-char-cnn" target="_blank">Code</a>]
  
  
    [<a href="https://arxiv.org/abs/1609.07568" target="_blank">Arxiv</a>]
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Discriminating between closely-related language varieties is considered a challenging and important task. This paper describes our submission to the DSL 2016 shared-task, which included two sub-tasks: one on discriminating similar languages and one on identifying Arabic dialects. We developed a character-level neural network for this task. Given a sequence of characters, our model embeds each character in vector space, runs the sequence through multiple convolutions with different filter widths, and pools the convolutional representations to obtain a hidden vector representation of the text that is used for predicting the language or dialect. We primarily focused on the Arabic dialect identification task and obtained an F1 score of 0.4834, ranking 6th out of 18 participants. We also analyze errors made by our system on the Arabic data in some detail, and point to challenges such an approach is faced with.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[Coling]</abbr>


<div id="romeo-EtAl:2016:COLING">
  
    <span class="title">Neural Attention for Learning to Rank Questions in Community Question Answering.</span>
    <span class="author">
      
        
          
          
            
              Salvatore Romeo,
            
          
        
      
        
          
          
            
              Giovanni Da San Martino,
            
          
        
      
        
          
          
            
              <a href="http://www.lsi.upc.edu/~albarron/" target="_blank">Alberto Barrón-Cedeño</a>,
            
          
        
      
        
          
          
            
              <a href="http://disi.unitn.it/moschitti/" target="_blank">Alessandro Moschitti</a>,
            
          
        
      
        
          
          
            <em>Yonatan Belinkov</em>,
          
        
      
        
          
          
            
              <a href="http://people.csail.mit.edu/wnhsu/" target="_blank">Wei-Ning Hsu</a>,
            
          
        
      
        
          
          
            
              <a href="http://people.csail.mit.edu/yzhang87/" target="_blank">Yu Zhang</a>,
            
          
        
      
        
          
          
            
              Mitra Mohtarami,
            
          
        
      
        
          
          
          
            
              <a href="http://people.csail.mit.edu/jrg/" target="_blank">James Glass</a>
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Proceedings of the 26th International Conference on Computational Linguistics: Technical Papers (Coling)</em>
    
    
      2016
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/coling2016.pdf" target="_blank">PDF</a>]
  
  
  
  
  
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>In real-world data, e.g., from Web forums,  text is often contaminated with
	redundant or irrelevant content, which leads to introducing noise in machine
	learning algorithms.
	In this paper, we apply Long Short-Term Memory networks with an attention
	mechanism, which can select important parts of text for the task of similar
	question retrieval from community Question Answering (cQA) forums. 
	In particular, we use the attention weights for both selecting entire sentences
	and their subparts, i.e., word/chunk, from shallow syntactic trees. More
	interestingly, we apply tree kernels to the filtered text representations, thus
	exploiting the implicit features of the subtree space for learning question
	reranking. Our results show that the attention-based pruning allows for
	achieving the top position in the cQA challenge of SemEval 2016, with a
	relatively large gap from the other participants while greatly decreasing
	running time.</p>
  </span>
  
</div>
</li>
<li>

<div id="belinkov-EtAl:2016:LT4DH">
  
    <span class="title">Shamela: A Large-Scale Historical Arabic Corpus.</span>
    <span class="author">
      
        
          
          
            <em>Yonatan Belinkov</em>,
          
        
      
        
          
          
            
              <a href="https://uri.academia.edu/AlexanderMagidow" target="_blank">Alexander Magidow</a>,
            
          
        
      
        
          
          
            
              <a href="https://alraqmiyyat.github.io" target="_blank">Maxim Romanov</a>,
            
          
        
      
        
          
          
            
              <a href="https://biu.academia.edu/AviShmidman/" target="_blank">Avi Shmidman</a>,
            
          
        
      
        
          
          
          
            
              <a href="http://u.cs.biu.ac.il/~koppel/" target="_blank">Moshe Koppel</a>
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Proceedings of the Workshop on Language Technology Resources and Tools for Digital Humanities (LT4DH at Coling)</em>
    
    
      2016
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/lt4dh2016.pdf" target="_blank">PDF</a>]
  
  
  
  
    [<a href="https://belinkov.com/assets/pdf/lt4dh2016-slides.pdf" target="_blank">Slides</a>]
  
  
  
    [<a href="https://arxiv.org/abs/1612.08989" target="_blank">Arxiv</a>]
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Arabic is a widely-spoken language with a rich and long history spanning more
	than fourteen centuries. Yet existing Arabic corpora largely focus on the
	modern period or lack sufficient diachronic information. We develop a
	large-scale, historical corpus of Arabic of about 1 billion words from diverse
	periods of time. We clean this corpus, process it with a morphological
	analyzer, and enhance it by detecting parallel passages and automatically
	dating undated texts. We demonstrate its utility with selected case-studies in
	which we show its application to the digital humanities.</p>
  </span>
  
</div>
</li>
<li>

<div id="belinkov-glass:2016:SeMaT">
  
    <span class="title">Large-Scale Machine Translation between Arabic and Hebrew: Available Corpora and Initial Results.</span>
    <span class="author">
      
        
          
          
            <em>Yonatan Belinkov</em>,
          
        
      
        
          
          
          
            
              <a href="http://people.csail.mit.edu/jrg/" target="_blank">James Glass</a>
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Proceedings of the Workshop on Semitic Machine Translation (SeMaT at AMTA)</em>
    
    
      2016
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/semat2016.pdf" target="_blank">PDF</a>]
  
  
  
  
    [<a href="https://belinkov.com/assets/pdf/semat2016-slides.pdf" target="_blank">Slides</a>]
  
  
  
    [<a href="https://arxiv.org/abs/1609.07701" target="_blank">Arxiv</a>]
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Machine translation between Arabic and Hebrew has so far been limited by a lack of parallel corpora, despite the political and cultural importance of this language pair. Previous work relied on manually-crafted grammars or pivoting via English, both of which are unsatisfactory for building a scalable and accurate MT system. In this work, we compare standard phrase-based and neural systems on Arabic-Hebrew translation. We experiment with tokenization by external tools and sub-word modeling by character-level neural models, and show that both methods lead to improved translation performance, with a small advantage to the neural models.</p>
  </span>
  
</div>
</li>
<li>

<div id="aharoni-goldberg-belinkov:2016:SIGMORPHON">
  
    <span class="title">Improving Sequence to Sequence Learning for Morphological Inflection Generation: The BIU-MIT Systems for the SIGMORPHON 2016 Shared Task for Morphological Reinflection.</span>
    <span class="author">
      
        
          
          
            
              <a href="http://roeeaharoni.com" target="_blank">Roee Aharoni</a>,
            
          
        
      
        
          
          
            
              <a href="https://www.cs.bgu.ac.il/~yoavg/uni/" target="_blank">Yoav Goldberg</a>,
            
          
        
      
        
          
          
          
            <em>Yonatan Belinkov</em>
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Proceedings of the 14th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology (SIGMORPHON at ACL)</em>
    
    
      2016
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/sigmorphon2016.pdf" target="_blank">PDF</a>]
  
  
  
  
  
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Morphological reinflection is the task of generating a target form given a source form and the morpho-syntactic attributes of the target (and, optionally, of the source). This work presents the submission of Bar Ilan University and the Massachusetts Institute of Technology for the morphological reinflection shared task held at SIGMORPHON 2016. The submission includes two recurrent neural network architectures for learning morphological reinflection from incomplete inflection tables while using several novel ideas for this task: morpho-syntactic attribute embeddings, modeling the concept of templatic morphology, bidirectional input character representations and neural discriminative string transduction. The reported results for the proposed models over the ten languages in the shared task bring this submission to the second/third place (depending on the language) on all three sub-tasks out of eight participating teams, while training only on the Restricted category data.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[SemEval]</abbr>


<div id="mohtarami-EtAl:2016:SemEval">
  
    <span class="title">SLS at SemEval-2016 Task 3: Neural-based Approaches for Ranking in Community Question Answering.</span>
    <span class="author">
      
        
          
          
            
              Mitra Mohtarami,
            
          
        
      
        
          
          
            <em>Yonatan Belinkov</em>,
          
        
      
        
          
          
            
              <a href="http://people.csail.mit.edu/wnhsu/" target="_blank">Wei-Ning Hsu</a>,
            
          
        
      
        
          
          
            
              <a href="http://people.csail.mit.edu/yzhang87/" target="_blank">Yu Zhang</a>,
            
          
        
      
        
          
          
            
              Tao Lei,
            
          
        
      
        
          
          
            
              Kfir Bar,
            
          
        
      
        
          
          
            
              Scott Cyphers,
            
          
        
      
        
          
          
          
            
              <a href="http://people.csail.mit.edu/jrg/" target="_blank">Jim Glass</a>
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval)</em>
    
    
      2016
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/semeval2016.pdf" target="_blank">PDF</a>]
  
  
  
  
  
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Community question answering platforms need to automatically rank answers and questions with respect to a given question. In this paper, we present the approaches for the Answer Selection and Question Retrieval tasks of SemEval-2016 (task 3). We develop a bag-of-vectors approach with various vector- and text-based features, and different neural network approaches including CNNs and LSTMs to capture the semantic similarity between questions and answers for ranking purpose. Our evaluation demonstrates that our approaches significantly outperform the baselines.</p>
  </span>
  
</div>
</li></ol>
<h2 class="bibliography">2015</h2>
<ol class="bibliography"><li>
  <abbr>[EMNLP]</abbr>


<div id="belinkov-glass:2015:EMNLP">
  
    <span class="title">Arabic Diacritization with Recurrent Neural Networks.</span>
    <span class="author">
      
        
          
          
            <em>Yonatan Belinkov</em>,
          
        
      
        
          
          
          
            
              <a href="http://people.csail.mit.edu/jrg/" target="_blank">James Glass</a>
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>
    
    
      2015
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/emnlp2015.pdf" target="_blank">PDF</a>]
  
  
  
    [<a href="https://belinkov.com/assets/pdf/emnlp2015-poster.pdf" target="_blank">Poster</a>]
  
  
  
    [<a href="https://github.com/boknilev/diacritization" target="_blank">Code</a>]
  
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Arabic, Hebrew, and similar languages are typically written without diacritics, leading to ambiguity and posing a major challenge for core language processing tasks like speech recognition. Previous approaches to automatic diacritization employed a variety of machine learning techniques. However, they typically rely on existing tools like morphological analyzers and therefore cannot be easily extended to new genres and languages. We develop a recurrent neural network with long short-term memory layers for predicting diacritics in Arabic text. Our language-independent approach is trained solely from diacritized text without relying on external tools. We show experimentally that our model can rival state-of-the-art methods that have access to additional resources.</p>
  </span>
  
</div>
</li>
<li>

<div id="belinkov-barroncedeno-mubarak:2015:WANLP">
  
    <span class="title">Answer Selection in Arabic Community Question Answering: A Feature-Rich Approach.</span>
    <span class="author">
      
        
          
          
            <em>Yonatan Belinkov</em>,
          
        
      
        
          
          
            
              <a href="http://www.lsi.upc.edu/~albarron/" target="_blank">Alberto Barrón-Cedeño</a>,
            
          
        
      
        
          
          
          
            
              Hamdy Mubarak
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Proceedings of the Second Workshop on Arabic Natural Language Processing (ANLP)</em>
    
    
      2015
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/anlp2015.pdf" target="_blank">PDF</a>]
  
  
  
  
  
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>The task of answer selection in community question answering consists of identifying pertinent answers from a pool of user-generated comments related to a question. The recent SemEval-2015 introduced a shared task on community question answering, providing a corpus and evaluation scheme. In this paper we address the problem of answer selection in Arabic. Our proposed model includes a manifold of features including lexical and semantic similarities, vector representations, and rankings. We investigate the contribution of each set of features in a supervised setting. We show that employing a feature combination by means of a linear support vector machine achieves a better performance than that of the competition winner (F1 of 79.25 compared to 78.55).</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[SemEval]</abbr>


<div id="belinkov-EtAl:2015:SemEval">
  
    <span class="title">VectorSLU: A Continuous Word Vector Approach to Answer Selection in Community Question Answering Systems.</span>
    <span class="author">
      
        
          
          
            <em>Yonatan Belinkov</em>,
          
        
      
        
          
          
            
              Mitra Mohtarami,
            
          
        
      
        
          
          
            
              Scott Cyphers,
            
          
        
      
        
          
          
          
            
              <a href="http://people.csail.mit.edu/jrg/" target="_blank">James Glass</a>
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval)</em>
    
    
      2015
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/semeval2015.pdf" target="_blank">PDF</a>]
  
  
  
  
  
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Continuous word and phrase vectors have proven useful in a number of NLP tasks. Here we describe our experience using them as a source of features for the SemEval-2015 task 3, consisting of two community question answering subtasks: Answer Selection for categorizing answers as potential, good, and bad with regards to their corresponding questions; and YES/NO inference for predicting a yes, no, or unsure response to a YES/NO question using all of its good answers. Our system ranked 6th and 1st in the English answer selection and YES/NO inference subtasks respectively, and 2nd in the Arabic answer selection subtask.</p>
  </span>
  
</div>
</li></ol>
<h2 class="bibliography">2014</h2>
<ol class="bibliography"><li>
  <abbr>[TACL]</abbr>


<div id="belinkovTACL:2014">
  
    <span class="title">Exploring Compositional Architectures and Word Vector Representations for Prepositional Phrase Attachment.</span>
    <span class="author">
      
        
          
          
            <em>Yonatan Belinkov</em>,
          
        
      
        
          
          
            
              Tao Lei,
            
          
        
      
        
          
          
            
              Regina Barzilay,
            
          
        
      
        
          
          
          
            
              <a href="http://www.cs.tau.ac.il/~gamir/" target="_blank">Amir Globerson</a>
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>Transactions of the Association for Computational Linguistics (TACL)</em>
    
    
      2014
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/tacl2014.pdf" target="_blank">PDF</a>]
  
  
  
  
    [<a href="https://belinkov.com/assets/pdf/tacl2014-slides.pdf" target="_blank">Slides</a>]
  
  
    [<a href="https://github.com/boknilev/pp-attachment" target="_blank">Code</a>]
  
  
  
  
    [<a href="https://vimeo.com/248514317" target="_blank">Talk</a>]
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Prepositional phrase (PP) attachment disambiguation is a known challenge in syntactic parsing. The lexical sparsity associated with PP attachments motivates research in word representations that can capture pertinent syntactic and semantic features of the word. One promising solution is to use word vectors induced from large amounts of raw text. However, state-of-the-art systems that employ such representations yield modest gains in PP attachment accuracy.
In this paper, we show that word vector representations can yield significant PP attachment performance gains. This is achieved via a non-linear architecture that is discriminatively trained to maximize PP attachment accuracy. The architecture is initialized with word vectors trained from unlabeled data, and relearns those to maximize attachment accuracy. We obtain additional performance gains with alternative representations such as dependency- based word vectors. When tested on both English and Arabic datasets, our method outperforms both a strong SVM classifier and state-of-the-art parsers. For instance, we achieve 82.6% PP attachment accuracy on Arabic, while the Turbo and Charniak self-trained parsers obtain 76.7% and 80.8% respectively.</p>
  </span>
  
</div>
</li>
<li>

<div id="Arts2014357">
  
    <span class="title">arTenTen: Arabic Corpus and Word Sketches.</span>
    <span class="author">
      
        
          
          
            
              Tressy Arts,
            
          
        
      
        
          
          
            <em>Yonatan Belinkov</em>,
          
        
      
        
          
          
            
              <a href="http://www.nizarhabash.com" target="_blank">Nizar Habash</a>,
            
          
        
      
        
          
          
            
              <a href="https://www.kilgarriff.co.uk" target="_blank">Adam Kilgarriff</a>,
            
          
        
      
        
          
          
          
            
              Vit Suchomel
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>Journal of King Saud University - Computer and Information Sciences</em>
    
    
      2014
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/jksu2014.pdf" target="_blank">PDF</a>]
  
  
  
  
  
  
  
    [<a href="https://www.sketchengine.co.uk/artenten-corpus" target="_blank">URL</a>]
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>We present arTenTen, a web-crawled corpus of Arabic, gathered in 2012. arTenTen consists of 5.8-billion words. A chunk of it has been lemmatized and part-of-speech (POS) tagged with the MADA tool and subsequently loaded into Sketch Engine, a leading corpus query tool, where it is open for all to use. We have also created ‘word sketches’: one-page, automatic, corpus-derived summaries of a word’s grammatical and collocational behavior. We use examples to demonstrate what the corpus can show us regarding Arabic words and phrases and how this can support lexicography and inform linguistic research.
The article also presents the ‘sketch grammar’ (the basis for the word sketches) in detail, describes the process of building and processing the corpus, and considers the role of the corpus in additional research on Arabic.</p>
  </span>
  
</div>
</li>
<li>

<div id="belinkov:2014:mastersthesit">
  
    <span class="title">The Arabic Dialect of Ǧisir izZarga: Linguistic description and a preliminary classification, with sample texts.</span>
    <span class="author">
      
        
          
          
          
            <em>Yonatan Belinkov</em>
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>Master's Thesis, Tel Aviv University</em>
    
    
      2014
    
    </span>
    

    
  

  

  <span class="links">
  
  
    [<a href="https://belinkov.com/assets/pdf/tau-ma-thesis.pdf" target="_blank">PDF</a>]
  
  
  
  
  
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
</div>
</li>
<li>

<div id="belinkov:2014:mastersthesis">
  
    <span class="title">Neural Network Architectures for Prepositional Phrase Attachment Disambiguation.</span>
    <span class="author">
      
        
          
          
          
            <em>Yonatan Belinkov</em>
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>Master's Thesis, Massachusetts Institute of Technology</em>
    
    
      2014
    
    </span>
    

    
  

  

  <span class="links">
  
  
    [<a href="https://belinkov.com/assets/pdf/mit-sm-thesis-2014.pdf" target="_blank">PDF</a>]
  
  
  
  
  
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
</div>
</li></ol>
<h2 class="bibliography">2013</h2>
<ol class="bibliography"><li>
  <abbr>[ACL]</abbr>


<div id="sajjad-darwish-belinkov:2013:Short">
  
    <span class="title">Translating Dialectal Arabic to English.</span>
    <span class="author">
      
        
          
          
            
              <a href="https://hsajjad.github.io" target="_blank">Hassan Sajjad</a>,
            
          
        
      
        
          
          
            
              Kareem Darwish,
            
          
        
      
        
          
          
          
            <em>Yonatan Belinkov</em>
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL)</em>
    
    
      2013
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/acl2013.pdf" target="_blank">PDF</a>]
  
  
  
  
  
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>We present a dialectal Egyptian Arabic to English statistical machine translation system that leverages dialectal to Modern Standard Arabic (MSA) adaptation. In contrast to previous work, we first narrow down the gap between Egyptian and MSA by applying an automatic character-level transformational model that changes Egyptian to EG’ , which looks similar to MSA. The transformations include morphological, phonological and spelling changes. The transformation reduces the out-of-vocabulary (OOV) words from 5.2% to 2.6% and gives a gain of 1.87 BLEU points. Further, adapting large MSA/English parallel data increases the lexical coverage, reduces OOVs to 0.7% and leads to an absolute BLEU improvement of 2.73 points.</p>
  </span>
  
</div>
</li>
<li>

<div id="arTenTen">
  
    <span class="title">arTenTen: a new, vast corpus for Arabic.</span>
    <span class="author">
      
        
          
          
            <em>Yonatan Belinkov</em>,
          
        
      
        
          
          
            
              <a href="http://www.nizarhabash.com" target="_blank">Nizar Habash</a>,
            
          
        
      
        
          
          
            
              <a href="https://www.kilgarriff.co.uk" target="_blank">Adam Kilgarriff</a>,
            
          
        
      
        
          
          
            
              Noam Ordan,
            
          
        
      
        
          
          
            
              Ryan Roth,
            
          
        
      
        
          
          
          
            
              Vít Suchomel
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Proceedings of the Second Workshop on Arabic Corpus Linguistics (WACL)</em>
    
    
      2013
    
    </span>
    

    
  

  

  <span class="links">
  
  
    [<a href="https://belinkov.com/assets/pdf/wacl2013.pdf" target="_blank">PDF</a>]
  
  
  
  
  
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
</div>
</li></ol>

  </article>

  

  

  

  

</div>

      </div>
    </div>

    <footer class="site-footer">

  <div class="wrapper">
  	<p class="small">
    © Copyright 2025 Yonatan Belinkov.
    Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme.
</p>
  </div>

</footer>


    <!-- Load jQuery -->
<script src="//code.jquery.com/jquery-1.12.4.min.js"></script>

<!-- Load Common JS -->
<script src="https://belinkov.com/assets/js/common.js"></script>

<!-- Load KaTeX -->
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.6.0/katex.min.css">
<script src="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.6.0/katex.min.js"></script>
<script src="https://belinkov.com/assets/js/katex.js"></script>

<!-- Include custom icon fonts -->
<link rel="stylesheet" href="https://belinkov.com/assets/css/font-awesome.min.css">
<link rel="stylesheet" href="https://belinkov.com/assets/css/academicons.min.css">


  </body>

</html>
