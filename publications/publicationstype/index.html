<!DOCTYPE html>
<html lang="en">

  <head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">

  <title>Yonatan Belinkov | publications</title>
  <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
">

  <link rel="stylesheet" href="https://belinkov.com/assets/css/main.css">
  <link rel="canonical" href="https://belinkov.com/publications/publicationstype/">

  <!-- Styles -->
  
  <link rel="shortcut icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üéì</text></svg>">
  

</head>


  <body>

    <header class="site-header">

  <div class="wrapper">

    
    <span class="site-title">
        
        <strong>Yonatan</strong> Belinkov
    </span>
    


    <nav class="site-nav">

      <div class="trigger">
        <!-- About -->
        <a class="page-link" href="https://belinkov.com/">about</a>

        <!-- Blog -->
	<!-- 
        <a class="page-link" href="https://belinkov.com/blog/">blog</a>
	-->

        <!-- Pages -->
        
          
        
          
            <a class="page-link" href="https://belinkov.com/publications/">publications</a>
          
        
          
            <a class="page-link" href="https://belinkov.com/students/">group</a>
          
        
          
        
          
            <a class="page-link" href="https://belinkov.com/teaching/">teaching</a>
          
        
          
            <a class="page-link" href="https://belinkov.com/talks/">talks</a>
          
        
          
            <a class="page-link" href="https://belinkov.com/advice/">advice</a>
          
        
          
            <a class="page-link" href="https://belinkov.com/assets/pdf/cv-belinkov.pdf">cv</a>
          
        
          
        
          
        

      </div>
    </nav>

  </div>

</header>



    <div class="page-content">
      <div class="wrapper">
        <div class="post">

  <header class="post-header">
    <h1 class="post-title">publications</h1>
     <!-- <h5 class="post-description"></h5> -->
  </header>

  <article class="post-content publications clearfix">
    <p><a class="pure-button" href="https://belinkov.com/publications/">By Year</a>
<a class="pure-button" href="https://belinkov.com/publications/publicationstype/">By Type</a></p>

<h2 class="bibliography">Journal Articles</h2>
<ol class="bibliography"><li>
  <abbr>[NEJLT]</abbr>


<div id="tirosh-becker:2022">
  
    <span class="title">Part-of-Speech and Morphological Tagging of Algerian Judeo-Arabic.</span>
    <span class="author">
      
        
          
          
            
              Ofra Tirosh-Becker*,
            
          
        
      
        
          
          
            
              Michal Kessler*,
            
          
        
      
        
          
          
            
              Oren Becker,
            
          
        
      
        
          
          
          
            <em>Yonatan Belinkov</em>
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>The Northern European Journal of Language Technology (NEJLT)</em>
    
    
      2022
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/nejlt2022.pdf" target="_blank">PDF</a>]
  
  
  
  
  
    [<a href="https://github.com/technion-cs-nlp/nlp4aja" target="_blank">Code</a>]
  
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Most linguistic studies of Judeo-Arabic, the ensemble of dialects spoken and written by Jews in Arab lands, are qualitative in nature and rely on laborious manual annotation work, and are therefore limited in scale. In this work, we develop automatic methods for morpho-syntactic tagging of Algerian Judeo-Arabic texts published by Algerian Jews in the 19th‚Äì20th centuries, based on a linguistically tagged corpus. First, we describe our semi-automatic approach for preprocessing these texts. Then, we experiment with both an off-the-shelf morphological tagger and several specially designed neural network taggers. Finally, we perform a real-world evaluation of new texts that were never tagged before in comparison with human expert annotators. Our experimental results demonstrate that these methods can dramatically speed up and improve the linguistic research pipeline, enabling linguists to study these dialects on a much greater scale.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[CL]</abbr>


<div id="belinkov:2022:cl">
  
    <span class="title">Probing Classifiers: Promises, Shortcomings, and Advances.</span>
    <span class="author">
      
        
          
          
          
            <em>Yonatan Belinkov</em>
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>Computational Linguistics</em>
    
    
      2022
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/cl2021.pdf" target="_blank">PDF</a>]
  
  
  
  
  
  
    [<a href="https://arxiv.org/abs/2102.12452" target="_blank">Arxiv</a>]
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Probing classifiers have emerged as one of the prominent methodologies for interpreting and analyzing deep neural network models of natural language processing. The basic idea is simple ‚Äî a classifier is trained to predict some linguistic property from a model‚Äôs representations ‚Äî and has been used to examine a wide variety of models and properties. However, recent studies have demonstrated various methodological limitations of this approach. This article critically reviews the probing classifiers framework, highlighting their promises, shortcomings, and advances.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[CL]</abbr>


<div id="belinkov:durrani:nmt">
  
    <span class="title">On the Linguistic Representational Power of Neural Machine Translation Models.</span>
    <span class="author">
      
        
          
          
            <em>Yonatan Belinkov*</em>,
          
        
      
        
          
          
            
              <a href="http://alt.qcri.org/~ndurrani/" target="_blank">Nadir Durrani*</a>,
            
          
        
      
        
          
          
            
              <a href="https://fdalvi.github.io" target="_blank">Fahim Dalvi</a>,
            
          
        
      
        
          
          
            
              <a href="https://hsajjad.github.io" target="_blank">Hassan Sajjad</a>,
            
          
        
      
        
          
          
          
            
              <a href="http://people.csail.mit.edu/jrg/" target="_blank">James Glass</a>
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>Computational Linguistics</em>
    
    
      2020
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/cl2020.pdf" target="_blank">PDF</a>]
  
  
  
  
  
  
    [<a href="https://arxiv.org/abs/1911.00317" target="_blank">Arxiv</a>]
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Despite the recent success of deep neural networks in natural language processing (NLP), their interpretability remains a challenge. We analyze the representations learned by neural machine translation models at various levels of granularity and evaluate their quality through relevant extrinsic properties. In particular, we seek answers to the following questions: (i) How accurately is word-structure captured within the learned representations, an important aspect in translating morphologically-rich languages? (ii) Do the representations capture long-range dependencies, and effectively handle syntactically divergent languages? (iii) Do the representations capture lexical semantics? We conduct a thorough investigation along several parameters: (i) Which layers in the architecture capture each of these linguistic phenomena; (ii) How does the choice of translation unit (word, character, or subword unit) impact the linguistic properties captured by the underlying representations? (iii) Do the encoder and decoder learn differently and independently? (iv) Do the representations learned by multilingual NMT models capture the same amount of linguistic information as their bilingual counterparts? Our data-driven, quantitative evaluation illuminates important aspects in NMT models and their ability to capture various linguistic phenomena. We show that deep NMT models learn a non-trivial amount of linguistic information. Notable findings include: i) Word morphology and part-of-speech information are captured at the lower layers of the model; (ii) In contrast, lexical semantics or non-local syntactic and semantic dependencies are better represented at the higher layers; (iii) Representations learned using characters are more informed about wordmorphology compared to those learned using subword units; and (iv) Representations learned by multilingual models are richer compared to bilingual models.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[LRE]</abbr>


<div id="belinkov:magidow:arabic">
  
    <span class="title">Studying the History of the Arabic Language: Language Technology and
               a Large-Scale Historical Corpus.</span>
    <span class="author">
      
        
          
          
            <em>Yonatan Belinkov</em>,
          
        
      
        
          
          
            
              <a href="https://uri.academia.edu/AlexanderMagidow" target="_blank">Alexander Magidow</a>,
            
          
        
      
        
          
          
            
              <a href="http://www.lsi.upc.edu/~albarron/" target="_blank">Alberto Barr√≥n-Cede√±o</a>,
            
          
        
      
        
          
          
            
              <a href="https://biu.academia.edu/AviShmidman/" target="_blank">Avi Shmidman</a>,
            
          
        
      
        
          
          
          
            
              <a href="https://alraqmiyyat.github.io" target="_blank">Maxim Romanov</a>
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>Language Resources and Evaluation</em>
    
    
      2019
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/lre2019.pdf" target="_blank">PDF</a>]
  
  
  
  
  
    [<a href="https://github.com/boknilev/periodization" target="_blank">Code</a>]
  
  
    [<a href="http://arxiv.org/abs/1809.03891" target="_blank">Arxiv</a>]
  
  
    [<a href="https://link.springer.com/article/10.1007/s10579-019-09460-w" target="_blank">URL</a>]
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Arabic is a widely-spoken language with a long and rich history, but existing corpora and language technology focus mostly on modern Arabic and its varieties. Therefore, studying the history of the language has so far been mostly limited to manual analyses on a small scale. In this work, we present a large-scale historical corpus of the written Arabic language, spanning 1400 years. We describe our efforts to clean and process this corpus using Arabic NLP tools, including the identification of reused text. We study the history of the Arabic language using a novel automatic periodization algorithm, as well as other techniques. Our findings confirm the established division of written Arabic into Modern Standard and Classical Arabic, and confirm other established periodizations, while suggesting that written Arabic may be divisible into still further periods of development.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[TACL]</abbr>


<div id="belinkov2019analysis">
  
    <span class="title">Analysis Methods in Neural Language Processing: A Survey.</span>
    <span class="author">
      
        
          
          
            <em>Yonatan Belinkov</em>,
          
        
      
        
          
          
          
            
              <a href="http://people.csail.mit.edu/jrg/" target="_blank">James Glass</a>
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>Transactions of the Association for Computational Linguistics (TACL)</em>
    
    
      2019
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/tacl2019.pdf" target="_blank">PDF</a>]
  
  
  
    [<a href="https://belinkov.com/assets/pdf/tacl2019-poster.pdf" target="_blank">Poster</a>]
  
  
  
  
    [<a href="https://arxiv.org/abs/1812.08951" target="_blank">Arxiv</a>]
  
  
    [<a href="https://boknilev.github.io/nlp-analysis-methods" target="_blank">URL</a>]
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>The field of natural language processing has seen impressive progress in recent years, with neural network models replacing many of the traditional systems. A plethora of new models have been proposed, many of which are thought to be opaque compared to their feature-rich counterparts. This has led researchers to analyze, interpret, and evaluate neural networks in novel and more fine-grained ways. In this survey paper, we review analysis methods in neural language processing, categorize them according to prominent research trends, highlight existing limitations, and point to potential directions for future work.</p>
  </span>
  
</div>
</li>
<li>

<div id="adi:2017:ibmjournal">
  
    <span class="title">Analysis of sentence embedding models using prediction tasks in natural language processing.</span>
    <span class="author">
      
        
          
          
            
              <a href="http://adiyoss.github.io" target="_blank">Yossi Adi</a>,
            
          
        
      
        
          
          
            
              Einat Kermany,
            
          
        
      
        
          
          
            <em>Yonatan Belinkov</em>,
          
        
      
        
          
          
            
              <a href="http://researcher.watson.ibm.com/researcher/view.php?person=il-OFERL" target="_blank">Ofer Lavi</a>,
            
          
        
      
        
          
          
          
            
              <a href="https://www.cs.bgu.ac.il/~yoavg/uni/" target="_blank">Yoav Goldberg</a>
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>IBM Journal of Research and Development</em>
    
    
      2017
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
  
  
  
  
  
  
    [<a href="https://ieeexplore.ieee.org/document/8030297/" target="_blank">URL</a>]
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>The tremendous success of word embeddings in improving the ability of computers to perform natural language tasks has shifted the research on language representation from word representation to focus on sentence representation. This shift introduced a plethora of methods for learning vector representations of sentences, many of them based on compositional methods over word embeddings. These vectors are used as features for subsequent machine learning tasks or for pretraining in the context of deep learning. However, not much is known about the properties that are encoded in these sentence representations and about the language information they encapsulate. Recent studies analyze the encoded representations and the kind of information they capture. In this paper, we analyze results from a previous study on the ability of models to encode basic properties such as content, order, and length. Our analysis led to new insights, such as the effect of word frequency or word distance on the ability to encode content and order.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[IPM]</abbr>


<div id="Romeo2017">
  
    <span class="title">Language processing and learning models for community question answering in Arabic.</span>
    <span class="author">
      
        
          
          
            
              Salvatore Romeo,
            
          
        
      
        
          
          
            
              Giovanni Da San Martino,
            
          
        
      
        
          
          
            <em>Yonatan Belinkov</em>,
          
        
      
        
          
          
            
              <a href="http://www.lsi.upc.edu/~albarron/" target="_blank">Alberto Barr√≥n-Cede√±o</a>,
            
          
        
      
        
          
          
            
              Mohamed Eldesouki,
            
          
        
      
        
          
          
            
              Kareem Darwish,
            
          
        
      
        
          
          
            
              Hamdy Mubarak,
            
          
        
      
        
          
          
            
              <a href="http://people.csail.mit.edu/jrg/" target="_blank">James Glass</a>,
            
          
        
      
        
          
          
          
            
              <a href="http://disi.unitn.it/moschitti/" target="_blank">Alessandro Moschitti</a>
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>Information Processing &amp; Management</em>
    
    
      2017
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/ipm2017.pdf" target="_blank">PDF</a>]
  
  
  
  
  
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>In this paper we focus on the problem of question ranking in community question answering (cQA) forums in Arabic. We address the task with machine learning algorithms using advanced Arabic text representations. The latter are obtained by applying tree kernels to constituency parse trees combined with textual similarities, including word embeddings. Our two main contributions are: (i)‚ÄØan Arabic language processing pipeline based on UIMA‚Äîfrom segmentation to constituency parsing‚Äîbuilt on top of Farasa, a state-of-the-art Arabic language processing toolkit; and (ii)‚ÄØthe application of long short-term memory neural networks to identify the best text fragments in questions to be used in our tree-kernel-based ranker. Our thorough experimentation on a recently released cQA dataset shows that the Arabic linguistic processing provided by Farasa produces strong results and that neural networks combined with tree kernels further boost the performance in terms of both efficiency and accuracy. Our approach also enables an implicit comparison between different processing pipelines as our tests on Farasa and Stanford parsers demonstrate.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[TACL]</abbr>


<div id="belinkovTACL:2014">
  
    <span class="title">Exploring Compositional Architectures and Word Vector Representations for Prepositional Phrase Attachment.</span>
    <span class="author">
      
        
          
          
            <em>Yonatan Belinkov</em>,
          
        
      
        
          
          
            
              Tao Lei,
            
          
        
      
        
          
          
            
              <a href="http://people.csail.mit.edu/regina/" target="_blank">Regina Barzilay</a>,
            
          
        
      
        
          
          
          
            
              <a href="http://www.cs.tau.ac.il/~gamir/" target="_blank">Amir Globerson</a>
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>Transactions of the Association for Computational Linguistics (TACL)</em>
    
    
      2014
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/tacl2014.pdf" target="_blank">PDF</a>]
  
  
  
  
    [<a href="https://belinkov.com/assets/pdf/tacl2014-slides.pdf" target="_blank">Slides</a>]
  
  
    [<a href="https://github.com/boknilev/pp-attachment" target="_blank">Code</a>]
  
  
  
  
    [<a href="https://vimeo.com/248514317" target="_blank">Talk</a>]
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Prepositional phrase (PP) attachment disambiguation is a known challenge in syntactic parsing. The lexical sparsity associated with PP attachments motivates research in word representations that can capture pertinent syntactic and semantic features of the word. One promising solution is to use word vectors induced from large amounts of raw text. However, state-of-the-art systems that employ such representations yield modest gains in PP attachment accuracy.
In this paper, we show that word vector representations can yield significant PP attachment performance gains. This is achieved via a non-linear architecture that is discriminatively trained to maximize PP attachment accuracy. The architecture is initialized with word vectors trained from unlabeled data, and relearns those to maximize attachment accuracy. We obtain additional performance gains with alternative representations such as dependency- based word vectors. When tested on both English and Arabic datasets, our method outperforms both a strong SVM classifier and state-of-the-art parsers. For instance, we achieve 82.6% PP attachment accuracy on Arabic, while the Turbo and Charniak self-trained parsers obtain 76.7% and 80.8% respectively.</p>
  </span>
  
</div>
</li>
<li>

<div id="Arts2014357">
  
    <span class="title">arTenTen: Arabic Corpus and Word Sketches.</span>
    <span class="author">
      
        
          
          
            
              Tressy Arts,
            
          
        
      
        
          
          
            <em>Yonatan Belinkov</em>,
          
        
      
        
          
          
            
              <a href="http://www.nizarhabash.com" target="_blank">Nizar Habash</a>,
            
          
        
      
        
          
          
            
              <a href="https://www.kilgarriff.co.uk" target="_blank">Adam Kilgarriff</a>,
            
          
        
      
        
          
          
          
            
              Vit Suchomel
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>Journal of King Saud University - Computer and Information Sciences</em>
    
    
      2014
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/jksu2014.pdf" target="_blank">PDF</a>]
  
  
  
  
  
  
  
    [<a href="https://www.sketchengine.co.uk/artenten-corpus" target="_blank">URL</a>]
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>We present arTenTen, a web-crawled corpus of Arabic, gathered in 2012. arTenTen consists of 5.8-billion words. A chunk of it has been lemmatized and part-of-speech (POS) tagged with the MADA tool and subsequently loaded into Sketch Engine, a leading corpus query tool, where it is open for all to use. We have also created ‚Äòword sketches‚Äô: one-page, automatic, corpus-derived summaries of a word‚Äôs grammatical and collocational behavior. We use examples to demonstrate what the corpus can show us regarding Arabic words and phrases and how this can support lexicography and inform linguistic research.
The article also presents the ‚Äòsketch grammar‚Äô (the basis for the word sketches) in detail, describes the process of building and processing the corpus, and considers the role of the corpus in additional research on Arabic.</p>
  </span>
  
</div>
</li></ol>
<h2 class="bibliography">Conference Papers</h2>
<ol class="bibliography"><li>
  <abbr>[ICCV]</abbr>


<div id="orgad:iccv:time">
  
    <span class="title">Editing Implicit Assumptions in Text-to-Image Diffusion Models.</span>
    <span class="author">
      
        
          
          
            
              <a href="https://orgadhadas.github.io" target="_blank">Hadas Orgad</a>,
            
          
        
      
        
          
          
            
              Bahjat Kawar,
            
          
        
      
        
          
          
          
            <em>Yonatan Belinkov</em>
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</em>
    
    
      2023
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
  
  
  
  
    [<a href="https://github.com/bahjat-kawar/time-diffusion" target="_blank">Code</a>]
  
  
    [<a href="https://arxiv.org/abs/2303.08084" target="_blank">Arxiv</a>]
  
  
    [<a href="https://time-diffusion.github.io" target="_blank">URL</a>]
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Text-to-image diffusion models often make implicit assumptions about the world when generating images. While some assumptions are useful (e.g., the sky is blue), they can also be outdated, incorrect, or reflective of social biases present in the training data. Thus, there is a need to control these assumptions without requiring explicit user input or costly re-training. In this work, we aim to edit a given implicit assumption in a pre-trained diffusion model. Our Text-to-Image Model Editing method, TIME for short, receives a pair of inputs: a "source" under-specified prompt for which the model makes an implicit assumption (e.g., "a pack of roses"), and a "destination" prompt that describes the same setting, but with a specified desired attribute (e.g., "a pack of blue roses"). TIME then updates the model‚Äôs cross-attention layers, as these layers assign visual meaning to textual tokens. We edit the projection matrices in these layers such that the source prompt is projected close to the destination prompt. Our method is highly efficient, as it modifies a mere 2.2% of the model‚Äôs parameters in under one second. To evaluate model editing approaches, we introduce TIMED (TIME Dataset), containing 147 source and destination prompt pairs from various domains. Our experiments (using Stable Diffusion) show that TIME is successful in model editing, generalizes well for related prompts unseen during editing, and imposes minimal effect on unrelated generations.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[ACL]</abbr>


<div id="orgad:acl:dfl">
  
    <span class="title">BLIND: Bias Removal With No Demographics.</span>
    <span class="author">
      
        
          
          
            
              <a href="https://orgadhadas.github.io" target="_blank">Hadas Orgad</a>,
            
          
        
      
        
          
          
          
            <em>Yonatan Belinkov</em>
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (ACL)</em>
    
    
      2023
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/acl2023-blind.pdf" target="_blank">PDF</a>]
  
  
  
  
  
    [<a href="https://github.com/technion-cs-nlp/BLIND" target="_blank">Code</a>]
  
  
    [<a href="https://arxiv.org/abs/2212.10563" target="_blank">Arxiv</a>]
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Models trained on real-world data tend to imitate and amplify social biases. Common methods to mitigate biases require prior information on the types of biases that should be mitigated (e.g., gender or racial bias) and the social groups associated with each data sample. In this work, we introduce BLIND, a method for bias removal with no prior knowledge of the demographics in the dataset. While training a model on a downstream task, BLIND detects biased samples using an auxiliary model that predicts the main model‚Äôs success, and down-weights those samples during the training process. Experiments with racial and gender biases in sentiment classification and occupation classification tasks demonstrate that BLIND mitigates social biases without relying on a costly demographic annotation process. Our method is competitive with other methods that require demographic information and sometimes even surpasses them.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[ACL]</abbr>


<div id="iskander:acl:igbp">
  
    <span class="title">Shielded Representations: Protecting Sensitive Attributes Through
Iterative Gradient-Based Projection.</span>
    <span class="author">
      
        
          
          
            
              Shadi Iskander,
            
          
        
      
        
          
          
            
              Kira Radinsky,
            
          
        
      
        
          
          
          
            <em>Yonatan Belinkov</em>
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Findings of the Association for Computational Linguistics (ACL)</em>
    
    
      2023
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/acl2023-findings.pdf" target="_blank">PDF</a>]
  
  
  
  
  
    [<a href="https://github.com/technion-cs-nlp/igbp_nonlinear-removal" target="_blank">Code</a>]
  
  
    [<a href="https://arxiv.org/abs/2305.10204" target="_blank">Arxiv</a>]
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Natural language processing models tend to learn and encode social biases present in the data. One popular approach for addressing such biases is to eliminate encoded information from the model‚Äôs representations. However, current methods are restricted to removing only linearly encoded information. In this work, we propose Iterative Gradient-Based Projection (IGBP), a novel method for removing non-linear encoded concepts from neural representations. Our method consists of iteratively training neural classifiers to predict a particular attribute we seek to eliminate, followed by a projection of the representation on a hypersurface, such that the classifiers become oblivious to the target attribute. We evaluate the effectiveness of our method on the task of removing gender and race information as sensitive attributes. Our results demonstrate that IGBP is effective in mitigating bias through intrinsic and extrinsic evaluations, with minimal impact on downstream task accuracy.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[ACL]</abbr>


<div id="ratner:2023:pcw">
  
    <span class="title">Parallel Context Windows for Large Language Models.</span>
    <span class="author">
      
        
          
          
            
              Nir Ratner,
            
          
        
      
        
          
          
            
              Yoav Levine,
            
          
        
      
        
          
          
            <em>Yonatan Belinkov</em>,
          
        
      
        
          
          
            
              Ori Ram,
            
          
        
      
        
          
          
            
              Omri Abend,
            
          
        
      
        
          
          
            
              Udi Karpas,
            
          
        
      
        
          
          
            
              Amnon Shashua,
            
          
        
      
        
          
          
            
              Kevin Leyton-Brown,
            
          
        
      
        
          
          
          
            
              Yoav Shoham
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (ACL)</em>
    
    
      2023
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/acl2023-pcw.pdf" target="_blank">PDF</a>]
  
  
  
  
  
    [<a href="https://github.com/ai21labs/parallel-context-windows" target="_blank">Code</a>]
  
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>When applied for processing long text, Large Language Models (LLMs) are limited by their context window. Existing efforts to address this limitation involve training specialized architectures, and cannot be easily applied to off-the-shelf LLMs. We present Parallel Context Windows (PCW), a method that alleviates the context window restriction for any off-the-shelf LLM without further training. The key to the approach is to carve a long context into chunks (‚Äúwindows‚Äù), restrict the attention mechanism to apply only within each window, and re-use the positional embeddings across the windows. Our main results test the PCW approach on in-context learning with models that range in size between 750 million and 178 billion parameters, and show substantial improvements for tasks with diverse input and output spaces. We show additional benefits in other settings where long context windows may be beneficial: multi-hop questions and retrieval-augmented question answering with multiple retrieved documents. Our results highlight Parallel Context Windows as a promising method for applying off-the-shelf LLMs in a range of settings that require long text sequences. We make our code publicly available at https://github.com/ai21labs/parallel-context-windows.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[ACL]</abbr>


<div id="ram:2023:acl">
  
    <span class="title">What Are You Token About? Dense Retrieval as Distributions Over the Vocabulary.</span>
    <span class="author">
      
        
          
          
            
              Ori Ram,
            
          
        
      
        
          
          
            
              Liat Bezalel,
            
          
        
      
        
          
          
            
              Adi Zicher,
            
          
        
      
        
          
          
            <em>Yonatan Belinkov</em>,
          
        
      
        
          
          
            
              Jonathan Berant,
            
          
        
      
        
          
          
          
            
              <a href="http://www.cs.tau.ac.il/~gamir/" target="_blank">Amir Globerson</a>
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (ACL)</em>
    
    
      2023
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/acl2023-token.pdf" target="_blank">PDF</a>]
  
  
  
  
  
    [<a href="https://github.com/oriram/dense-retrieval-projections" target="_blank">Code</a>]
  
  
    [<a href="https://arxiv.org/abs/2212.10380" target="_blank">Arxiv</a>]
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Dual encoders are now the dominant architecture for dense retrieval. Yet, we have little understanding of how they represent text, and why this leads to good performance. In this work, we shed light on this question via distributions over the vocabulary. We propose to interpret the vector representations produced by dual encoders by projecting them into the model‚Äôs vocabulary space. We show that the resulting projections contain rich semantic information, and draw connection between them and sparse retrieval. We find that this view can offer an explanation for some of the failure cases of dense retrievers. For example, we observe that the inability of models to handle tail entities is correlated with a tendency of the token distributions to forget some of the tokens of those entities. We leverage this insight and propose a simple way to enrich query and passage representations with lexical information at inference time, and show that this significantly improves performance compared to the original model in zero-shot settings, and specifically on the BEIR benchmark.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[ICLR]</abbr>


<div id="dotan:2023:iclr">
  
    <span class="title">Multiple sequence alignment as a sequence-to-sequence learning problem.</span>
    <span class="author">
      
        
          
          
            
              Edo Dotan,
            
          
        
      
        
          
          
            <em>Yonatan Belinkov</em>,
          
        
      
        
          
          
            
              Oren Avram,
            
          
        
      
        
          
          
            
              Elya Wygoda,
            
          
        
      
        
          
          
            
              Noa Ecker,
            
          
        
      
        
          
          
            
              Michael Alburquerque,
            
          
        
      
        
          
          
            
              Omri Keren,
            
          
        
      
        
          
          
            
              Gil Loewenthal,
            
          
        
      
        
          
          
          
            
              Tal Pupko
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In International Conference on Learning Representations (ICLR)</em>
    
    
      2023
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/iclr2023-msa.pdf" target="_blank">PDF</a>]
  
  
  
  
  
  
    [<a href="https://www.biorxiv.org/content/10.1101/2022.07.22.501063v1" target="_blank">Arxiv</a>]
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>The sequence alignment problem is one of the most fundamental problems in bioinformatics and a plethora of methods were devised to tackle it. Here we introduce BetaAlign, a methodology for aligning sequences using an NLP approach. BetaAlign accounts for the possible variability of the evolutionary process among different datasets by using an ensemble of transformers, each trained on millions of samples generated from a different evolutionary model. Our approach leads to alignment accuracy that is similar and often better than commonly used methods, such as MAFFT, DIALIGN, ClustalW, T-Coffee, PRANK, and MUSCLE.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[ICLR]</abbr>


<div id="meng:2023:memit">
  
    <span class="title">Mass-Editing Memory in a Transformer.</span>
    <span class="author">
      
        
          
          
            
              Kevin Meng,
            
          
        
      
        
          
          
            
              Arnab Sen Sharma,
            
          
        
      
        
          
          
            
              Alex Andonian,
            
          
        
      
        
          
          
            <em>Yonatan Belinkov</em>,
          
        
      
        
          
          
          
            
              <a href="https://baulab.info" target="_blank">David Bau</a>
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In International Conference on Learning Representations (ICLR, notable top-25%)</em>
    
    
      2023
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/iclr2023-memit.pdf" target="_blank">PDF</a>]
  
  
  
  
  
  
    [<a href="https://arxiv.org/abs/2210.07229" target="_blank">Arxiv</a>]
  
  
    [<a href="http://memit.baulab.info" target="_blank">URL</a>]
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Recent work has shown exciting promise in updating large language models with new memories, so as to replace obsolete information or add specialized knowledge. However, this line of work is predominantly limited to updating single associations. We develop MEMIT, a method for directly updating a language model with many memories, demonstrating experimentally that it can scale up to thousands of associations for GPT-J (6B) and GPT-NeoX (20B), exceeding prior work by an order of magnitude. Our code and data will be open-sourced upon publication.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[AAAI]</abbr>


<div id="carmeli:2023:AAAI">
  
    <span class="title">Emergent Quantized Communication.</span>
    <span class="author">
      
        
          
          
            
              Boaz Carmeli,
            
          
        
      
        
          
          
            
              Ron Meir,
            
          
        
      
        
          
          
          
            <em>Yonatan Belinkov</em>
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Proceedings of the Thirty-Seventh AAAI Conference on Artificial Intelligence (AAAI)</em>
    
    
      2023
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/aaai2023.pdf" target="_blank">PDF</a>]
  
  
  
    [<a href="https://belinkov.com/assets/pdf/aaai2023-poster.pdf" target="_blank">Poster</a>]
  
  
  
  
    [<a href="http://arxiv.org/abs/2211.02412" target="_blank">Arxiv</a>]
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>The field of emergent communication aims to understand the characteristics of communication as it emerges from artificial agents solving tasks that require information exchange. Communication with discrete messages is considered a desired characteristic, for both scientific and applied reasons. However, training a multi-agent system with discrete communication is not straightforward, requiring either reinforcement learning algorithms or relaxing the discreteness requirement via a continuous approximation such as the Gumbel-softmax. Both these solutions result in poor performance compared to fully continuous communication. In this work, we propose an alternative approach to achieve discrete communication ‚Äì quantization of communicated messages. Using message quantization allows us to train the model end-to-end, achieving superior performance in multiple setups. Moreover, quantization is a natural framework that runs the gamut from continuous to discrete communication. Thus, it sets the ground for a broader view of multi-agent communication in the deep learning era.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[NeurIPS]</abbr>


<div id="meng:2022:NeurIPS">
  
    <span class="title">Locating and Editing Factual Associations in GPT.</span>
    <span class="author">
      
        
          
          
            
              Kevin Meng*,
            
          
        
      
        
          
          
            
              <a href="https://baulab.info" target="_blank">David Bau*</a>,
            
          
        
      
        
          
          
            
              Alex Andonian,
            
          
        
      
        
          
          
          
            <em>Yonatan Belinkov</em>
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Advances in Neural Information Processing Systems (NeurIPS)</em>
    
    
      2022
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
  
  
  
  
    [<a href="https://github.com/kmeng01/rome" target="_blank">Code</a>]
  
  
    [<a href="https://arxiv.org/abs/2202.05262" target="_blank">Arxiv</a>]
  
  
    [<a href="http://rome.baulab.info" target="_blank">URL</a>]
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>We analyze the storage and recall of factual associations in autoregressive transformer language models, finding evidence that these associations correspond to localized, directly-editable computations. We first develop a causal intervention for identifying neuron activations that are decisive in a model‚Äôs factual predictions. This reveals a distinct set of steps in middle-layer feed-forward modules that mediate factual predictions while processing subject tokens. To test our hypothesis that these computations correspond to factual association recall, we modify feed-forward weights to update specific factual associations using Rank-One Model Editing (ROME).  We find that ROME is effective on a standard zero-shot relation extraction (zsRE) model-editing task, comparable to existing methods. To perform a more sensitive evaluation, we also evaluate ROME on a new dataset of counterfactual assertions, on which it simultaneously maintains both specificity and generalization, whereas other methods sacrifice one or another. Our results confirm an important role for mid-layer feed-forward modules in storing factual associations and suggest that direct manipulation of computational mechanisms may be a feasible approach for model editing. The code, dataset, visualizations, and an interactive demo notebook are available in the supplemental materials.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[EMNLP]</abbr>


<div id="zaman2022nli">
  
    <span class="title">A Multilingual Perspective Towards the Evaluation of Attribution Methods in Natural Language Inference.</span>
    <span class="author">
      
        
          
          
            
              Kerem Zaman,
            
          
        
      
        
          
          
          
            <em>Yonatan Belinkov</em>
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>
    
    
      2022
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
  
  
  
  
    [<a href="https://github.com/KeremZaman/explaiNLI" target="_blank">Code</a>]
  
  
    [<a href="https://arxiv.org/abs/2204.05428" target="_blank">Arxiv</a>]
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Most evaluations of attribution methods focus on the English language. In this work, we present a multilingual approach for evaluating attribution methods for the Natural Language Inference (NLI) task in terms of plausibility and faithfulness properties. First, we introduce a novel cross-lingual strategy to measure faithfulness based on word alignments, which eliminates the potential downsides of erasure-based evaluations. We then perform a comprehensive evaluation of attribution methods, considering different output mechanisms and aggregation methods. Finally, we augment the XNLI dataset with highlight-based explanations, providing a multilingual NLI dataset with highlights, which may support future exNLP studies. Our results show that attribution methods performing best for plausibility and faithfulness are different.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[NeurIPS]</abbr>


<div id="bansal:2022:NeurIPS">
  
    <span class="title">Measures of Information Reflect Memorization Patterns.</span>
    <span class="author">
      
        
          
          
            
              Rachit Bansal,
            
          
        
      
        
          
          
            
              Danish Pruthi,
            
          
        
      
        
          
          
          
            <em>Yonatan Belinkov</em>
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Advances in Neural Information Processing Systems (NeurIPS)</em>
    
    
      2022
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
  
  
  
  
    [<a href="https://github.com/technion-cs-nlp/Information-Reflects-Memorization" target="_blank">Code</a>]
  
  
    [<a href="https://arxiv.org/abs/2210.09404" target="_blank">Arxiv</a>]
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Neural networks are known to exploit spurious artifacts (or shortcuts) that co-occur with a target label, exhibiting heuristic memorization. On the other hand, networks have been shown to memorize training examples, resulting in example-level memorization. These kinds of memorization impede generalization of networks beyond their training distributions. Detecting such memorization could be challenging, often requiring researchers to curate tailored test sets. In this work, we hypothesize‚Äîand subsequently show‚Äîthat the diversity in the activation patterns of different neurons is reflective of model generalization and memorization. We quantify the diversity in the neural activations through information-theoretic measures and find support for our hypothesis on experiments spanning several natural language and vision tasks. Importantly, we discover that information organization points to the two forms of memorization, even for neural activations computed on unlabeled in-distribution examples. Lastly, we demonstrate the utility of our findings for the problem of model selection.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[NAACL]</abbr>


<div id="orgad:2022:NAACL">
  
    <span class="title">How Gender Debiasing Affects Internal Model Representations, and Why It Matters.</span>
    <span class="author">
      
        
          
          
            
              <a href="https://orgadhadas.github.io" target="_blank">Hadas Orgad</a>,
            
          
        
      
        
          
          
            
              Seraphina Goldfarb-Tarrant,
            
          
        
      
        
          
          
          
            <em>Yonatan Belinkov</em>
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)</em>
    
    
      2022
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/naacl2022.pdf" target="_blank">PDF</a>]
  
  
  
  
  
    [<a href="https://github.com/technion-cs-nlp/gender_internal" target="_blank">Code</a>]
  
  
    [<a href="https://arxiv.org/abs/2204.06827" target="_blank">Arxiv</a>]
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Common studies of gender bias in NLP focus either on extrinsic bias measured by model performance on a downstream task or on intrinsic bias found in models‚Äô internal representations. However, the relationship between extrinsic and intrinsic bias is relatively unknown. In this work, we illuminate this relationship by measuring both quantities together: we debias a model during downstream fine-tuning, which reduces extrinsic bias, and measure the effect on intrinsic bias, which is operationalized as bias extractability with information-theoretic probing. Through experiments on two tasks and multiple bias metrics, we show that our intrinsic bias metric is a better indicator of debiasing than (a contextual adaptation of) the standard WEAT metric, and can also expose cases of superficial debiasing. Our framework provides a comprehensive perspective on bias in NLP models, which can be applied to deploy NLP systems in a more informed manner. Our code will be made publicly available.</p>
  </span>
  
</div>
</li>
<li>

<div id="antverg:2022:DeepLoNLP">
  
    <span class="title">IDANI: Inference-time Domain Adaptation via Neuron-level Interventions.</span>
    <span class="author">
      
        
          
          
            
              Omer Antverg,
            
          
        
      
        
          
          
            
              Eyal Ben-David,
            
          
        
      
        
          
          
          
            <em>Yonatan Belinkov</em>
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Proceedings of the Third Workshop on Deep Learning for Low-Resource NLP (DeepLoNLP)</em>
    
    
      2022
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/deeplo2022.pdf" target="_blank">PDF</a>]
  
  
  
    [<a href="https://belinkov.com/assets/pdf/deeplo2022-poster.pdf" target="_blank">Poster</a>]
  
  
  
    [<a href="https://github.com/technion-cs-nlp/idani" target="_blank">Code</a>]
  
  
    [<a href="https://arxiv.org/abs/2206.00259" target="_blank">Arxiv</a>]
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Large pre-trained models are usually fine-tuned on downstream task data, and tested on unseen data. When the train and test data come from different domains, the model is likely to struggle, as it is not adapted to the test domain. We propose a new approach for domain adaptation (DA), using neuron-level interventions: We modify the representation of each test example in specific neurons, resulting in a counterfactual example from the source domain, which the model is more familiar with. The modified example is then fed back into the model. While most other DA methods are applied during training time, ours is applied during inference only, making it more efficient and applicable. Our experiments show that our method improves performance on unseen domains.</p>
  </span>
  
</div>
</li>
<li>

<div id="orgad:2022:GeBNLP">
  
    <span class="title">Choose Your Lenses: Flaws in Gender Bias Evaluation.</span>
    <span class="author">
      
        
          
          
            
              <a href="https://orgadhadas.github.io" target="_blank">Hadas Orgad</a>,
            
          
        
      
        
          
          
          
            <em>Yonatan Belinkov</em>
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Proceedings of the Fourth Workshop on Gender Bias in NLP (GeBNLP)</em>
    
    
      2022
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/genbnlp2022.pdf" target="_blank">PDF</a>]
  
  
  
  
  
  
    [<a href="https://arxiv.org/abs/2210.11471" target="_blank">Arxiv</a>]
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Considerable efforts to measure and mitigate gender bias in recent years have led to the introduction of an abundance of tasks, datasets, and metrics used in this vein. 
In this position paper, we assess the current paradigm of gender bias evaluation and identify several flaws in it. First, we highlight the importance of extrinsic bias metrics that measure how a model‚Äôs performance on some task is affected by gender, as opposed to intrinsic evaluations of model representations, which are less strongly connected to specific harms to people interacting with systems. Second, we find that datasets and metrics are often coupled, and discuss how their coupling hinders the ability to obtain reliable conclusions, and how one may decouple them. We then investigate the effect of the chosen dataset or metric on bias measurement, finding significant variations across each of them. Finally, we propose several guidelines for more reliable gender bias evaluation.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[*SEM]</abbr>


<div id="asael:2022:starsem">
  
    <span class="title">A Generative Approach for Mitigating Structural Biases in Natural Language Inference.</span>
    <span class="author">
      
        
          
          
            
              Dimion Asael,
            
          
        
      
        
          
          
            
              Zachary Ziegler,
            
          
        
      
        
          
          
          
            <em>Yonatan Belinkov</em>
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Proceedings of the Eleventh Joint Conference on Lexical and Computational Semantics (*SEM)</em>
    
    
      2022
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/starsem2022.pdf" target="_blank">PDF</a>]
  
  
  
  
  
    [<a href="https://github.com/technion-cs-nlp/Generative-NLI" target="_blank">Code</a>]
  
  
    [<a href="https://arxiv.org/abs/2108.14006" target="_blank">Arxiv</a>]
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Many natural language inference (NLI) datasets contain biases that allow models to perform well by only using a biased subset of the input, without considering the remainder features. For instance, models are able to classify samples by only using the hypothesis, without learning the true relationship between it and the premise. These structural biases lead discriminative models to learn unintended superficial features and generalize poorly out of the training distribution. In this work, we reformulate NLI as a generative task, where a model is conditioned on the biased subset of the input and the label and generates the remaining subset of the input. We show that by imposing a uniform prior, we obtain a provably unbiased model. Through synthetic experiments, we find this approach to be highly robust to large amounts of bias. We then demonstrate empirically on two types of natural bias that this approach leads to fully unbiased models in practice. However, we find that generative models are difficult to train and generally perform worse than discriminative baselines. We highlight the difficulty of the generative modeling task in the context of NLI as a cause for this worse performance. Finally, by fine-tuning the generative model with a discriminative objective, we reduce the performance gap between the generative model and the discriminative baseline, while allowing for a small amount of bias.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[ICLR]</abbr>


<div id="antverg:2022:iclr">
  
    <span class="title">On the Pitfalls of Analyzing Individual Neurons in Language Models.</span>
    <span class="author">
      
        
          
          
            
              Omer Antverg,
            
          
        
      
        
          
          
          
            <em>Yonatan Belinkov</em>
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In International Conference on Learning Representations (ICLR)</em>
    
    
      2022
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/iclr2022.pdf" target="_blank">PDF</a>]
  
  
  
    [<a href="https://belinkov.com/assets/pdf/iclr2022-poster.pdf" target="_blank">Poster</a>]
  
  
  
    [<a href="https://github.com/technion-cs-nlp/Individual-Neurons-Pitfalls" target="_blank">Code</a>]
  
  
    [<a href="https://arxiv.org/abs/2110.07483" target="_blank">Arxiv</a>]
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>While many studies have shown that linguistic information is encoded in hidden word representations, few have studied individual neurons, to show how and in which neurons it is encoded. Among these, the common approach is to use an external probe to rank neurons according to their relevance to some linguistic attribute, and to evaluate the obtained ranking using the same probe that produced it. We show two pitfalls in this methodology: 1. It confounds distinct factors: probe quality and ranking quality. We separate them and draw conclusions on each. 2. It focuses on encoded information, rather than information that is used by the model. We show that these are not the same. We compare two recent ranking methods and a simple one we introduce, and evaluate them with regard to both of these aspects.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[AAAI]</abbr>


<div id="stacey:2022:AAAI">
  
    <span class="title">Supervising Model Attention with Human Explanations for Robust Natural Language Inference.</span>
    <span class="author">
      
        
          
          
            
              Joe Stacey,
            
          
        
      
        
          
          
            <em>Yonatan Belinkov</em>,
          
        
      
        
          
          
          
            
              Marek Rei
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Proceedings of the Thirty-Sixth AAAI Conference on Artificial Intelligence (AAAI)</em>
    
    
      2022
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/aaai2022.pdf" target="_blank">PDF</a>]
  
  
  
  
  
    [<a href="https://github.com/joestacey/NLI_with_a_human_touch" target="_blank">Code</a>]
  
  
    [<a href="https://arxiv.org/abs/2104.08142" target="_blank">Arxiv</a>]
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Natural Language Inference (NLI) models are known to learn from biases and artefacts within their training data, impacting how well they generalise to other unseen datasets. Existing de-biasing approaches focus on preventing the models from learning these biases, which can result in restrictive models and lower performance. We instead investigate teaching the model how a human would approach the NLI task, in order to learn features that will generalise better to previously unseen examples. Using natural language explanations, we supervise the model‚Äôs attention weights to encourage more attention to be paid to the words present in the explanations, significantly improving model performance. Our experiments show that the in-distribution improvements of this method are also accompanied by out-of-distribution improvements, with the supervised models learning from features that generalise better to other NLI datasets. Analysis of the model indicates that human explanations encourage increased attention on the important words, with more attention paid to words in the premise and less attention paid to punctuation and stop-words.</p>
  </span>
  
</div>
</li>
<li>

<div id="belinkov:2022:aima">
  
    <span class="title">Large-Scale Electronic Corpora and the Study of Middle and Mixed Arabic.</span>
    <span class="author">
      
        
          
          
          
            <em>Yonatan Belinkov</em>
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Middle and Mixed Arabic over Time and across Written and Oral Genres: From Legal Documents to Television and Internet through Literature. Proceedings of the IVth AIMA International Conference (Emory University, Atlanta, GA, USA, 12‚Äì15 October 2013)</em>
    
    
      2022
    
    </span>
    

    
  

  

  <span class="links">
  
  
    [<a href="https://belinkov.com/assets/pdf/aima2021.pdf" target="_blank">PDF</a>]
  
  
  
  
  
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
</div>
</li>
<li>
  <abbr>[NeurIPS]</abbr>


<div id="dranker:2021:neurips">
  
    <span class="title">IRM‚Äîwhen it works and when it doesn‚Äôt: A test case of natural language inference.</span>
    <span class="author">
      
        
          
          
            
              Yana Dranker,
            
          
        
      
        
          
          
            
              He He,
            
          
        
      
        
          
          
          
            <em>Yonatan Belinkov</em>
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Advances in Neural Information Processing Systems (NeurIPS)</em>
    
    
      2021
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/neurips2021.pdf" target="_blank">PDF</a>]
  
  
  
    [<a href="https://belinkov.com/assets/pdf/neurips2021-poster.pdf" target="_blank">Poster</a>]
  
  
  
    [<a href="https://github.com/technion-cs-nlp/irm-for-nli" target="_blank">Code</a>]
  
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Invariant Risk Minimization (IRM) is a recently proposed framework for out- of-distribution (o.o.d) generalization. Most of the studies on IRM so far have focused on theoretical results, toy problems, and simple models. In this work, we investigate the applicability of IRM to bias mitigation‚Äîa special case of o.o.d generalization‚Äîin increasingly naturalistic settings and deep models. Using natural language inference (NLI) as a test case, we start with a setting where both the dataset and the bias are synthetic, continue with a natural dataset and synthetic bias, and end with a fully realistic setting with natural datasets and bias. Our results show that in naturalistic settings, learning complex features in place of the bias proves to be difficult, leading to a rather small improvement over empirical risk minimization. Moreover, we find that in addition to being sensitive to random seeds, the performance of IRM also depends on several critical factors, notably dataset size, bias prevalence, and bias strength, thus limiting IRM‚Äôs advantage in practical scenarios. Our results highlight key challenges in applying IRM to real-world scenarios, calling for a more naturalistic characterization of the problem setup for o.o.d generalization.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[EMNLP]</abbr>


<div id="Mendelson:2021:emnlp">
  
    <span class="title">Debiasing Methods in Natural Language Understanding Make Bias More Accessible.</span>
    <span class="author">
      
        
          
          
            
              Michael Mendelson,
            
          
        
      
        
          
          
          
            <em>Yonatan Belinkov</em>
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>
    
    
      2021
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/emnlp2021.pdf" target="_blank">PDF</a>]
  
  
  
  
  
    [<a href="https://github.com/technion-cs-nlp/bias-probing" target="_blank">Code</a>]
  
  
    [<a href="https://arxiv.org/abs/2109.04095" target="_blank">Arxiv</a>]
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Model robustness to bias is often determined by the generalization on carefully designed out-of-distribution datasets. Recent debiasing methods in natural language understanding (NLU) improve performance on such datasets by pressuring models into making unbiased predictions. An underlying assumption behind such methods is that this also leads to the discovery of more robust features in the model‚Äôs inner representations. We propose a general probing-based framework that allows for post-hoc interpretation of biases in language models, and use an information-theoretic approach to measure the extractability of certain biases from the model‚Äôs representations. We experiment with several NLU datasets and known biases, and show that, counter-intuitively, the more a language model is pushed towards a debiased regime, the more bias is actually encoded in its inner representations.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[ACL]</abbr>


<div id="finlayson-meuller:2021:acl">
  
    <span class="title">Causal Analysis of Syntactic Agreement Mechanisms in Neural Language Models.</span>
    <span class="author">
      
        
          
          
            
              Matthew Finlayson*,
            
          
        
      
        
          
          
            
              Aaron Mueller*,
            
          
        
      
        
          
          
            
              <a href="https://www.sebastiangehrmann.com" target="_blank">Sebastian Gehrmann</a>,
            
          
        
      
        
          
          
            
              <a href="http://www.eecs.harvard.edu/~shieber/index.html" target="_blank">Stuart Shieber</a>,
            
          
        
      
        
          
          
            
              Tal Linzen,
            
          
        
      
        
          
          
          
            <em>Yonatan Belinkov</em>
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Proceedings of the Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (ACL-IJCNLP)</em>
    
    
      2021
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/acl2021.pdf" target="_blank">PDF</a>]
  
  
  
  
  
    [<a href="https://github.com/mattf1n/lm- intervention" target="_blank">Code</a>]
  
  
    [<a href="http://arxiv.org/abs/2106.06087" target="_blank">Arxiv</a>]
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Targeted syntactic evaluations have demonstrated the ability of language models to perform subject-verb agreement given difficult contexts. To elucidate the mechanisms by which the models accomplish this behavior, this study applies causal mediation analysis to pre-trained neural language models. We investigate the magnitude of models‚Äô preferences for grammatical inflections, as well as whether neurons process subject-verb agreement similarly across sentences with different syntactic structures. We uncover similarities and differences across architectures and model sizes‚Äînotably, that larger models do not necessarily learn stronger preferences. We also observe two distinct mechanisms for producing subject-verb agreement depending on the syntactic structure of the input sentence. Finally, we find that language models rely on similar sets of neurons when given sentences with similar syntactic structure.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[ICASSP]</abbr>


<div id="chung:2021:icassp">
  
    <span class="title">Similarity Analysis of Self-Supervised Speech Representations.</span>
    <span class="author">
      
        
          
          
            
              Yu-An Chung,
            
          
        
      
        
          
          
            <em>Yonatan Belinkov</em>,
          
        
      
        
          
          
          
            
              <a href="http://people.csail.mit.edu/jrg/" target="_blank">James Glass</a>
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>
    
    
      2021
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/icassp2021.pdf" target="_blank">PDF</a>]
  
  
  
  
  
  
    [<a href="http://arxiv.org/abs/2010.11481" target="_blank">Arxiv</a>]
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Self-supervised speech representation learning has recently been a prosperous research topic. Many algorithms have been proposed for learning useful representations from large-scale unlabeled data, and their applications to a wide range of speech tasks have also been investigated. However, there has been little research focusing on understanding the properties of existing approaches. In this work, we aim to provide a comparative study of some of the most representative self-supervised algorithms. Specifically, we quantify the similarities between different self-supervised representations using existing similarity measures. We also design probing tasks to study the correlation between the models‚Äô pre-training loss and the amount of specific speech information contained in their learned representations. In addition to showing how various self-supervised models behave differently given the same input, our study also finds that the training objective has a higher impact on representation similarity than architectural choices such as building blocks¬†(RNN/Transformer/CNN) and directionality¬†(uni/bidirectional). Our results also suggest that there exists a strong correlation between pre-training loss and downstream performance for some self-supervised algorithms.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[ICLR]</abbr>


<div id="sanh:2021:iclr">
  
    <span class="title">Learning from others‚Äô mistakes: Avoiding dataset biases without modeling them.</span>
    <span class="author">
      
        
          
          
            
              Victor Sanh,
            
          
        
      
        
          
          
            
              Thomas Wolf,
            
          
        
      
        
          
          
            <em>Yonatan Belinkov</em>,
          
        
      
        
          
          
          
            
              Alexander M. Rush
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In International Conference on Learning Representations (ICLR)</em>
    
    
      2021
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/iclr2021-implicit.pdf" target="_blank">PDF</a>]
  
  
  
  
  
  
    [<a href="https://arxiv.org/abs/2012.01300" target="_blank">Arxiv</a>]
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>State-of-the-art natural language processing (NLP) models often learn to model dataset biases and surface form correlations instead of features that target the intended underlying task. Previous work has demonstrated effective methods to circumvent these issues when knowledge of the bias is available. We consider cases where the bias issues may not be explicitly identified, and show a method for training models that learn to ignore these problematic correlations. Our approach relies on the observation that models with limited capacity primarily learn to exploit biases in the dataset. We can leverage the errors of such limited capacity models to train a more robust model in a product of experts, thus bypassing the need to hand-craft a biased model. We show the effectiveness of this method to retain improvements in out-of-distribution settings even if no particular bias is targeted by the biased model.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[ICLR]</abbr>


<div id="mahabadi:2021:iclr">
  
    <span class="title">Variational Information Bottleneck for Effective Low-Resource Fine-Tuning.</span>
    <span class="author">
      
        
          
          
            
              Rabeeh Karimi Mahabadi,
            
          
        
      
        
          
          
            <em>Yonatan Belinkov</em>,
          
        
      
        
          
          
          
            
              James Henderson
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In International Conference on Learning Representations (ICLR)</em>
    
    
      2021
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/iclr2021-vib.pdf" target="_blank">PDF</a>]
  
  
  
  
  
    [<a href="https://github.com/rabeehk/vibert" target="_blank">Code</a>]
  
  
    [<a href="https://arxiv.org/abs/2106.05469" target="_blank">Arxiv</a>]
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>While large-scale pretrained language models have obtained impressive results when fine-tuned on a wide variety of tasks, they still often suffer from overfitting in low-resource scenarios. Since such models are general-purpose feature extractors, many of these features are inevitably irrelevant for a given target task.  We propose to use Variational Information Bottleneck (VIB) to suppress irrelevant features when fine-tuning on low-resource target tasks, and show that our method successfully reduces overfitting.  Moreover, we show that our VIB model finds sentence representations that are more robust to biases in natural language inference datasets, and thereby obtains better generalization to out-of-domain datasets. Evaluation on seven low-resource datasets in different tasks shows that our method significantly improves transfer learning in low-resource scenarios, surpassing prior work. Moreover, it improves generalization on 13 out of 15 out-of-domain natural language inference benchmarks.  Our code is publicly available in https://github.com/rabeehk/vibert.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[EACL]</abbr>


<div id="ravichander:2021:eacl">
  
    <span class="title">Probing the Probing Paradigm: Does Probing Accuracy Entail Task Relevance?.</span>
    <span class="author">
      
        
          
          
            
              Abhilasha Ravichander,
            
          
        
      
        
          
          
            <em>Yonatan Belinkov</em>,
          
        
      
        
          
          
          
            
              Eduard Hovy
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics (EACL)</em>
    
    
      2021
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/eacl2021.pdf" target="_blank">PDF</a>]
  
  
  
  
  
  
    [<a href="https://arxiv.org/abs/2005.00719" target="_blank">Arxiv</a>]
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Although neural models have achieved impressive results on several NLP benchmarks, little is understood about the mechanisms they use to perform language tasks. Thus, much recent attention has been devoted to analyzing the sentence representations learned by neural encoders, through the lens of ‚Äòprobing‚Äô tasks. However, to what extent was the information encoded in sentence representations, as discovered through a probe, actually used by the model to perform its task? In this work, we examine this probing paradigm through a case study in Natural Language Inference, showing that models can learn to encode linguistic properties even if they are not needed for the task on which the model was trained. We further identify that pretrained word embeddings play a considerable role in encoding these properties rather than the training task itself, highlighting the importance of careful controls when designing probing experiments. Finally, through a set of controlled synthetic tasks, we demonstrate models can encode these properties considerably above chance-level even when distributed in the data as random noise, calling into question the interpretation of absolute claims on probing tasks.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[NeurIPS]</abbr>


<div id="vig:2020:neurips">
  
    <span class="title">Investigating Gender Bias in Language Models Using Causal Mediation Analysis.</span>
    <span class="author">
      
        
          
          
            
              Jesse Vig*,
            
          
        
      
        
          
          
            
              <a href="https://www.sebastiangehrmann.com" target="_blank">Sebastian Gehrmann*</a>,
            
          
        
      
        
          
          
            <em>Yonatan Belinkov*</em>,
          
        
      
        
          
          
            
              Sharon Qian,
            
          
        
      
        
          
          
            
              Daniel Nevo,
            
          
        
      
        
          
          
            
              Yaron Singer,
            
          
        
      
        
          
          
          
            
              <a href="http://www.eecs.harvard.edu/~shieber/index.html" target="_blank">Stuart Shieber</a>
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Advances in Neural Information Processing Systems (NeurIPS, Spotlight presentation)</em>
    
    
      2020
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/neurips2020.pdf" target="_blank">PDF</a>]
  
  
  
  
  
    [<a href="https://github.com/sebastianGehrmann/CausalMediationAnalysis" target="_blank">Code</a>]
  
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Many interpretation methods for neural models in natural language processing investigate how information is encoded inside hidden representations. However, these methods can only measure whether the information exists, not whether it is actually used by the model.  We propose a methodology grounded in the theory of causal mediation analysis for interpreting which parts of a model are causally implicated in its behavior. The approach enables us to analyze the mechanisms which facilitate the flow of information from input to output through various model components, known as mediators. As a case study, we apply this methodology to analyzing gender bias in pre-trained Transformer language models. We study the role of individual neurons and attention heads in mediating gender bias across three datasets designed to gauge a model‚Äôs sensitivity to gender bias. Our mediation analysis reveals that gender bias effects are concentrated in specific components of the model that may exhibit highly specialized behavior.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[WMT]</abbr>


<div id="li-etal-2019-findingt">
  
    <span class="title">Findings of the WMT 2020 Shared Task on Machine Translation Robustness.</span>
    <span class="author">
      
        
          
          
            
              Lucia Specia,
            
          
        
      
        
          
          
            
              Zhenhao Li,
            
          
        
      
        
          
          
            
              Juan Pino,
            
          
        
      
        
          
          
            
              Vishrav Chaudhary,
            
          
        
      
        
          
          
            
              Guzm√°n Guzman,
            
          
        
      
        
          
          
            
              Graham Neubig,
            
          
        
      
        
          
          
            
              <a href="http://alt.qcri.org/~ndurrani/" target="_blank">Nadir Durrani</a>,
            
          
        
      
        
          
          
            <em>Yonatan Belinkov</em>,
          
        
      
        
          
          
            
              Philipp Koehn,
            
          
        
      
        
          
          
            
              <a href="https://hsajjad.github.io" target="_blank">Hassan Sajjad</a>,
            
          
        
      
        
          
          
            
              Paul Michel,
            
          
        
      
        
          
          
          
            
              Xian Li
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Proceedings of the Fifth Conference on Machine Translation</em>
    
    
      2020
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/wmt2020-robustness.pdf" target="_blank">PDF</a>]
  
  
  
  
  
  
  
    [<a href="http://www.statmt.org/wmt20/robustness.html" target="_blank">URL</a>]
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>We report the findings of the second edition of the shared task on improving robustness in Machine Translation (MT). The task aims to test current machine translation systems in their ability to handle challenges facing MT models to be deployed in the real world, including domain diversity and non-standard texts common in user generated content, especially in social media. We cover two language pairs ‚Äì English-German and English-Japanese and provide test sets in zero-shot and few-shot variants. Participating systems are evaluated both automatically and manually, with an additional human evaluation for ‚Äúcatastrophic errors‚Äù. We received 59 submissions by 11 participating teams from a variety of types of institutions.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[EMNLP]</abbr>


<div id="durrani:2020:emnlp">
  
    <span class="title">Analyzing Individual Neurons in Pre-trained Language Models.</span>
    <span class="author">
      
        
          
          
            
              <a href="http://alt.qcri.org/~ndurrani/" target="_blank">Nadir Durrani</a>,
            
          
        
      
        
          
          
            
              <a href="https://hsajjad.github.io" target="_blank">Hassan Sajjad</a>,
            
          
        
      
        
          
          
            
              <a href="https://fdalvi.github.io" target="_blank">Fahim Dalvi</a>,
            
          
        
      
        
          
          
          
            <em>Yonatan Belinkov</em>
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>
    
    
      2020
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/emnlp2020-neurons.pdf" target="_blank">PDF</a>]
  
  
  
  
  
  
    [<a href="https://arxiv.org/abs/2010.02695" target="_blank">Arxiv</a>]
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>While a lot of analysis has been carried to demonstrate linguistic knowledge captured by the representations learned within deep NLP models, very little attention has been paid towards individual neurons. We carry out a neuron-level analysis using core linguistic tasks of predicting morphology, syntax and semantics, on pre-trained language models, with questions like: i) do individual neurons in pre-trained models capture linguistic information? ii) which parts of the network learn more about certain linguistic phenomena? iii) how distributed or focused is the information? and iv) how do various architectures differ in learning these properties? We found small subsets of neurons to predict linguistic tasks, with lower level tasks (such as morphology) localized in fewer neurons, compared to higher level task of predicting syntax. Our study also reveals interesting cross architectural comparisons. For example, we found neurons in XLNet to be more localized and disjoint when predicting properties compared to BERT and others, where they are more distributed and coupled.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[EMNLP]</abbr>


<div id="dalvi:2020:emnlp">
  
    <span class="title">Analyzing Redundancy in Pretrained Transformer Models.</span>
    <span class="author">
      
        
          
          
            
              <a href="https://fdalvi.github.io" target="_blank">Fahim Dalvi</a>,
            
          
        
      
        
          
          
            
              <a href="https://hsajjad.github.io" target="_blank">Hassan Sajjad</a>,
            
          
        
      
        
          
          
            
              <a href="http://alt.qcri.org/~ndurrani/" target="_blank">Nadir Durrani</a>,
            
          
        
      
        
          
          
          
            <em>Yonatan Belinkov</em>
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>
    
    
      2020
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/emnlp2020-redundancy.pdf" target="_blank">PDF</a>]
  
  
  
  
  
  
    [<a href="https://arxiv.org/abs/2004.04010" target="_blank">Arxiv</a>]
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Transformer-based deep NLP models are trained using hundreds of millions of parameters, limiting their applicability in computationally constrained environments. In this paper, we study the cause of these limitations by defining a notion of Redundancy, which we categorize into two classes: General Redundancy and Task-specific Redundancy. We dissect two popular pretrained models, BERT and XLNet, studying how much redundancy they exhibit at a representation-level and at a more fine-grained neuron-level. Our analysis reveals interesting insights, such as: i) 85% of the neurons across the network are redundant and ii) at least 92% of them can be removed when optimizing towards a downstream task. Based on our analysis, we present an efficient feature-based transfer learning procedure, which maintains 97% performance while using at-most 10% of the original neurons.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[ACL]</abbr>


<div id="wu:2020:acl">
  
    <span class="title">Similarity Analysis of Contextual Word Representation Models.</span>
    <span class="author">
      
        
          
          
            
              John M. Wu*,
            
          
        
      
        
          
          
            <em>Yonatan Belinkov*</em>,
          
        
      
        
          
          
            
              <a href="https://hsajjad.github.io" target="_blank">Hassan Sajjad</a>,
            
          
        
      
        
          
          
            
              <a href="http://alt.qcri.org/~ndurrani/" target="_blank">Nadir Durrani</a>,
            
          
        
      
        
          
          
            
              <a href="https://fdalvi.github.io" target="_blank">Fahim Dalvi</a>,
            
          
        
      
        
          
          
          
            
              <a href="http://people.csail.mit.edu/jrg/" target="_blank">James Glass</a>
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL)</em>
    
    
      2020
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/acl2020.pdf" target="_blank">PDF</a>]
  
  
  
  
  
    [<a href="https://github.com/johnmwu/contextual-corr-analysis" target="_blank">Code</a>]
  
  
    [<a href="https://arxiv.org/abs/2005.01172" target="_blank">Arxiv</a>]
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>This paper investigates contextual word representation models from the lens of similarity analysis. Given a collection of trained models, we measure the similarity of their internal representations and attention. Critically, these models come from  vastly different architectures. We use existing and novel similarity measures that aim to gauge the level of localization of information in the deep models, and facilitate the investigation of which design factors affect model similarity, without requiring any external linguistic annotation.  The analysis reveals that  models within the same family are more similar to one another, as may be expected. Surprisingly, different architectures  have rather similar representations, but different individual neurons. We also observed differences in information localization in lower and higher layers and found that higher layers are more affected by fine-tuning on downstream tasks.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[ACL]</abbr>


<div id="Mahabadi:2020:acl">
  
    <span class="title">End-to-End Bias Mitigation by Modelling Biases in Corpora.</span>
    <span class="author">
      
        
          
          
            
              Rabeeh Karimi Mahabadi,
            
          
        
      
        
          
          
            <em>Yonatan Belinkov</em>,
          
        
      
        
          
          
          
            
              James Henderson
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL)</em>
    
    
      2020
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/acl2020-biases.pdf" target="_blank">PDF</a>]
  
  
  
  
  
    [<a href="https://github.com/rabeehk/robust-nli" target="_blank">Code</a>]
  
  
    [<a href="http://arxiv.org/abs/1909.06321" target="_blank">Arxiv</a>]
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Several recent studies have shown that strong natural language understanding
(NLU) models are prone to relying on unwanted dataset biases without learning
the underlying task, resulting in models that fail to generalize to
out-of-domain datasets and are likely to perform poorly in real-world
scenarios. We propose two learning strategies to train neural models, which are
more robust to such biases and transfer better to out-of-domain datasets. The
biases are specified in terms of one or more bias-only models, which learn to
leverage the dataset biases. During training, the bias-only models‚Äô predictions
are used to adjust the loss of the base model to reduce its reliance on biases
by down-weighting the biased examples and focusing the training on the hard
examples. We experiment on large-scale natural language inference and fact
verification benchmarks, evaluating on out-of-domain datasets that are
specifically designed to assess the robustness of models against known biases
in the training data. Results show that our debiasing methods greatly improve
robustness in all settings and better transfer to other textual entailment
datasets. Our code and data are publicly available in
\urlhttps://github.com/rabeehk/robust-nli.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[ACL]</abbr>


<div id="abdou:2020:acl">
  
    <span class="title">The Sensitivity of Language Models and Humans to Winograd Schema Perturbations.</span>
    <span class="author">
      
        
          
          
            
              Mostafa Abdou,
            
          
        
      
        
          
          
            
              Vinit Ravishankar,
            
          
        
      
        
          
          
            
              Maria Barrett,
            
          
        
      
        
          
          
            <em>Yonatan Belinkov</em>,
          
        
      
        
          
          
            
              Desmond Elliott,
            
          
        
      
        
          
          
          
            
              Anders S√∏gaard
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL)</em>
    
    
      2020
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/acl2020-winograd.pdf" target="_blank">PDF</a>]
  
  
  
  
  
  
    [<a href="https://arxiv.org/abs/2005.01348" target="_blank">Arxiv</a>]
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Large-scale pretrained language models are the major driving force behind recent improvements in performance on the Winograd Schema Challenge, a widely employed test of commonsense reasoning ability. We show, however, with a new diagnostic dataset, that these models are sensitive to linguistic perturbations of the Winograd examples that minimally affect human understanding. Our results highlight interesting differences between humans and language models: language models are more sensitive to a number or gender alternations and synonym replacements than humans, and humans are more stable and consistent in their predictions, maintain a much higher absolute performance, and perform better on non-associative instances than associative ones. 
Overall, humans are correct more often than out-of-the-box models, and the models are sometimes right for the wrong reasons. Finally, we show that fine-tuning on a large, task-specific dataset can offer a solution to these issues.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[ICLR]</abbr>


<div id="Rosenfeld2019ACP">
  
    <span class="title">A Constructive Prediction of the Generalization Error Across Scales.</span>
    <span class="author">
      
        
          
          
            
              Jonathan S. Rosenfeld,
            
          
        
      
        
          
          
            
              Amir Rosenfeld,
            
          
        
      
        
          
          
            <em>Yonatan Belinkov</em>,
          
        
      
        
          
          
          
            
              Nir Shavit
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In International Conference on Learning Representations (ICLR)</em>
    
    
      2020
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/iclr2020.pdf" target="_blank">PDF</a>]
  
  
  
  
  
  
    [<a href="https://arxiv.org/abs/1909.12673" target="_blank">Arxiv</a>]
  
  
   
  
    [Media: <a href="https://www.csail.mit.edu/news/predicting-how-well-neural-networks-will-scale" target="_blank">MIT CSAIL News</a>, <a href="https://info.deeplearning.ai/the-batch-tracking-chinas-covid-19-revival-a-robot-star-is-born-discovering-new-antibiotics-rightsizing-neural-networks" target="_blank">The Batch</a>] 
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>The dependency of the generalization error of neural networks on model and dataset size is of critical importance both in practice and for understanding the theory of neural networks. Nevertheless, the functional form of this dependency remains elusive. In this work, we present a functional form which approximates well the generalization error in practice. Capitalizing on the successful concept of model scaling (e.g., width, depth), we are able to simultaneously construct such a form and specify the exact models which can attain it across model/data scales. Our construction follows insights obtained from observations conducted over a range of model/data scales, in various model types and datasets, in vision and language tasks. We show that the form both fits the observations well across scales, and provides accurate predictions from small- to large-scale models and data.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[Interspeech]</abbr>


<div id="belinkov:2019:interpseech">
  
    <span class="title">Analyzing Phonetic and Graphemic Representations in End-to-End Automatic Speech Recognition.</span>
    <span class="author">
      
        
          
          
            <em>Yonatan Belinkov</em>,
          
        
      
        
          
          
            
              Ahmed Ali,
            
          
        
      
        
          
          
          
            
              <a href="http://people.csail.mit.edu/jrg/" target="_blank">James Glass</a>
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Proceedings of Interspeech</em>
    
    
      2019
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/interspeech2019.pdf" target="_blank">PDF</a>]
  
  
  
  
  
  
    [<a href="https://arxiv.org/abs/1907.04224" target="_blank">Arxiv</a>]
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>End-to-end neural network systems for automatic speech recognition (ASR) are trained from acoustic features to text transcriptions. In contrast to modular ASR systems, which contain separately-trained components for acoustic modeling, pronunciation lexicon, and language modeling, the end-to-end paradigm is both conceptually simpler and has the potential benefit of training the entire system on the end task. However, such neural network models are more opaque: it is not clear how to interpret the role of different parts of the network and what information it learns during training. In this paper, we analyze the learned internal representations in an end-to-end ASR model. We evaluate the representation quality in terms of several classification tasks, comparing phonemes and graphemes, as well as different articulatory features. We study two languages (English and Arabic) and three datasets, finding remarkable consistency in how different properties are represented in different layers of the deep neural network.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[WMT]</abbr>


<div id="li-etal-2019-findings">
  
    <span class="title">Findings of the First Shared Task on Machine Translation Robustness.</span>
    <span class="author">
      
        
          
          
            
              Xian Li,
            
          
        
      
        
          
          
            
              Paul Michel,
            
          
        
      
        
          
          
            
              Antonios Anastasopoulos,
            
          
        
      
        
          
          
            <em>Yonatan Belinkov</em>,
          
        
      
        
          
          
            
              <a href="http://alt.qcri.org/~ndurrani/" target="_blank">Nadir Durrani</a>,
            
          
        
      
        
          
          
            
              Orhan Firat,
            
          
        
      
        
          
          
            
              Philipp Koehn,
            
          
        
      
        
          
          
            
              Graham Neubig,
            
          
        
      
        
          
          
            
              Juan Pino,
            
          
        
      
        
          
          
          
            
              <a href="https://hsajjad.github.io" target="_blank">Hassan Sajjad</a>
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Proceedings of the Fourth Conference on Machine Translation</em>
    
    
      2019
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/wmt2019-robustness.pdf" target="_blank">PDF</a>]
  
  
  
  
  
  
  
    [<a href="http://www.statmt.org/wmt19/robustness.html" target="_blank">URL</a>]
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>We share the findings of the first shared task on improving robustness of Machine Translation (MT). The task provides a testbed representing challenges facing MT models deployed in the real world, and facilitates new approaches to improve models‚Äô robustness to noisy input and domain mismatch. We focus on two language pairs (English-French and English-Japanese), and the submitted systems are evaluated on a blind test set consisting of noisy comments on Reddit and professionally sourced translations. As a new task, we received 23 submissions by 11 participating teams from universities, companies, national labs, etc. All submitted systems achieved large improvements over baselines, with the best improvement having +22.33 BLEU. We evaluated submissions by both human judgment and automatic evaluation (BLEU), which shows high correlations (Pearson‚Äôs r = 0.94 and 0.95). Furthermore, we conducted a qualitative analysis of the submitted systems using compare-mt, which revealed their salient differences in handling challenges in this task. Such analysis provides additional insights when there is occasional disagreement between human judgment and BLEU, e.g. systems better at producing colloquial expressions received higher score from human judgment.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[ACL]</abbr>


<div id="luo:2019:acl">
  
    <span class="title">Improving Neural Language Models by Segmenting, Attending, and Predicting the Future.</span>
    <span class="author">
      
        
          
          
            
              Hongyin Luo,
            
          
        
      
        
          
          
            
              Lan Jiang,
            
          
        
      
        
          
          
            <em>Yonatan Belinkov</em>,
          
        
      
        
          
          
          
            
              <a href="http://people.csail.mit.edu/jrg/" target="_blank">James Glass</a>
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (ACL)</em>
    
    
      2019
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/acl2019-lm.pdf" target="_blank">PDF</a>]
  
  
  
  
  
  
    [<a href="https://arxiv.org/abs/1906.01702" target="_blank">Arxiv</a>]
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Common language models typically predict the next word given the context. In this work, we propose a method that improves language modeling by learning to align the given context and the following phrase. The model does not require any linguistic annotation of phrase segmentation. Instead, we define syntactic heights and phrase segmentation rules, enabling the model to automatically induce phrases, recognize their task-specific heads, and generate phrase embeddings in an unsupervised learning manner. Our method can easily be applied to language models with different network architectures since an independent module is used for phrase induction and context-phrase alignment, and no change is required in the underlying language modeling network. Experiments have shown that our model outperformed several strong baseline models on different data sets. We achieved a new state-of-the-art performance of 17.4 perplexity on the Wikitext-103 dataset. Additionally, visualizing the outputs of the phrase induction module showed that our model is able to learn approximate phrase-level structural knowledge without any annotation.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[CogSci]</abbr>


<div id="hahn:2019:surprisal">
  
    <span class="title">Character-based Surprisal as a Model of Human Reading in the Presence of Errors.</span>
    <span class="author">
      
        
          
          
            
              Michael Hahn,
            
          
        
      
        
          
          
            
              Frank Keller,
            
          
        
      
        
          
          
            
              <a href="http://yonatanbisk.com" target="_blank">Yonatan Bisk</a>,
            
          
        
      
        
          
          
          
            <em>Yonatan Belinkov</em>
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Proceedings of the 41st Annual Meeting of the Cognitive Science Society (CogSci, Oral presentation)</em>
    
    
      2019
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/cogsci2019.pdf" target="_blank">PDF</a>]
  
  
  
  
  
  
    [<a href="https://arxiv.org/abs/1902.00595" target="_blank">Arxiv</a>]
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Intuitively, human readers cope easily with errors in text; typos, misspelling, word substitutions, etc. do not unduly disrupt natural reading. Previous work indicates that letter transpositions result in increased reading times, but it is unclear if this effect generalizes to more natural errors. In this paper, we report an eye-tracking study that compares two error types (letter transpositions and naturally occurring misspelling) and two error rates (10% or 50% of all words contain errors). We find that human readers show unimpaired comprehension in spite of these errors, but error words cause more reading difficulty than correct words. Also, transpositions are more difficult than misspellings, and a high error rate increases difficulty for all words, including correct ones. We then present a computational model that uses character-based (rather than traditional word-based) surprisal to account for these results. The model explains that transpositions are harder than misspellings because they contain unexpected letter combinations. It also explains the error rate effect: upcoming words are more difficult to predict when the context is degraded, leading to increased surprisal.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[ACL]</abbr>


<div id="belinkov:2019:acl">
  
    <span class="title">Don‚Äôt Take the Premise for Granted: Mitigating Artifacts in Natural Language Inference.</span>
    <span class="author">
      
        
          
          
            <em>Yonatan Belinkov*</em>,
          
        
      
        
          
          
            
              <a href="http://www.cs.jhu.edu/~apoliak1/" target="_blank">Adam Poliak*</a>,
            
          
        
      
        
          
          
            
              <a href="http://www.eecs.harvard.edu/~shieber/index.html" target="_blank">Stuart M. Shieber</a>,
            
          
        
      
        
          
          
            
              <a href="http://www.cs.jhu.edu/~vandurme/index.html" target="_blank">Benjamin Van Durme</a>,
            
          
        
      
        
          
          
          
            
              Alexander M. Rush
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (ACL)</em>
    
    
      2019
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/acl2019.pdf" target="_blank">PDF</a>]
  
  
  
  
    [<a href="https://belinkov.com/assets/pdf/acl2019-slides.pdf" target="_blank">Slides</a>]
  
  
    [<a href="https://github.com/azpoliak/robust-nli" target="_blank">Code</a>]
  
  
    [<a href="https://arxiv.org/abs/1907.04380" target="_blank">Arxiv</a>]
  
  
  
    [<a href="https://vimeo.com/384034160" target="_blank">Talk</a>]
   
  
    [Media: <a href="https://www.seas.harvard.edu/news/2019/07/better-way-to-train-machine-learning-models" target="_blank">Havard News</a>, <a href="https://techxplore.com/news/2019-07-ai-human-bias.html" target="_blank">TechXplore</a>] 
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Natural Language Inference (NLI) datasets often contain hypothesis-only biases‚Äîartifacts that allow models to achieve non-trivial performance without learning whether a premise entails a hypothesis. We propose two probabilistic methods to build models that are more robust to such biases and better transfer across datasets. In contrast to standard approaches to NLI, our methods predict the probability of a premise given a hypothesis and NLI label, discouraging models from ignoring the premise. We evaluate our methods on synthetic and existing NLI datasets by training on datasets containing biases and testing on datasets containing no (or different) hypothesis-only biases. Our results indicate that these methods can make NLI models more robust to dataset-specific artifacts, transferring better than a baseline architecture in 9 out of 12 NLI datasets. Additionally, we provide an extensive analysis of the interplay of our methods with known biases in NLI datasets, as well as the effects of encouraging models to ignore biases and fine-tuning on target datasets.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[NAACL]</abbr>


<div id="durrani:2019:NAACL">
  
    <span class="title">One Size Does Not Fit All: Comparing NMT Representations of Different Granularities.</span>
    <span class="author">
      
        
          
          
            
              <a href="http://alt.qcri.org/~ndurrani/" target="_blank">Nadir Durrani</a>,
            
          
        
      
        
          
          
            
              <a href="https://fdalvi.github.io" target="_blank">Fahim Dalvi</a>,
            
          
        
      
        
          
          
            
              <a href="https://hsajjad.github.io" target="_blank">Hassan Sajjad</a>,
            
          
        
      
        
          
          
            <em>Yonatan Belinkov</em>,
          
        
      
        
          
          
          
            
              Preslav Nakov
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Proceedings of the 17th Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)</em>
    
    
      2019
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/naacl2019-nmt.pdf" target="_blank">PDF</a>]
  
  
  
  
  
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Recent work has shown that contextualized word representations derived from neural machine translation are a viable alternative to such from simple word predictions tasks. This is because the internal understanding that needs to be built in order to be able to translate from one language to another is much more comprehensive. Unfortunately, computational and memory limitations as of present prevent NMT models from using large word vocabularies, and thus alternatives such as subword units (BPE and morphological segmentations) and characters have been used. Here we study the impact of using different kinds of units on the quality of the resulting representations when used to model morphology, syntax, and semantics. We found that while representations derived from subwords are slightly better for modeling syntax, character-based representations are superior for modeling morphology and are also more robust to noisy input.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[NAACL]</abbr>


<div id="liu:2019:NAACL">
  
    <span class="title">Linguistic Knowledge and Transferability of Contextual Representations.</span>
    <span class="author">
      
        
          
          
            
              Nelson F. Liu,
            
          
        
      
        
          
          
            
              Matt Gardner,
            
          
        
      
        
          
          
            <em>Yonatan Belinkov</em>,
          
        
      
        
          
          
            
              Matthew Peters,
            
          
        
      
        
          
          
          
            
              Noah A. Smith
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Proceedings of the 17th Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)</em>
    
    
      2019
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/naacl2019-cwr.pdf" target="_blank">PDF</a>]
  
  
  
  
  
  
    [<a href="https://arxiv.org/abs/1903.08855" target="_blank">Arxiv</a>]
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Contextual word representations derived from large-scale neural language models are successful across a diverse set of NLP tasks, suggesting that they encode useful and transferable features of language. To shed light on the linguistic knowledge they capture, we study the representations produced by several recent pretrained contextualizers (variants of ELMo, the OpenAI transformer LM, and BERT) with a suite of sixteen diverse probing tasks. We find that linear models trained on top of frozen contextual representations are competitive with state-of-the-art task-specific models in many cases, but fail on tasks requiring fine-grained linguistic knowledge (e.g., conjunct identification). To investigate the transferability of contextual word representations, we quantify differences in the transferability of individual layers within contextualizers, especially between RNNs and transformers. For instance, higher layers of RNNs are more task-specific, while transformer layers do not exhibit the same monotonic trend. In addition, to better understand what makes contextual word representations transferable, we compare language model pretraining with eleven supervised pretraining tasks. For any given task, pretraining on a closely related task yields better performance than language model pretraining (which is better on average) when the pretraining dataset is fixed. However, language model pretraining on more data gives the best results.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[*SEM]</abbr>


<div id="belinkov:2019:starsem">
  
    <span class="title">On Adversarial Removal of Hypothesis-only Bias in Natural Language Inference.</span>
    <span class="author">
      
        
          
          
            <em>Yonatan Belinkov*</em>,
          
        
      
        
          
          
            
              <a href="http://www.cs.jhu.edu/~apoliak1/" target="_blank">Adam Poliak*</a>,
            
          
        
      
        
          
          
            
              <a href="http://www.eecs.harvard.edu/~shieber/index.html" target="_blank">Stuart M. Shieber</a>,
            
          
        
      
        
          
          
            
              <a href="http://www.cs.jhu.edu/~vandurme/index.html" target="_blank">Benjamin Van Durme</a>,
            
          
        
      
        
          
          
          
            
              Alexander M. Rush
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Proceedings of the Eighth Joint Conference on Lexical and Computational Semantics (*SEM, Oral presentation)</em>
    
    
      2019
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/starsem2019.pdf" target="_blank">PDF</a>]
  
  
  
  
    [<a href="https://belinkov.com/assets/pdf/starsem2019-slides.pdf" target="_blank">Slides</a>]
  
  
    [<a href="https://github.com/azpoliak/robust-nli" target="_blank">Code</a>]
  
  
    [<a href="https://arxiv.org/abs/1907.04389" target="_blank">Arxiv</a>]
  
  
   
  
    [Media: <a href="https://www.seas.harvard.edu/news/2019/07/better-way-to-train-machine-learning-models" target="_blank">Havard News</a>, <a href="https://techxplore.com/news/2019-07-ai-human-bias.html" target="_blank">TechXplore</a>] 
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Popular Natural Language Inference (NLI) datasets have been shown to be tainted by hypothesis-only biases. Adversarial learning may help models ignore sensitive biases and spurious correlations in data. We evaluate whether adversarial learning can be used in NLI to encourage models to learn representations free of hypothesis-only biases. Our analyses indicate that the representations learned via adversarial learning may be less biased, with only small drops in NLI accuracy.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[ICLR]</abbr>


<div id="bau:2019:ICLR">
  
    <span class="title">Identifying and Controlling Important Neurons in Neural Machine Translation.</span>
    <span class="author">
      
        
          
          
            
              <a href="https://baulab.info" target="_blank">D. Anthony Bau*</a>,
            
          
        
      
        
          
          
            <em>Yonatan Belinkov*</em>,
          
        
      
        
          
          
            
              <a href="https://hsajjad.github.io" target="_blank">Hassan Sajjad</a>,
            
          
        
      
        
          
          
            
              <a href="http://alt.qcri.org/~ndurrani/" target="_blank">Nadir Durrani</a>,
            
          
        
      
        
          
          
            
              <a href="https://fdalvi.github.io" target="_blank">Fahim Dalvi</a>,
            
          
        
      
        
          
          
          
            
              <a href="http://people.csail.mit.edu/jrg/" target="_blank">James Glass</a>
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In International Conference on Learning Representations (ICLR)</em>
    
    
      2019
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/iclr2019.pdf" target="_blank">PDF</a>]
  
  
  
    [<a href="https://belinkov.com/assets/pdf/iclr2019-poster.pdf" target="_blank">Poster</a>]
  
  
  
  
    [<a href="https://arxiv.org/abs/1811.01157" target="_blank">Arxiv</a>]
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Neural machine translation (NMT) models learn representations containing substantial linguistic information. However, it is not clear if such information is fully distributed or if some of it can be attributed to individual neurons. We develop unsupervised methods for discovering important neurons in NMT models. Our methods rely on the intuition that different models learn similar properties, and do not require any costly external supervision. We show experimentally that translation quality depends on the discovered neurons, and find that many of them capture common linguistic phenomena. Finally, we show how to control NMT translations in predictable ways, by modifying activations of individual neurons.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[SCiL]</abbr>


<div id="suzgun:2019:SCiL">
  
    <span class="title">On Evaluating the Generalization of LSTM Models in Formal Languages.</span>
    <span class="author">
      
        
          
          
            
              Mirac Suzgun,
            
          
        
      
        
          
          
            <em>Yonatan Belinkov</em>,
          
        
      
        
          
          
          
            
              <a href="http://www.eecs.harvard.edu/~shieber/index.html" target="_blank">Stuart M. Shieber</a>
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Proceedings of the Society for Computation in Linguistics (SCiL)</em>
    
    
      2019
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/scil2019.pdf" target="_blank">PDF</a>]
  
  
  
  
  
    [<a href="https://github.com/suzgunmirac/lstm-eval" target="_blank">Code</a>]
  
  
    [<a href="https://arxiv.org/abs/1811.01001" target="_blank">Arxiv</a>]
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Recurrent Neural Networks (RNNs) are theoretically Turing-complete and established themselves as a dominant model for language processing. Yet, there still remains an uncertainty regarding their language learning capabilities. In this paper, we empirically evaluate the inductive learning capabilities of Long Short-Term Memory networks, a popular extension of simple RNNs, to learn simple formal languages, in particular a^nb^n, a^nb^nc^n, and a^nb^nc^nd^n. We investigate the influence of various aspects of learning, such as training data regimes and model capacity, on the generalization to unobserved samples. We find striking differences in model performances under different training settings and highlight the need for careful analysis and assessment when making claims about the learning capabilities of neural network models.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[AAAI]</abbr>


<div id="dalvi:2019:AAAI">
  
    <span class="title">What Is One Grain of Sand in the Desert? Analyzing Individual Neurons in Deep NLP Models.</span>
    <span class="author">
      
        
          
          
            
              <a href="https://fdalvi.github.io" target="_blank">Fahim Dalvi</a>,
            
          
        
      
        
          
          
            
              <a href="http://alt.qcri.org/~ndurrani/" target="_blank">Nadir Durrani</a>,
            
          
        
      
        
          
          
            
              <a href="https://hsajjad.github.io" target="_blank">Hassan Sajjad</a>,
            
          
        
      
        
          
          
            <em>Yonatan Belinkov</em>,
          
        
      
        
          
          
            
              <a href="https://baulab.info" target="_blank">D. Anthony Bau</a>,
            
          
        
      
        
          
          
          
            
              <a href="http://people.csail.mit.edu/jrg/" target="_blank">James Glass</a>
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Proceedings of the Thirty-Third AAAI Conference on Artificial Intelligence (AAAI, Oral presentation)</em>
    
    
      2019
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/aaai2019.pdf" target="_blank">PDF</a>]
  
  
  
    [<a href="https://belinkov.com/assets/pdf/aaai2019-poster.pdf" target="_blank">Poster</a>]
  
  
  
  
    [<a href="https://arxiv.org/abs/1812.09355" target="_blank">Arxiv</a>]
  
  
   
  
    [Media: <a href="http://news.mit.edu/2019/neural-networks-nlp-microscope-0201" target="_blank">MIT News</a>, <a href="https://cacm.acm.org/news/234509-putting-neural-networks-under-the-microscope/fulltext" target="_blank">ACM Tech News</a>] 
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Despite the remarkable evolution of deep neural networks in natural language processing (NLP), their interpretability remains a challenge. Previous work largely focused on what these models learn at the representation level. We break this analysis down further and study individual dimensions (neurons) in the vector representation learned by end-to-end neural models in NLP tasks. We propose two methods: Linguistic Correlation Analysis, based on a supervised method to extract the most relevant neurons with respect to an extrinsic task, and Cross-model Correlation Analysis, an unsupervised method to extract salient neurons w.r.t. the model itself. We evaluate the effectiveness of our techniques by ablating the identified neurons and reevaluating the network‚Äôs performance for two tasks: neural machine translation (NMT) and neural language modeling (NLM). We further present a comprehensive analysis of neurons with the aim to address the following questions: i) how localized or distributed are different linguistic properties in the models? ii) are certain neurons exclusive to some properties and not others? iii) is the information more or less distributed in NMT vs. NLM? and iv) how important are the neurons identified through the linguistic correlation method to the overall task? Our code is publicly available1 as part of the NeuroX toolkit (Dalvi et al. 2019).</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[AAAI]</abbr>


<div id="dalvi:2019:AAAI:demo">
  
    <span class="title">NeuroX: A Toolkit for Analyzing Individual Neurons in Neural Networks.</span>
    <span class="author">
      
        
          
          
            
              <a href="https://fdalvi.github.io" target="_blank">Fahim Dalvi</a>,
            
          
        
      
        
          
          
            
              Avery Nortonsmith,
            
          
        
      
        
          
          
            
              <a href="https://baulab.info" target="_blank">D. Anthony Bau</a>,
            
          
        
      
        
          
          
            <em>Yonatan Belinkov</em>,
          
        
      
        
          
          
            
              <a href="https://hsajjad.github.io" target="_blank">Hassan Sajjad</a>,
            
          
        
      
        
          
          
            
              <a href="http://alt.qcri.org/~ndurrani/" target="_blank">Nadir Durrani</a>,
            
          
        
      
        
          
          
          
            
              <a href="http://people.csail.mit.edu/jrg/" target="_blank">James Glass</a>
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Proceedings of the Thirty-Third AAAI Conference on Artificial Intelligence (AAAI): Demonstrations Track</em>
    
    
      2019
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/aaai2019-demo.pdf" target="_blank">PDF</a>]
  
  
  
  
  
    [<a href="https://github.com/fdalvi/NeuroX" target="_blank">Code</a>]
  
  
    [<a href="https://arxiv.org/abs/1812.09359" target="_blank">Arxiv</a>]
  
  
   
  
    [Media: <a href="http://news.mit.edu/2019/neural-networks-nlp-microscope-0201" target="_blank">MIT News</a>, <a href="https://cacm.acm.org/news/234509-putting-neural-networks-under-the-microscope/fulltext" target="_blank">ACM Tech News</a>] 
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>We present a toolkit to facilitate the interpretation and understanding of neural network models. The toolkit provides several methods to identify salient neurons with respect to the model itself or an external task. A user can visualize selected neurons, ablate them to measure their effect on the model accuracy, and manipulate them to control the behavior of the model at the test time. Such an analysis has a potential to serve as a springboard in various research directions, such as understanding the model, better architectural choices, model distillation and controlling data biases. The toolkit is available for download.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[NAACL]</abbr>


<div id="poliak:2018:NAACL">
  
    <span class="title">On the Evaluation of Semantic Phenomena in Neural Machine Translation Using Natural Language Inference.</span>
    <span class="author">
      
        
          
          
            
              <a href="http://www.cs.jhu.edu/~apoliak1/" target="_blank">Adam Poliak</a>,
            
          
        
      
        
          
          
            <em>Yonatan Belinkov</em>,
          
        
      
        
          
          
            
              <a href="http://people.csail.mit.edu/jrg/" target="_blank">James Glass</a>,
            
          
        
      
        
          
          
          
            
              <a href="http://www.cs.jhu.edu/~vandurme/index.html" target="_blank">Benjamin Van Durme</a>
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Proceedings of the 16th Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)</em>
    
    
      2018
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/naacl2018.pdf" target="_blank">PDF</a>]
  
  
  
  
  
    [<a href="https://github.com/boknilev/nmt-repr-analysis" target="_blank">Code</a>]
  
  
    [<a href="https://arxiv.org/abs/1804.09779" target="_blank">Arxiv</a>]
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>We propose a process for investigating the extent to which sentence representations arising from neural machine translation (NMT) systems encode distinct semantic phenomena. We use these representations as features to train a natural language inference (NLI) classifier based on datasets recast from existing semantic annotations. In applying this process to a representative NMT system, we find its encoder appears most suited to supporting inferences at the syntax-semantics interface, as compared to anaphora resolution requiring world-knowledge. We conclude with a discussion on the merits and potential deficiencies of the existing process, and how it may be improved and extended as a broader framework for evaluating semantic coverage.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[ICLR]</abbr>


<div id="belinkov:2018:ICLR">
  
    <span class="title">Synthetic and Natural Noise Both Break Neural Machine Translation.</span>
    <span class="author">
      
        
          
          
            <em>Yonatan Belinkov*</em>,
          
        
      
        
          
          
          
            
              <a href="http://yonatanbisk.com" target="_blank">Yonatan Bisk*</a>
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In International Conference on Learning Representations (ICLR, Oral presentation)</em>
    
    
      2018
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/iclr2018.pdf" target="_blank">PDF</a>]
  
  
  
  
  
    [<a href="https://github.com/ybisk/charNMT-noise" target="_blank">Code</a>]
  
  
    [<a href="https://arxiv.org/abs/1711.02173" target="_blank">Arxiv</a>]
  
  
   
  
    [Media: <a href="https://www.inside.com.tw/2017/11/20/machine-translation" target="_blank">Taiwanese Tech news</a>, <a href="https://thegradient.pub/frontiers-of-generalization-in-natural-language-processing/" target="_blank">The Gradient</a>] 
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Character-based neural machine translation (NMT) models alleviate out-of-vocabulary issues, learn morphology, and move us closer to completely end-to-end translation systems.  Unfortunately, they are also very brittle and easily falter when presented with noisy data. In this paper, we confront NMT models with synthetic and natural sources of noise. We find that state-of-the-art models fail to translate even moderately noisy texts that humans have no trouble comprehending. We explore two approaches to increase model robustness: structure-invariant word representations and robust training on noisy texts. We find that a model based on a character convolutional neural network is able to simultaneously learn representations robust to multiple kinds of noise.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[NeurIPS]</abbr>


<div id="belinkov:2017:nips">
  
    <span class="title">Analyzing Hidden Representations in End-to-End Automatic Speech Recognition Systems.</span>
    <span class="author">
      
        
          
          
            <em>Yonatan Belinkov</em>,
          
        
      
        
          
          
          
            
              <a href="http://people.csail.mit.edu/jrg/" target="_blank">James Glass</a>
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Advances in Neural Information Processing Systems (NeurIPS)</em>
    
    
      2017
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/nips2017.pdf" target="_blank">PDF</a>]
  
  
  
    [<a href="https://belinkov.com/assets/pdf/nips2017-poster.pdf" target="_blank">Poster</a>]
  
  
  
    [<a href="https://github.com/boknilev/asr-repr-analysis" target="_blank">Code</a>]
  
  
    [<a href="https://arxiv.org/abs/1709.04482" target="_blank">Arxiv</a>]
  
  
   
  
    [Media: <a href="http://news.mit.edu/2017/reading-neural-network-mind-1211" target="_blank">MIT News</a>, <a href="https://cacm.acm.org/news/223498-reading-a-neural-networks-mind/fulltext" target="_blank">ACM Tech News</a>] 
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Neural models have become ubiquitous in automatic speech recognition systems. While neural networks are typically used as acoustic models in more complex systems, recent studies have explored end-to-end speech recognition systems based on neural networks, which can be trained to directly predict text from input acoustic features. Although such systems are conceptually elegant and simpler than traditional systems, it is less obvious how to interpret the trained models. 
In this work, we analyze the speech representations learned by a deep end-to-end model that is based on convolutional and recurrent layers, and trained with a connectionist temporal classification (CTC) loss. We use a pre-trained model to generate frame-level features which are given to a classifier that is trained on frame classification into phones. We evaluate representations from different layers of the deep model and compare their quality for predicting phone labels. Our experiments shed light on important aspects of the end-to-end model such as layer depth, model complexity, and other design choices. </p>
  </span>
  
</div>
</li>
<li>
  <abbr>[IJCNLP]</abbr>


<div id="dalvi:2017:ijcnlp">
  
    <span class="title">Understanding and Improving Morphological Learning in the Neural Machine Translation Decoder.</span>
    <span class="author">
      
        
          
          
            
              <a href="https://fdalvi.github.io" target="_blank">Fahim Dalvi</a>,
            
          
        
      
        
          
          
            
              <a href="http://alt.qcri.org/~ndurrani/" target="_blank">Nadir Durrani</a>,
            
          
        
      
        
          
          
            
              <a href="https://hsajjad.github.io" target="_blank">Hassan Sajjad</a>,
            
          
        
      
        
          
          
            <em>Yonatan Belinkov</em>,
          
        
      
        
          
          
          
            
              Stephan Vogel
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Proceedings of the 8th International Joint Conference on Natural Language Processing (IJCNLP)</em>
    
    
      2017
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/ijcnlp2017-decoder.pdf" target="_blank">PDF</a>]
  
  
  
  
  
    [<a href="https://github.com/fdalvi/seq2seq-attn-multitask" target="_blank">Code</a>]
  
  
  
   
  
    [Media: <a href="http://news.mit.edu/2017/reading-neural-network-mind-1211" target="_blank">MIT News</a>] 
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>End-to-end training makes the neural machine translation (NMT) architecture simpler, yet elegant compared to traditional statistical machine translation (SMT). However, little is known about linguistic patterns of morphology, syntax and semantics learned during the training of NMT systems, and more importantly, which parts of the architecture are responsible for learning each of these phenomena. In this paper we i) analyze how much morphology an NMT decoder learns, and ii) investigate whether injecting target morphology into the decoder helps it produce better translations. To this end we present three methods: i) joint generation, ii) joint-data learning, and iii) multi-task learning. Our results show that explicit morphological information helps the decoder learn target language morphology and improves the translation quality by 0.2‚Äì0.6 BLEU points.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[IJCNLP]</abbr>


<div id="belinkov:2017:ijcnlp">
  
    <span class="title">Evaluating Layers of Representation in Neural Machine Translation on Part-of-Speech and Semantic Tagging Tasks.</span>
    <span class="author">
      
        
          
          
            <em>Yonatan Belinkov</em>,
          
        
      
        
          
          
            
              Llu√≠s M√†rquez,
            
          
        
      
        
          
          
            
              <a href="https://hsajjad.github.io" target="_blank">Hassan Sajjad</a>,
            
          
        
      
        
          
          
            
              <a href="http://alt.qcri.org/~ndurrani/" target="_blank">Nadir Durrani</a>,
            
          
        
      
        
          
          
            
              <a href="https://fdalvi.github.io" target="_blank">Fahim Dalvi</a>,
            
          
        
      
        
          
          
          
            
              <a href="http://people.csail.mit.edu/jrg/" target="_blank">James Glass</a>
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Proceedings of the 8th International Joint Conference on Natural Language Processing (IJCNLP)</em>
    
    
      2017
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/ijcnlp2017-semantics.pdf" target="_blank">PDF</a>]
  
  
  
  
  
    [<a href="https://github.com/boknilev/nmt-repr-analysis" target="_blank">Code</a>]
  
  
    [<a href="https://arxiv.org/abs/1801.07772" target="_blank">Arxiv</a>]
  
  
   
  
    [Media: <a href="http://news.mit.edu/2017/reading-neural-network-mind-1211" target="_blank">MIT News</a>, <a href="https://cacm.acm.org/news/223498-reading-a-neural-networks-mind/fulltext" target="_blank">ACM Tech News</a>] 
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>While neural machine translation (NMT) models provide improved translation quality in an elegant framework, it is less clear what they learn about language. Recent work has started evaluating the quality of vector representations learned by NMT models on morphological and syntactic tasks. In this paper, we investigate the representations learned at different layers of NMT encoders. We train NMT systems on parallel data and use the models to extract features for training a classifier on two tasks: part-of-speech and semantic tagging. We then measure the performance of the classifier as a proxy to the quality of the original NMT model for the given task. Our quantitative analysis yields interesting insights regarding representation learning in NMT models. For instance, we find that higher layers are better at learning semantics while lower layers tend to be better for part-of-speech tagging. We also observe little effect of the target language on source-side representations, especially in higher quality models.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[Interspeech]</abbr>


<div id="khurana:2017:interpseech">
  
    <span class="title">QMDIS: QCRI-MIT Advanced Dialect Identification System.</span>
    <span class="author">
      
        
          
          
            
              Sameer Khurana,
            
          
        
      
        
          
          
            
              Maryam Najafian,
            
          
        
      
        
          
          
            
              Ahmed Ali,
            
          
        
      
        
          
          
            
              Tuka Al Hanai,
            
          
        
      
        
          
          
            <em>Yonatan Belinkov</em>,
          
        
      
        
          
          
          
            
              <a href="http://people.csail.mit.edu/jrg/" target="_blank">James Glass</a>
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Proceedings of Interspeech</em>
    
    
      2017
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/interspeech2017.pdf" target="_blank">PDF</a>]
  
  
  
  
  
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>As a continuation of our efforts towards tackling the problem of spoken Dialect Identification (DID) for Arabic languages, we present the QCRI-MIT Advanced Dialect Identification System (QMDIS). QMDIS is an automatic spoken DID system for Dialectal Arabic (DA). In this paper, we report a comprehensive study of the three main components used in the spoken DID task: phonotactic, lexical and acoustic. We use Support Vector Machines (SVMs), Logistic Regression (LR) and Convolutional Neural Networks (CNNs) as backend classifiers throughout the study. We perform all our experiments on a publicly available dataset and present new state-of-the-art results. QMDIS discriminates between the five most widely used dialects of Arabic: namely Egyptian, Gulf, Levantine, North African, and Modern Standard Arabic (MSA). We report around 73% accuracy for system combination. All the data and the code used in our experiments are publicly available for research.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[ACL]</abbr>


<div id="belinkov:2017:acl">
  
    <span class="title">What do Neural Machine Translation Models Learn about Morphology?.</span>
    <span class="author">
      
        
          
          
            <em>Yonatan Belinkov</em>,
          
        
      
        
          
          
            
              <a href="http://alt.qcri.org/~ndurrani/" target="_blank">Nadir Durrani</a>,
            
          
        
      
        
          
          
            
              <a href="https://fdalvi.github.io" target="_blank">Fahim Dalvi</a>,
            
          
        
      
        
          
          
            
              <a href="https://hsajjad.github.io" target="_blank">Hassan Sajjad</a>,
            
          
        
      
        
          
          
          
            
              <a href="http://people.csail.mit.edu/jrg/" target="_blank">James Glass</a>
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL)</em>
    
    
      2017
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/acl2017.pdf" target="_blank">PDF</a>]
  
  
  
    [<a href="https://belinkov.com/assets/pdf/acl2017-poster.pdf" target="_blank">Poster</a>]
  
  
  
    [<a href="https://github.com/boknilev/nmt-repr-analysis" target="_blank">Code</a>]
  
  
    [<a href="https://arxiv.org/abs/1704.03471" target="_blank">Arxiv</a>]
  
  
  
    [<a href="https://vimeo.com/234955191" target="_blank">Talk</a>]
   
  
    [Media: <a href="https://soundcloud.com/nlp-highlights/27-what-do-neural-machine-translation-models-learn-about-morphology-with-yonatan-belinkov" target="_blank">NLP Highlights</a>] 
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Neural machine translation (MT) models obtain state-of-the-art performance while maintaining a simple, end-to-end architecture. However, little is known about what these models learn about source and target languages during the training process. In this work, we analyze the representations learned by neural MT models at various levels of granularity and empirically evaluate the quality of the representations for learning morphology through extrinsic part-of-speech and morphological tagging tasks. We conduct a thorough investigation along several parameters: word-based vs. character-based representations, depth of the encoding layer, the identity of the target language, and encoder vs. decoder representations. Our data-driven, quantitative evaluation sheds light on important aspects in the neural MT system and its ability to capture word structure.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[ACL]</abbr>


<div id="sajjad:2017:acl">
  
    <span class="title">Challenging Language-Dependent Segmentation for Arabic: An Application to Machine Translation and Part-of-Speech Tagging.</span>
    <span class="author">
      
        
          
          
            
              <a href="https://hsajjad.github.io" target="_blank">Hassan Sajjad</a>,
            
          
        
      
        
          
          
            
              <a href="https://fdalvi.github.io" target="_blank">Fahim Dalvi</a>,
            
          
        
      
        
          
          
            
              <a href="http://alt.qcri.org/~ndurrani/" target="_blank">Nadir Durrani</a>,
            
          
        
      
        
          
          
            
              Ahmed Abdelali,
            
          
        
      
        
          
          
            <em>Yonatan Belinkov</em>,
          
        
      
        
          
          
          
            
              Stephan Vogel
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL)</em>
    
    
      2017
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/acl2017-short.pdf" target="_blank">PDF</a>]
  
  
  
  
  
  
    [<a href="https://arxiv.org/abs/1709.00616" target="_blank">Arxiv</a>]
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Word segmentation plays a pivotal role in improving any Arabic NLP application. Therefore, a lot of research has been spent in improving its accuracy. Off-the-shelf tools, however, are: i) complicated to use and ii) domain/dialect dependent. We explore three language-independent alternatives to morphological segmentation using: i) data-driven sub-word units, ii) characters as a unit of learning, and iii) word embeddings learned using a character CNN (Convolution Neural Network). On the tasks of Machine Translation and POS tagging, we found these methods to achieve close to, and occasionally surpass state-of-the-art performance. In our analysis, we show that a neural machine translation system is sensitive to the ratio of source and target tokens, and a ratio close to 1 or greater, gives optimal performance.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[ICLR]</abbr>


<div id="adi:2017:ICLR">
  
    <span class="title">Fine-grained Analysis of Sentence Embeddings Using Auxiliary Prediction Tasks.</span>
    <span class="author">
      
        
          
          
            
              <a href="http://adiyoss.github.io" target="_blank">Yossi Adi</a>,
            
          
        
      
        
          
          
            
              Einat Kermany,
            
          
        
      
        
          
          
            <em>Yonatan Belinkov</em>,
          
        
      
        
          
          
            
              <a href="http://researcher.watson.ibm.com/researcher/view.php?person=il-OFERL" target="_blank">Ofer Lavi</a>,
            
          
        
      
        
          
          
          
            
              <a href="https://www.cs.bgu.ac.il/~yoavg/uni/" target="_blank">Yoav Goldberg</a>
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In International Conference on Learning Representations (ICLR)</em>
    
    
      2017
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/iclr2017.pdf" target="_blank">PDF</a>]
  
  
  
  
  
  
    [<a href="https://arxiv.org/abs/1608.04207" target="_blank">Arxiv</a>]
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>There is a lot of research interest in encoding variable length sentences into fixed length vectors, in a way that preserves the sentence meanings. Two common methods include representations based on averaging word vectors, and representations based on the hidden states of recurrent neural networks such as LSTMs. The sentence vectors are used as features for subsequent machine learning tasks or for pre-training in the context of deep learning. However, not much is known about the properties that are encoded in these sentence representations and about the language information they capture.
We propose a framework that facilitates better understanding of the encoded representations. We define prediction tasks around isolated aspects of sentence structure (namely sentence length, word content, and word order), and score representations by the ability to train a classifier to solve each prediction task when using the representation as input. We demonstrate the potential contribution of the approach by analyzing different sentence representation mechanisms. The analysis sheds light on the relative strengths of different sentence embedding methods with respect to these low level prediction tasks, and on the effect of the encoded vector‚Äôs dimensionality on the resulting representations.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[Coling]</abbr>


<div id="romeo-EtAl:2016:COLING">
  
    <span class="title">Neural Attention for Learning to Rank Questions in Community Question Answering.</span>
    <span class="author">
      
        
          
          
            
              Salvatore Romeo,
            
          
        
      
        
          
          
            
              Giovanni Da San Martino,
            
          
        
      
        
          
          
            
              <a href="http://www.lsi.upc.edu/~albarron/" target="_blank">Alberto Barr√≥n-Cede√±o</a>,
            
          
        
      
        
          
          
            
              <a href="http://disi.unitn.it/moschitti/" target="_blank">Alessandro Moschitti</a>,
            
          
        
      
        
          
          
            <em>Yonatan Belinkov</em>,
          
        
      
        
          
          
            
              <a href="http://people.csail.mit.edu/wnhsu/" target="_blank">Wei-Ning Hsu</a>,
            
          
        
      
        
          
          
            
              <a href="http://people.csail.mit.edu/yzhang87/" target="_blank">Yu Zhang</a>,
            
          
        
      
        
          
          
            
              Mitra Mohtarami,
            
          
        
      
        
          
          
          
            
              <a href="http://people.csail.mit.edu/jrg/" target="_blank">James Glass</a>
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Proceedings of the 26th International Conference on Computational Linguistics: Technical Papers (Coling)</em>
    
    
      2016
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/coling2016.pdf" target="_blank">PDF</a>]
  
  
  
  
  
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>In real-world data, e.g., from Web forums,  text is often contaminated with
	redundant or irrelevant content, which leads to introducing noise in machine
	learning algorithms.
	In this paper, we apply Long Short-Term Memory networks with an attention
	mechanism, which can select important parts of text for the task of similar
	question retrieval from community Question Answering (cQA) forums. 
	In particular, we use the attention weights for both selecting entire sentences
	and their subparts, i.e., word/chunk, from shallow syntactic trees. More
	interestingly, we apply tree kernels to the filtered text representations, thus
	exploiting the implicit features of the subtree space for learning question
	reranking. Our results show that the attention-based pruning allows for
	achieving the top position in the cQA challenge of SemEval 2016, with a
	relatively large gap from the other participants while greatly decreasing
	running time.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[EMNLP]</abbr>


<div id="belinkov-glass:2015:EMNLP">
  
    <span class="title">Arabic Diacritization with Recurrent Neural Networks.</span>
    <span class="author">
      
        
          
          
            <em>Yonatan Belinkov</em>,
          
        
      
        
          
          
          
            
              <a href="http://people.csail.mit.edu/jrg/" target="_blank">James Glass</a>
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>
    
    
      2015
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/emnlp2015.pdf" target="_blank">PDF</a>]
  
  
  
    [<a href="https://belinkov.com/assets/pdf/emnlp2015-poster.pdf" target="_blank">Poster</a>]
  
  
  
    [<a href="https://github.com/boknilev/diacritization" target="_blank">Code</a>]
  
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Arabic, Hebrew, and similar languages are typically written without diacritics, leading to ambiguity and posing a major challenge for core language processing tasks like speech recognition. Previous approaches to automatic diacritization employed a variety of machine learning techniques. However, they typically rely on existing tools like morphological analyzers and therefore cannot be easily extended to new genres and languages. We develop a recurrent neural network with long short-term memory layers for predicting diacritics in Arabic text. Our language-independent approach is trained solely from diacritized text without relying on external tools. We show experimentally that our model can rival state-of-the-art methods that have access to additional resources.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[ACL]</abbr>


<div id="sajjad-darwish-belinkov:2013:Short">
  
    <span class="title">Translating Dialectal Arabic to English.</span>
    <span class="author">
      
        
          
          
            
              <a href="https://hsajjad.github.io" target="_blank">Hassan Sajjad</a>,
            
          
        
      
        
          
          
            
              Kareem Darwish,
            
          
        
      
        
          
          
          
            <em>Yonatan Belinkov</em>
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL)</em>
    
    
      2013
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/acl2013.pdf" target="_blank">PDF</a>]
  
  
  
  
  
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>We present a dialectal Egyptian Arabic to English statistical machine translation system that leverages dialectal to Modern Standard Arabic (MSA) adaptation. In contrast to previous work, we first narrow down the gap between Egyptian and MSA by applying an automatic character-level transformational model that changes Egyptian to EG‚Äô , which looks similar to MSA. The transformations include morphological, phonological and spelling changes. The transformation reduces the out-of-vocabulary (OOV) words from 5.2% to 2.6% and gives a gain of 1.87 BLEU points. Further, adapting large MSA/English parallel data increases the lexical coverage, reduces OOVs to 0.7% and leads to an absolute BLEU improvement of 2.73 points.</p>
  </span>
  
</div>
</li></ol>
<h2 class="bibliography">Workshop Papers</h2>
<ol class="bibliography"><li>

<div id="saleh:2020">
  
    <span class="title">Probing Neural Dialog Models for Conversational Understanding.</span>
    <span class="author">
      
        
          
          
            
              Abdelrhman Saleh,
            
          
        
      
        
          
          
            
              Tovly Deutsch,
            
          
        
      
        
          
          
            
              Stephen Casper,
            
          
        
      
        
          
          
            <em>Yonatan Belinkov</em>,
          
        
      
        
          
          
          
            
              <a href="http://www.eecs.harvard.edu/~shieber/index.html" target="_blank">Stuart Shieber</a>
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Proceedings of the Second Workshop on NLP for Conversational AI (NLP4ConvAI)</em>
    
    
      2020
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/nlp4convai2020.pdf" target="_blank">PDF</a>]
  
  
  
  
  
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>The predominant approach to open-domain dialog generation relies on end-to-end training of neural models on chat datasets. However, this approach provides little insight as to what these models learn (or do not learn) about engaging in dialog. In this study, we analyze the internal representations learned by neural open-domain dialog systems and evaluate the quality of these representations for learning basic conversational skills. Our results suggest that standard open-domain dialog systems struggle with answering questions, inferring contradiction, and determining the topic of conversation, among other tasks. We also find that the dyadic, turn-taking nature of dialog is not fully leveraged by these models. By exploring these limitations, we highlight the need for additional research into architectures and training methods that can better capture high-level information about dialog.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[Blackbox]</abbr>


<div id="vig:2019:blackbox">
  
    <span class="title">Analyzing the Structure of Attention in a Transformer Language Model.</span>
    <span class="author">
      
        
          
          
            
              Jesse Vig,
            
          
        
      
        
          
          
          
            <em>Yonatan Belinkov</em>
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Proceedings of the Second BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP at ACL</em>
    
    
      2019
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/blackboxnlp2019.pdf" target="_blank">PDF</a>]
  
  
  
  
  
  
    [<a href="https://arxiv.org/abs/1906.04284" target="_blank">Arxiv</a>]
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>The Transformer is a fully attention-based alternative to recurrent networks that has achieved state-of-the-art results across a range of NLP tasks. In this paper, we analyze the structure of attention in a Transformer language model, the GPT-2 small pretrained model. We visualize attention for individual instances and analyze the interaction between attention and syntax over a large corpus. We find that attention targets different parts of speech at different layer depths within the model, and that attention aligns with dependency relations most strongly in the middle layers. We also find that the deepest layers of the model capture the most distant relationships. Finally, we extract exemplar sentences that reveal highly specific patterns targeted by particular attention heads.</p>
  </span>
  
</div>
</li>
<li>

<div id="suzgun:2019:delfol">
  
    <span class="title">LSTM Networks Can Perform Dynamic Counting.</span>
    <span class="author">
      
        
          
          
            
              Mirac Suzgun,
            
          
        
      
        
          
          
            
              <a href="https://www.sebastiangehrmann.com" target="_blank">Sebastian Gehrmann</a>,
            
          
        
      
        
          
          
            <em>Yonatan Belinkov</em>,
          
        
      
        
          
          
          
            
              <a href="http://www.eecs.harvard.edu/~shieber/index.html" target="_blank">Stuart Shieber</a>
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Proceedings of the First Workshop on Deep Learning and Formal Languages: Building Bridges</em>
    
    
      2019
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/delfol2019.pdf" target="_blank">PDF</a>]
  
  
  
  
  
  
    [<a href="https://arxiv.org/abs/1906.03648" target="_blank">Arxiv</a>]
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>In this paper, we systematically assess the ability of standard recurrent networks to perform dynamic counting and to encode hierarchical representations. All the neural models in our experiments are designed to be small-sized networks both to prevent them from memorizing the training sets and to visualize and interpret their behaviour at test time. Our results demonstrate that the Long Short-Term Memory (LSTM) networks can learn to recognize the well-balanced parenthesis language (Dyck-1) and the shuffles of multiple Dyck-1 languages, each defined over different parenthesis-pairs, by emulating simple real-time k-counter machines. To the best of our knowledge, this work is the first study to introduce the shuffle languages to analyze the computational power of neural networks. We also show that a single-layer LSTM with only one hidden unit is practically sufficient for recognizing the Dyck-1 language. However, none of our recurrent networks was able to yield a good performance on the Dyck-2 language learning task, which requires a model to have a stack-like mechanism for</p>
  </span>
  
</div>
</li>
<li>

<div id="grand:2019:SIVL">
  
    <span class="title">Adversarial Regularization for Visual Question Answering: Strengths, Shortcomings, and Side Effects.</span>
    <span class="author">
      
        
          
          
            
              Gabriel Grand,
            
          
        
      
        
          
          
          
            <em>Yonatan Belinkov</em>
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Proceedings of the 2nd Workshop on Shortcomings in Vision and Language (SiVL) at NAACL-HLT</em>
    
    
      2019
    
    </span>
    

    
  

  
    <span style="color:Tomato;">Best paper award</span>
  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/sivl2019.pdf" target="_blank">PDF</a>]
  
  
  
  
  
  
    [<a href="http://arxiv.org/abs/1906.08430" target="_blank">Arxiv</a>]
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Visual question answering (VQA) models have been shown to over-rely on linguistic biases in VQA datasets, answering questions ‚Äúblindly‚Äù without considering visual context. Adversarial regularization (AdvReg) aims to address this issue via an adversary subnetwork that encourages the main model to learn a bias-free representation of the question. In this work, we investigate the strengths and shortcomings of AdvReg with the goal of better understanding how it affects inference in VQA models. Despite achieving a new stateof-the-art on VQA-CP, we find that AdvReg yields several undesirable side-effects, including unstable gradients and sharply reduced performance on in-domain examples. We demonstrate that gradual introduction of regularization during training helps to alleviate, but not completely solve, these issues. Through error analyses, we observe that AdvReg improves generalization to binary questions, but impairs performance on questions with heterogeneous answer distributions. Qualitatively, we also find that regularized models tend to over-rely on visual features, while ignoring important linguistic cues in the question. Our results suggest that AdvReg requires further refinement before it can be considered a viable bias mitigation technique for VQA.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[IWSLT]</abbr>


<div id="sajjad:2017:iwslt">
  
    <span class="title">Neural Machine Translation Training in a Multi-Domain Scenario.</span>
    <span class="author">
      
        
          
          
            
              <a href="https://hsajjad.github.io" target="_blank">Hassan Sajjad</a>,
            
          
        
      
        
          
          
            
              <a href="http://alt.qcri.org/~ndurrani/" target="_blank">Nadir Durrani</a>,
            
          
        
      
        
          
          
            
              <a href="https://fdalvi.github.io" target="_blank">Fahim Dalvi</a>,
            
          
        
      
        
          
          
            <em>Yonatan Belinkov</em>,
          
        
      
        
          
          
          
            
              Stephan Vogel
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Proceedings of the International Workshop on Spoken Language Translation (IWSLT)</em>
    
    
      2017
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/iwslt2017.pdf" target="_blank">PDF</a>]
  
  
  
  
  
  
    [<a href="https://arxiv.org/abs/1708.08712" target="_blank">Arxiv</a>]
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>In this paper, we explore alternative ways to train a neural machine translation system in a multi-domain scenario. We investigate data concatenation (with fine tuning), model stacking (multi-level fine tuning), data selection and weighted ensemble. We evaluate these methods based on three criteria: i) translation quality, ii) training time, and iii) robustness towards out-of-domain tests. Our findings on Arabic-English and German-English language pairs show that the best translation quality can be achieved by building an initial system on a concatenation of available out-of-domain data and then fine-tuning it on in-domain data. Model stacking works best when training begins with the furthest out-of-domain data and the model is incrementally fine-tuned with the next furthest domain and so on. Data selection did not give the best results, but can be considered as a decent compromise between training time and translation quality. A weighted ensemble of different individual models performed better than data selection. It is beneficial in a scenario when there is no time for fine-tuning.</p>
  </span>
  
</div>
</li>
<li>

<div id="belinkov-glass:2016:VarDial">
  
    <span class="title">A Character-level Convolutional Neural Network for Distinguishing Similar Languages and Dialects.</span>
    <span class="author">
      
        
          
          
            <em>Yonatan Belinkov</em>,
          
        
      
        
          
          
          
            
              <a href="http://people.csail.mit.edu/jrg/" target="_blank">James Glass</a>
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Proceedings of the Third Workshop on NLP for Similar Languages, Varieties and Dialects (VarDial at Coling)</em>
    
    
      2016
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/vardial2016.pdf" target="_blank">PDF</a>]
  
  
  
    [<a href="https://belinkov.com/assets/pdf/vardial2016-poster.pdf" target="_blank">Poster</a>]
  
  
  
    [<a href="https://github.com/boknilev/dsl-char-cnn" target="_blank">Code</a>]
  
  
    [<a href="https://arxiv.org/abs/1609.07568" target="_blank">Arxiv</a>]
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Discriminating between closely-related language varieties is considered a challenging and important task. This paper describes our submission to the DSL 2016 shared-task, which included two sub-tasks: one on discriminating similar languages and one on identifying Arabic dialects. We developed a character-level neural network for this task. Given a sequence of characters, our model embeds each character in vector space, runs the sequence through multiple convolutions with different filter widths, and pools the convolutional representations to obtain a hidden vector representation of the text that is used for predicting the language or dialect. We primarily focused on the Arabic dialect identification task and obtained an F1 score of 0.4834, ranking 6th out of 18 participants. We also analyze errors made by our system on the Arabic data in some detail, and point to challenges such an approach is faced with.</p>
  </span>
  
</div>
</li>
<li>

<div id="belinkov-EtAl:2016:LT4DH">
  
    <span class="title">Shamela: A Large-Scale Historical Arabic Corpus.</span>
    <span class="author">
      
        
          
          
            <em>Yonatan Belinkov</em>,
          
        
      
        
          
          
            
              <a href="https://uri.academia.edu/AlexanderMagidow" target="_blank">Alexander Magidow</a>,
            
          
        
      
        
          
          
            
              <a href="https://alraqmiyyat.github.io" target="_blank">Maxim Romanov</a>,
            
          
        
      
        
          
          
            
              <a href="https://biu.academia.edu/AviShmidman/" target="_blank">Avi Shmidman</a>,
            
          
        
      
        
          
          
          
            
              <a href="http://u.cs.biu.ac.il/~koppel/" target="_blank">Moshe Koppel</a>
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Proceedings of the Workshop on Language Technology Resources and Tools for Digital Humanities (LT4DH at Coling)</em>
    
    
      2016
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/lt4dh2016.pdf" target="_blank">PDF</a>]
  
  
  
  
    [<a href="https://belinkov.com/assets/pdf/lt4dh2016-slides.pdf" target="_blank">Slides</a>]
  
  
  
    [<a href="https://arxiv.org/abs/1612.08989" target="_blank">Arxiv</a>]
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Arabic is a widely-spoken language with a rich and long history spanning more
	than fourteen centuries. Yet existing Arabic corpora largely focus on the
	modern period or lack sufficient diachronic information. We develop a
	large-scale, historical corpus of Arabic of about 1 billion words from diverse
	periods of time. We clean this corpus, process it with a morphological
	analyzer, and enhance it by detecting parallel passages and automatically
	dating undated texts. We demonstrate its utility with selected case-studies in
	which we show its application to the digital humanities.</p>
  </span>
  
</div>
</li>
<li>

<div id="belinkov-glass:2016:SeMaT">
  
    <span class="title">Large-Scale Machine Translation between Arabic and Hebrew: Available Corpora and Initial Results.</span>
    <span class="author">
      
        
          
          
            <em>Yonatan Belinkov</em>,
          
        
      
        
          
          
          
            
              <a href="http://people.csail.mit.edu/jrg/" target="_blank">James Glass</a>
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Proceedings of the Workshop on Semitic Machine Translation (SeMaT at AMTA)</em>
    
    
      2016
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/semat2016.pdf" target="_blank">PDF</a>]
  
  
  
  
    [<a href="https://belinkov.com/assets/pdf/semat2016-slides.pdf" target="_blank">Slides</a>]
  
  
  
    [<a href="https://arxiv.org/abs/1609.07701" target="_blank">Arxiv</a>]
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Machine translation between Arabic and Hebrew has so far been limited by a lack of parallel corpora, despite the political and cultural importance of this language pair. Previous work relied on manually-crafted grammars or pivoting via English, both of which are unsatisfactory for building a scalable and accurate MT system. In this work, we compare standard phrase-based and neural systems on Arabic-Hebrew translation. We experiment with tokenization by external tools and sub-word modeling by character-level neural models, and show that both methods lead to improved translation performance, with a small advantage to the neural models.</p>
  </span>
  
</div>
</li>
<li>

<div id="aharoni-goldberg-belinkov:2016:SIGMORPHON">
  
    <span class="title">Improving Sequence to Sequence Learning for Morphological Inflection Generation: The BIU-MIT Systems for the SIGMORPHON 2016 Shared Task for Morphological Reinflection.</span>
    <span class="author">
      
        
          
          
            
              <a href="http://roeeaharoni.com" target="_blank">Roee Aharoni</a>,
            
          
        
      
        
          
          
            
              <a href="https://www.cs.bgu.ac.il/~yoavg/uni/" target="_blank">Yoav Goldberg</a>,
            
          
        
      
        
          
          
          
            <em>Yonatan Belinkov</em>
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Proceedings of the 14th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology (SIGMORPHON at ACL)</em>
    
    
      2016
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/sigmorphon2016.pdf" target="_blank">PDF</a>]
  
  
  
  
  
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Morphological reinflection is the task of generating a target form given a source form and the morpho-syntactic attributes of the target (and, optionally, of the source). This work presents the submission of Bar Ilan University and the Massachusetts Institute of Technology for the morphological reinflection shared task held at SIGMORPHON 2016. The submission includes two recurrent neural network architectures for learning morphological reinflection from incomplete inflection tables while using several novel ideas for this task: morpho-syntactic attribute embeddings, modeling the concept of templatic morphology, bidirectional input character representations and neural discriminative string transduction. The reported results for the proposed models over the ten languages in the shared task bring this submission to the second/third place (depending on the language) on all three sub-tasks out of eight participating teams, while training only on the Restricted category data.</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[SemEval]</abbr>


<div id="mohtarami-EtAl:2016:SemEval">
  
    <span class="title">SLS at SemEval-2016 Task 3: Neural-based Approaches for Ranking in Community Question Answering.</span>
    <span class="author">
      
        
          
          
            
              Mitra Mohtarami,
            
          
        
      
        
          
          
            <em>Yonatan Belinkov</em>,
          
        
      
        
          
          
            
              <a href="http://people.csail.mit.edu/wnhsu/" target="_blank">Wei-Ning Hsu</a>,
            
          
        
      
        
          
          
            
              <a href="http://people.csail.mit.edu/yzhang87/" target="_blank">Yu Zhang</a>,
            
          
        
      
        
          
          
            
              Tao Lei,
            
          
        
      
        
          
          
            
              Kfir Bar,
            
          
        
      
        
          
          
            
              Scott Cyphers,
            
          
        
      
        
          
          
          
            
              <a href="http://people.csail.mit.edu/jrg/" target="_blank">Jim Glass</a>
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval)</em>
    
    
      2016
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/semeval2016.pdf" target="_blank">PDF</a>]
  
  
  
  
  
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Community question answering platforms need to automatically rank answers and questions with respect to a given question. In this paper, we present the approaches for the Answer Selection and Question Retrieval tasks of SemEval-2016 (task 3). We develop a bag-of-vectors approach with various vector- and text-based features, and different neural network approaches including CNNs and LSTMs to capture the semantic similarity between questions and answers for ranking purpose. Our evaluation demonstrates that our approaches significantly outperform the baselines.</p>
  </span>
  
</div>
</li>
<li>

<div id="belinkov-barroncedeno-mubarak:2015:WANLP">
  
    <span class="title">Answer Selection in Arabic Community Question Answering: A Feature-Rich Approach.</span>
    <span class="author">
      
        
          
          
            <em>Yonatan Belinkov</em>,
          
        
      
        
          
          
            
              <a href="http://www.lsi.upc.edu/~albarron/" target="_blank">Alberto Barr√≥n-Cede√±o</a>,
            
          
        
      
        
          
          
          
            
              Hamdy Mubarak
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Proceedings of the Second Workshop on Arabic Natural Language Processing (ANLP)</em>
    
    
      2015
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/anlp2015.pdf" target="_blank">PDF</a>]
  
  
  
  
  
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>The task of answer selection in community question answering consists of identifying pertinent answers from a pool of user-generated comments related to a question. The recent SemEval-2015 introduced a shared task on community question answering, providing a corpus and evaluation scheme. In this paper we address the problem of answer selection in Arabic. Our proposed model includes a manifold of features including lexical and semantic similarities, vector representations, and rankings. We investigate the contribution of each set of features in a supervised setting. We show that employing a feature combination by means of a linear support vector machine achieves a better performance than that of the competition winner (F1 of 79.25 compared to 78.55).</p>
  </span>
  
</div>
</li>
<li>
  <abbr>[SemEval]</abbr>


<div id="belinkov-EtAl:2015:SemEval">
  
    <span class="title">VectorSLU: A Continuous Word Vector Approach to Answer Selection in Community Question Answering Systems.</span>
    <span class="author">
      
        
          
          
            <em>Yonatan Belinkov</em>,
          
        
      
        
          
          
            
              Mitra Mohtarami,
            
          
        
      
        
          
          
            
              Scott Cyphers,
            
          
        
      
        
          
          
          
            
              <a href="http://people.csail.mit.edu/jrg/" target="_blank">James Glass</a>
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval)</em>
    
    
      2015
    
    </span>
    

    
  

  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [<a href="https://belinkov.com/assets/pdf/semeval2015.pdf" target="_blank">PDF</a>]
  
  
  
  
  
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Continuous word and phrase vectors have proven useful in a number of NLP tasks. Here we describe our experience using them as a source of features for the SemEval-2015 task 3, consisting of two community question answering subtasks: Answer Selection for categorizing answers as potential, good, and bad with regards to their corresponding questions; and YES/NO inference for predicting a yes, no, or unsure response to a YES/NO question using all of its good answers. Our system ranked 6th and 1st in the English answer selection and YES/NO inference subtasks respectively, and 2nd in the Arabic answer selection subtask.</p>
  </span>
  
</div>
</li>
<li>

<div id="arTenTen">
  
    <span class="title">arTenTen: a new, vast corpus for Arabic.</span>
    <span class="author">
      
        
          
          
            <em>Yonatan Belinkov</em>,
          
        
      
        
          
          
            
              <a href="http://www.nizarhabash.com" target="_blank">Nizar Habash</a>,
            
          
        
      
        
          
          
            
              <a href="https://www.kilgarriff.co.uk" target="_blank">Adam Kilgarriff</a>,
            
          
        
      
        
          
          
            
              Noam Ordan,
            
          
        
      
        
          
          
            
              Ryan Roth,
            
          
        
      
        
          
          
          
            
              V√≠t Suchomel
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Proceedings of the Second Workshop on Arabic Corpus Linguistics (WACL)</em>
    
    
      2013
    
    </span>
    

    
  

  

  <span class="links">
  
  
    [<a href="https://belinkov.com/assets/pdf/wacl2013.pdf" target="_blank">PDF</a>]
  
  
  
  
  
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
</div>
</li></ol>
<h2 class="bibliography">Theses</h2>
<ol class="bibliography"><li>

<div id="belinkov:2018:phdthesis">
  
    <span class="title">On Internal Language Representations in Deep Learning: An Analysis of Machine Translation and Speech Recognition.</span>
    <span class="author">
      
        
          
          
          
            <em>Yonatan Belinkov</em>
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>PhD Thesis, Massachusetts Institute of Technology</em>
    
    
      2018
    
    </span>
    

    
  

  

  <span class="links">
  
  
    [<a href="https://belinkov.com/assets/pdf/thesis2018.pdf" target="_blank">PDF</a>]
  
  
  
  
  
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
</div>
</li>
<li>

<div id="belinkov:2014:mastersthesit">
  
    <span class="title">The Arabic Dialect of «¶isir izZarga: Linguistic description and a preliminary classification, with sample texts.</span>
    <span class="author">
      
        
          
          
          
            <em>Yonatan Belinkov</em>
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>Master's Thesis, Tel Aviv University</em>
    
    
      2014
    
    </span>
    

    
  

  

  <span class="links">
  
  
    [<a href="https://belinkov.com/assets/pdf/tau-ma-thesis.pdf" target="_blank">PDF</a>]
  
  
  
  
  
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
</div>
</li>
<li>

<div id="belinkov:2014:mastersthesis">
  
    <span class="title">Neural Network Architectures for Prepositional Phrase Attachment Disambiguation.</span>
    <span class="author">
      
        
          
          
          
            <em>Yonatan Belinkov</em>
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>Master's Thesis, Massachusetts Institute of Technology</em>
    
    
      2014
    
    </span>
    

    
  

  

  <span class="links">
  
  
    [<a href="https://belinkov.com/assets/pdf/mit-sm-thesis-2014.pdf" target="_blank">PDF</a>]
  
  
  
  
  
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
</div>
</li></ol>

<h2 class="bibliography">Preprints</h2>
<ol class="bibliography"><li>

<div id="suzgun:dyck">
  
    <span class="title">Memory-Augmented Recurrent Neural Networks Can Learn Generalized Dyck Languages.</span>
    <span class="author">
      
        
          
          
            
              Mirac Suzgun,
            
          
        
      
        
          
          
            
              <a href="https://www.sebastiangehrmann.com" target="_blank">Sebastian Gehrmann</a>,
            
          
        
      
        
          
          
            <em>Yonatan Belinkov</em>,
          
        
      
        
          
          
          
            
              <a href="http://www.eecs.harvard.edu/~shieber/index.html" target="_blank">Stuart Shieber</a>
            
          
        
      
    </span>

    
    <span class="periodical">
    
    
    </span>
    

    
  

  

  <span class="links">
  
  
  
  
  
  
  
    [<a href="https://arxiv.org/abs/1911.03329" target="_blank">Arxiv</a>]
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
</div>
</li>
<li>

<div id="hernandez:lre">
  
    <span class="title">Linearity of Relation Decoding in Transformer Language Models.</span>
    <span class="author">
      
        
          
          
            
              Evan Hernandez,
            
          
        
      
        
          
          
            
              Arnab Sen Sharma,
            
          
        
      
        
          
          
            
              Tal Haklay,
            
          
        
      
        
          
          
            
              Kevin Meng,
            
          
        
      
        
          
          
            
              Martin Wattenberg,
            
          
        
      
        
          
          
            
              Jacob Andreas,
            
          
        
      
        
          
          
            <em>Yonatan Belinkov</em>,
          
        
      
        
          
          
          
            
              <a href="https://baulab.info" target="_blank">David Bau</a>
            
          
        
      
    </span>

    
    <span class="periodical">
    
    
    </span>
    

    
  

  

  <span class="links">
  
  
  
  
  
  
  
    [<a href="https://arxiv.org/abs/2308.09124" target="_blank">Arxiv</a>]
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
</div>
</li>
<li>

<div id="dotan:tokenization">
  
    <span class="title">Effect of Tokenization on Transformers for Biological Sequences.</span>
    <span class="author">
      
        
          
          
            
              Edo Dotan,
            
          
        
      
        
          
          
            
              Gal Jaschek,
            
          
        
      
        
          
          
            
              Tal Pupko,
            
          
        
      
        
          
          
          
            <em>Yonatan Belinkov</em>
          
        
      
    </span>

    
    <span class="periodical">
    
    
    </span>
    

    
  

  

  <span class="links">
  
  
  
  
  
  
  
    [<a href="https://www.biorxiv.org/content/10.1101/2023.08.15.553415" target="_blank">Arxiv</a>]
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
</div>
</li>
<li>

<div id="muhlgay:factuality">
  
    <span class="title">Generating Benchmarks for Factuality Evaluation of Language Models.</span>
    <span class="author">
      
        
          
          
            
              Dor Muhlgay,
            
          
        
      
        
          
          
            
              Ori Ram,
            
          
        
      
        
          
          
            
              Inbal Magar,
            
          
        
      
        
          
          
            
              Yoav Levine,
            
          
        
      
        
          
          
            
              Nir Ratner,
            
          
        
      
        
          
          
            <em>Yonatan Belinkov</em>,
          
        
      
        
          
          
            
              Omri Abend,
            
          
        
      
        
          
          
            
              Kevin Leyton-Brown,
            
          
        
      
        
          
          
            
              Amnon Shashua,
            
          
        
      
        
          
          
          
            
              Yoav Shoham
            
          
        
      
    </span>

    
    <span class="periodical">
    
    
    </span>
    

    
  

  

  <span class="links">
  
  
  
  
  
  
  
    [<a href="https://arxiv.org/abs/2307.06908" target="_blank">Arxiv</a>]
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
</div>
</li>
<li>

<div id="arad:refact">
  
    <span class="title">ReFACT: Updating Text-to-Image Models by Editing the Text Encoder.</span>
    <span class="author">
      
        
          
          
            
              Dana Arad,
            
          
        
      
        
          
          
            
              <a href="https://orgadhadas.github.io" target="_blank">Hadas Orgad</a>,
            
          
        
      
        
          
          
          
            <em>Yonatan Belinkov</em>
          
        
      
    </span>

    
    <span class="periodical">
    
    
    </span>
    

    
  

  

  <span class="links">
  
  
  
  
  
  
  
    [<a href="https://arxiv.org/abs/2306.00738" target="_blank">Arxiv</a>]
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
</div>
</li>
<li>

<div id="stolfo:math">
  
    <span class="title">Understanding Arithmetic Reasoning in Language Models using Causal Mediation Analysis.</span>
    <span class="author">
      
        
          
          
            
              Alessandro Stolfo,
            
          
        
      
        
          
          
            <em>Yonatan Belinkov</em>,
          
        
      
        
          
          
          
            
              Mrinmaya Sachan
            
          
        
      
    </span>

    
    <span class="periodical">
    
    
    </span>
    

    
  

  

  <span class="links">
  
  
  
  
  
  
  
    [<a href="https://arxiv.org/abs/2305.15054" target="_blank">Arxiv</a>]
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
</div>
</li>
<li>

<div id="katz:viz">
  
    <span class="title">Interpreting Transformer‚Äôs Attention Dynamic Memory and Visualizing the Semantic Information Flow of GPT.</span>
    <span class="author">
      
        
          
          
            
              Shachar Katz,
            
          
        
      
        
          
          
          
            <em>Yonatan Belinkov</em>
          
        
      
    </span>

    
    <span class="periodical">
    
    
    </span>
    

    
  

  

  <span class="links">
  
  
  
  
  
  
  
    [<a href="https://arxiv.org/abs/2305.13417" target="_blank">Arxiv</a>]
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
</div>
</li>
<li>

<div id="rahamim:contrasim">
  
    <span class="title">ContraSim ‚Äì A Similarity Measure Based on Contrastive Learning.</span>
    <span class="author">
      
        
          
          
            
              Adir Rahamim,
            
          
        
      
        
          
          
          
            <em>Yonatan Belinkov</em>
          
        
      
    </span>

    
    <span class="periodical">
    
    
    </span>
    

    
  

  

  <span class="links">
  
  
  
  
  
  
  
    [<a href="https://arxiv.org/abs/2303.16992" target="_blank">Arxiv</a>]
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
</div>
</li>
<li>

<div id="elazar2022">
  
    <span class="title">Measuring Causal Effects of Data Statistics on Language Model‚Äôs ‚ÄòFactual‚Äô Predictions.</span>
    <span class="author">
      
        
          
          
            
              Yanai Elazar,
            
          
        
      
        
          
          
            
              Nora Kassner,
            
          
        
      
        
          
          
            
              Shauli Ravfogel,
            
          
        
      
        
          
          
            
              Amir Feder,
            
          
        
      
        
          
          
            
              Abhilasha Ravichander,
            
          
        
      
        
          
          
            
              Marius Mosbach,
            
          
        
      
        
          
          
            <em>Yonatan Belinkov</em>,
          
        
      
        
          
          
            
              Hinrich Sch√ºtze,
            
          
        
      
        
          
          
          
            
              <a href="https://www.cs.bgu.ac.il/~yoavg/uni/" target="_blank">Yoav Goldberg</a>
            
          
        
      
    </span>

    
    <span class="periodical">
    
    
    </span>
    

    
  

  

  <span class="links">
  
  
  
  
  
  
  
    [<a href="https://arxiv.org/abs/2207.14251" target="_blank">Arxiv</a>]
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
</div>
</li>
<li>

<div id="vig:mediation">
  
    <span class="title">Causal Mediation Analysis for Interpreting NLP Models: The Case of Gender Bias.</span>
    <span class="author">
      
        
          
          
            
              Jesse Vig,
            
          
        
      
        
          
          
            
              <a href="https://www.sebastiangehrmann.com" target="_blank">Sebastian Gehrmann</a>,
            
          
        
      
        
          
          
            <em>Yonatan Belinkov</em>,
          
        
      
        
          
          
            
              Sharon Qian,
            
          
        
      
        
          
          
            
              Daniel Nevo,
            
          
        
      
        
          
          
            
              Yaron Singer,
            
          
        
      
        
          
          
          
            
              <a href="http://www.eecs.harvard.edu/~shieber/index.html" target="_blank">Stuart Shieber</a>
            
          
        
      
    </span>

    
    <span class="periodical">
    
    
    </span>
    

    
  

  

  <span class="links">
  
  
  
  
  
  
  
    [<a href="https://arxiv.org/abs/2004.12265" target="_blank">Arxiv</a>]
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
</div>
</li>
<li>

<div id="gandikota:uce">
  
    <span class="title">Unified Concept Editing in Diffusion Models.</span>
    <span class="author">
      
        
          
          
            
              Rohit Gandikota,
            
          
        
      
        
          
          
            
              <a href="https://orgadhadas.github.io" target="_blank">Hadas Orgad</a>,
            
          
        
      
        
          
          
            <em>Yonatan Belinkov</em>,
          
        
      
        
          
          
            
              Joanna Materzy≈Ñska,
            
          
        
      
        
          
          
          
            
              <a href="https://baulab.info" target="_blank">David Bau</a>
            
          
        
      
    </span>

    
    <span class="periodical">
    
    
    </span>
    

    
  

  

  <span class="links">
  
  
  
  
  
  
  
    [<a href="https://arxiv.org/abs/2308.14761" target="_blank">Arxiv</a>]
  
  
   
  
  </span>

  <!-- Hidden abstract block -->
  
</div>
</li></ol>


  </article>

  

  

  

  

</div>

      </div>
    </div>

    <footer class="site-footer">

  <div class="wrapper">
  	<p class="small">
    ¬© Copyright 2023 Yonatan Belinkov.
    Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme.
</p>
  </div>

</footer>


    <!-- Load jQuery -->
<script src="//code.jquery.com/jquery-1.12.4.min.js"></script>

<!-- Load Common JS -->
<script src="https://belinkov.com/assets/js/common.js"></script>

<!-- Load KaTeX -->
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.6.0/katex.min.css">
<script src="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.6.0/katex.min.js"></script>
<script src="https://belinkov.com/assets/js/katex.js"></script>

<!-- Include custom icon fonts -->
<link rel="stylesheet" href="https://belinkov.com/assets/css/font-awesome.min.css">
<link rel="stylesheet" href="https://belinkov.com/assets/css/academicons.min.css">


  </body>

</html>
